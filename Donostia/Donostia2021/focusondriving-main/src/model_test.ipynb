{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_test.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"conda_tensorflow2_p36","language":"python","name":"conda_tensorflow2_p36"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1SwmS1THSaU-"},"source":["# Script para testear los modelos generados y visualizar el resultado en imágenes\n","\n"]},{"cell_type":"code","metadata":{"id":"5ltm22Keg7Zp"},"source":["# comprobar que estamos utilizando GPU\n","!nvidia-smi\n","# !nvcc -V\n","# !nvidia-smi -q"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D433VXk9Ttzy"},"source":["# Instalar librerías no preinstaladas en Colab"]},{"cell_type":"code","metadata":{"id":"bnmJsEDF8Iv4","executionInfo":{"status":"ok","timestamp":1624088326419,"user_tz":-120,"elapsed":3606,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}}},"source":["!pip install imutils --quiet"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWQrI3pCS5im"},"source":["# Conexión a drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_gpnoa_8Iv0","executionInfo":{"status":"ok","timestamp":1624088349625,"user_tz":-120,"elapsed":23217,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}},"outputId":"53c19803-9657-4e20-fd46-f060f4c9c411"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","path_drive = '/content/gdrive/My Drive/Focusondriving'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zckHKFepuB2x","executionInfo":{"status":"ok","timestamp":1624088351264,"user_tz":-120,"elapsed":1644,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}}},"source":["import os\n","from imutils import paths\n","import cv2\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import heapq\n","from statistics import mean\n","\n","\n","from keras.models import Model\n","from keras.models import load_model \n","from keras.applications.vgg16 import VGG16, preprocess_input\n","from keras.preprocessing import image\n","from keras.optimizers import Adam\n","from keras.layers import Dense, Dropout, Flatten"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yj8Bf0hKUBEX"},"source":["# Función para crear la estructura del modelo para luego poder cargar los pesos de los modelos generados"]},{"cell_type":"code","metadata":{"id":"--v8D2Lh8Iv5","executionInfo":{"status":"ok","timestamp":1624088351266,"user_tz":-120,"elapsed":15,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}}},"source":["def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0, n_model=1):\n","    \"\"\"\n","    Compiles a model integrated with VGG16 pretrained layers\n","    \n","    input_shape: tuple - the shape of input images (width, height, channels)\n","    n_classes: int - number of classes for the output layer\n","    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n","    fine_tune: int - The number of pre-trained layers to unfreeze.\n","                If set to 0, all pretrained layers will freeze during training\n","    \"\"\"\n","    \n","    # Pretrained convolutional layers are loaded using the Imagenet weights.\n","    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n","    if n_model == 4:\n","        conv_base = VGG19(include_top=False,\n","                     weights='imagenet', \n","                     input_shape=input_shape)\n","    else:\n","        conv_base = VGG16(include_top=False,\n","                         weights='imagenet', \n","                         input_shape=input_shape)\n","    \n","    # Defines how many layers to freeze during training.\n","    # Layers in the convolutional base are switched from trainable to non-trainable\n","    # depending on the size of the fine-tuning parameter.\n","    if fine_tune > 0:\n","        for layer in conv_base.layers[:-fine_tune]:\n","            layer.trainable = False\n","    else:\n","        for layer in conv_base.layers:\n","            layer.trainable = False\n","\n","    # Create a new 'top' of the model (i.e. fully-connected layers).\n","    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n","    top_model = conv_base.output\n","    top_model = Flatten(name=\"flatten\")(top_model)\n","    if n_model == 1 or n_model == 2:\n","      top_model = Dense(4096, activation='relu')(top_model)\n","      top_model = Dense(1072, activation='relu')(top_model)\n","    \n","    if n_model == 3 or n_model == 4:\n","      top_model = Dense(4096, activation='relu')(top_model)\n","      top_model = Dense(1024, activation='relu')(top_model)\n","      top_model = Dense(256, activation='relu')(top_model)\n","      top_model = Dense(64, activation='relu')(top_model)\n","    \n","    if n_model == 5:\n","      top_model = Dense(4096, activation='relu')(top_model)\n","      top_model = Dense(2048, activation='relu')(top_model)\n","      top_model = Dense(1024, activation='relu')(top_model)\n","      top_model = Dense(512, activation='relu')(top_model)\n","      top_model = Dense(256, activation='relu')(top_model)\n","      top_model = Dense(128, activation='relu')(top_model)\n","      top_model = Dense(64, activation='relu')(top_model)\n","      top_model = Dense(32, activation='relu')(top_model)\n","\n","\n","    top_model = Dropout(0.2)(top_model)\n","    output_layer = Dense(n_classes, activation='softmax')(top_model)\n","    \n","    # Group the convolutional base and new fully-connected layers into a Model object.\n","    model = Model(inputs=conv_base.input, outputs=output_layer)\n","\n","    # Compiles the model for training.\n","    model.compile(optimizer=optimizer, \n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    \n","    return model"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2CH5xqRzVxhG"},"source":["# Función que accede a la carpeta test y testea el modelo foto a foto"]},{"cell_type":"code","metadata":{"id":"FbLlLrkixD-s","executionInfo":{"status":"ok","timestamp":1624088351698,"user_tz":-120,"elapsed":26,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}}},"source":["font = cv2.FONT_HERSHEY_SIMPLEX\n","fontScale = 1\n","fontColor = (255, 0, 0)\n","lineType = 2"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQnpeyxVto9h","scrolled":true,"executionInfo":{"status":"ok","timestamp":1624090222974,"user_tz":-120,"elapsed":360,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}}},"source":["def test_folder(test_data, model_name, model):\n","    directories = sorted(os.listdir(test_data))\n","    df = pd.DataFrame()\n","    for directory in directories:\n","        if '_' in directory:\n","            continue\n","        _dir = f'{test_data}/{directory}'\n","        len_files = len(os.listdir(_dir))\n","        category = _dir.split('/')[-1]\n","        test_name = _dir.split('/')[-2]\n","        print(f'{test_name} category {category} {len_files} files')\n","        img_list = paths.list_images(_dir)\n","        for i, img_path in enumerate(img_list):\n","            print(f\"{directory} {img_path}\")\n","            if i > 2:\n","              break\n","            x = preprocess_img(img_path)\n","            preds = model.predict(x)\n","            preds = list(preds[0])\n","\n","            list_desc_order = heapq.nlargest(2, range(len(preds)), key=preds.__getitem__)\n","\n","            result1 = f'c{list_desc_order[0]}'\n","            result2 = '-'\n","            result2_ = 0\n","            if preds[list_desc_order[1]] > 0.3:\n","                result2 = f'c{list_desc_order[1]}'\n","                result2_  = round(preds[list_desc_order[1]], 2)\n","            txt = f\"category {directory} result 1 {result1} {round(preds[list_desc_order[0]],2)} | result2 {result2} {result2_}\"\n","            txt = f\"categoria {directory}\"\n","            print(txt)\n","            score = round(preds[list_desc_order[0]], 2)*100\n","            score = int(score)\n","            txt2 = f\"resultado {result1} probabilidad {score}%\"\n","            img = cv2.imread(img_path)\n","            \n","            img = cv2.rectangle(img, pt1=(0, 380), pt2=(550, 480), color=(255, 255,255), thickness=-1)\n","            img = cv2.putText(img, txt, (20, 410), font, fontScale, (0, 0, 0), lineType)\n","            img = cv2.putText(img, txt2, (20, 450), font, fontScale, (255, 0, 0), lineType)\n","            img_name = img_path.split('/')[-1]\n","            img_name = f'{path_drive}/aws/img_results/cat_{directory}_res_{result1}_{img_name}'\n","            print(img_name)\n","            cv2.imwrite(img_name, img)\n","            correct2 = 0\n","            correct = 1 if result1 == directory or result2 == directory else 0\n","            if directory == 'c9':\n","                correct2 = 1 if result1 == 'c9' or result1 == 'c0' else 0\n","            error = 1 if correct == 0 else 0\n","\n","            df = df.append(pd.DataFrame({'path': [img_path],\n","                                         'name': [class_dict[directory]],\n","                                         'category': [directory],\n","                                         'result1': [result1],\n","                                         'result1%': [preds[list_desc_order[0]]],\n","                                         'result2': [result2],\n","                                         'result2%': [result2_],\n","                                         'correct': [correct],                                    \n","                                         'error': [error],\n","                                         'correct2': [correct2],}), ignore_index=True)\n","\n","            plt.imshow(img)\n","            plt.show()\n","\n","    return resume(df, test_name, model_name)"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"id":"seewml32fpG6","executionInfo":{"status":"ok","timestamp":1624090222977,"user_tz":-120,"elapsed":14,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}}},"source":["def preprocess_img(img_path):\n","  img = image.load_img(img_path, target_size=(224, 224))\n","  x = image.img_to_array(img) \n","  x = np.expand_dims(x, axis=0)\n","  x = preprocess_input(x)\n","  return x"],"execution_count":63,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JCl2hPDIdC7W"},"source":["# Testeo de modelos con diferentes conjuntos de test"]},{"cell_type":"code","metadata":{"id":"TYknxMontlfx","executionInfo":{"status":"ok","timestamp":1624090225572,"user_tz":-120,"elapsed":3,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}}},"source":["input_shape = (224, 224, 3)\n","n_classes=10\n","optim_2 = Adam(learning_rate=0.0001)\n","\n","test_model = {\n","    #'model1': f'{path_drive}/aws/model/Definitivo/model1.hdf5',\n","    #'model2': f'{path_drive}/aws/model/Definitivo/model2.hdf5',\n","    'model3': f'{path_drive}/aws/model/Definitivo/model3.hdf5',\n","    #'model4': f'{path_drive}/aws/model/Definitivo/model4.hdf5',\n","    #'model5': f'{path_drive}/aws/model/Definitivo/model5.hdf5'\n","    }\n","test_folders = [f'{path_drive}/aws/test',  f'{path_drive}/aws/test_henry_imanol']\n","test_folders = [f'{path_drive}/aws/test']\n"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9DOzrOozI80","executionInfo":{"status":"ok","timestamp":1624090225898,"user_tz":-120,"elapsed":2,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}}},"source":["class_dict = {\n","    'c0': 'hands on the wheel',\n","    'c1': 'mobile in right hand',\n","    'c2': 'talking on the phone with right hand',\n","    'c3': \"mobile in left hand\",\n","    'c4': 'talking on the phone with left hand',\n","    'c5': 'touching at the dash',\n","    'c6': 'drinking',\n","    'c7': 'reaching behind',\n","    'c8': 'touching the head',\n","    'c9': 'looking to the side'\n","}"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3tiWcSl8Iv7","executionInfo":{"status":"ok","timestamp":1624090249379,"user_tz":-120,"elapsed":22577,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}},"outputId":"4dbfbfc1-8ae6-4795-f5de-1f56d9dc8738"},"source":["for i, test_data in enumerate(test_folders):\n","  print(f'{1} test folder {test_data}')\n","  for key, path_model in test_model.items():\n","      print(f\"test model: {path_model.split('/')[-1]}\")\n","      model = create_model(input_shape, n_classes, optim_2, fine_tune=2, modelo=3)\n","      model.load_weights(path_model)\n","      df_resume_test = test_folder(test_data, model_name=f'model{key}', model=model)\n","      print(df_resume_test)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["1 test folder /content/gdrive/My Drive/Focusondriving/aws/test\n","test model: model3.hdf5\n","test category c0 114 files\n","c0 /content/gdrive/My Drive/Focusondriving/aws/test/c0/img_100313.jpg\n","categoria c0\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c0_res_c1_img_100313.jpg\n","c0 /content/gdrive/My Drive/Focusondriving/aws/test/c0/img_100310.jpg\n","categoria c0\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c0_res_c0_img_100310.jpg\n","c0 /content/gdrive/My Drive/Focusondriving/aws/test/c0/img_100315.jpg\n","categoria c0\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c0_res_c0_img_100315.jpg\n","c0 /content/gdrive/My Drive/Focusondriving/aws/test/c0/img_100839.jpg\n","test category c1 104 files\n","c1 /content/gdrive/My Drive/Focusondriving/aws/test/c1/img_100296.jpg\n","categoria c1\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c1_res_c1_img_100296.jpg\n","c1 /content/gdrive/My Drive/Focusondriving/aws/test/c1/img_100284.jpg\n","categoria c1\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c1_res_c1_img_100284.jpg\n","c1 /content/gdrive/My Drive/Focusondriving/aws/test/c1/img_100316.jpg\n","categoria c1\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c1_res_c1_img_100316.jpg\n","c1 /content/gdrive/My Drive/Focusondriving/aws/test/c1/img_100304.jpg\n","test category c2 113 files\n","c2 /content/gdrive/My Drive/Focusondriving/aws/test/c2/img_100287.jpg\n","categoria c2\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c2_res_c2_img_100287.jpg\n","c2 /content/gdrive/My Drive/Focusondriving/aws/test/c2/img_100834.jpg\n","categoria c2\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c2_res_c2_img_100834.jpg\n","c2 /content/gdrive/My Drive/Focusondriving/aws/test/c2/img_100844.jpg\n","categoria c2\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c2_res_c2_img_100844.jpg\n","c2 /content/gdrive/My Drive/Focusondriving/aws/test/c2/img_100849.jpg\n","test category c3 95 files\n","c3 /content/gdrive/My Drive/Focusondriving/aws/test/c3/img_100306.jpg\n","categoria c3\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c3_res_c3_img_100306.jpg\n","c3 /content/gdrive/My Drive/Focusondriving/aws/test/c3/img_100305.jpg\n","categoria c3\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c3_res_c3_img_100305.jpg\n","c3 /content/gdrive/My Drive/Focusondriving/aws/test/c3/img_100302.jpg\n","categoria c3\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c3_res_c3_img_100302.jpg\n","c3 /content/gdrive/My Drive/Focusondriving/aws/test/c3/img_100308.jpg\n","test category c4 108 files\n","c4 /content/gdrive/My Drive/Focusondriving/aws/test/c4/img_100288.jpg\n","categoria c4\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c4_res_c4_img_100288.jpg\n","c4 /content/gdrive/My Drive/Focusondriving/aws/test/c4/img_100292.jpg\n","categoria c4\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c4_res_c4_img_100292.jpg\n","c4 /content/gdrive/My Drive/Focusondriving/aws/test/c4/img_100831.jpg\n","categoria c4\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c4_res_c4_img_100831.jpg\n","c4 /content/gdrive/My Drive/Focusondriving/aws/test/c4/img_100826.jpg\n","test category c5 117 files\n","c5 /content/gdrive/My Drive/Focusondriving/aws/test/c5/img_10.jpg\n","categoria c5\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c5_res_c5_img_10.jpg\n","c5 /content/gdrive/My Drive/Focusondriving/aws/test/c5/img_1.jpg\n","categoria c5\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c5_res_c5_img_1.jpg\n","c5 /content/gdrive/My Drive/Focusondriving/aws/test/c5/img_100298.jpg\n","categoria c5\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c5_res_c5_img_100298.jpg\n","c5 /content/gdrive/My Drive/Focusondriving/aws/test/c5/img_100291.jpg\n","test category c6 106 files\n","c6 /content/gdrive/My Drive/Focusondriving/aws/test/c6/img_100289.jpg\n","categoria c6\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c6_res_c6_img_100289.jpg\n","c6 /content/gdrive/My Drive/Focusondriving/aws/test/c6/img_100303.jpg\n","categoria c6\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c6_res_c6_img_100303.jpg\n","c6 /content/gdrive/My Drive/Focusondriving/aws/test/c6/img_100311.jpg\n","categoria c6\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c6_res_c6_img_100311.jpg\n","c6 /content/gdrive/My Drive/Focusondriving/aws/test/c6/img_100307.jpg\n","test category c7 100 files\n","c7 /content/gdrive/My Drive/Focusondriving/aws/test/c7/img_100835.jpg\n","categoria c7\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c7_res_c7_img_100835.jpg\n","c7 /content/gdrive/My Drive/Focusondriving/aws/test/c7/img_100843.jpg\n","categoria c7\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c7_res_c7_img_100843.jpg\n","c7 /content/gdrive/My Drive/Focusondriving/aws/test/c7/img_102016.jpg\n","categoria c7\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c7_res_c7_img_102016.jpg\n","c7 /content/gdrive/My Drive/Focusondriving/aws/test/c7/img_102004.jpg\n","test category c8 68 files\n","c8 /content/gdrive/My Drive/Focusondriving/aws/test/c8/img_100290.jpg\n","categoria c8\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c8_res_c8_img_100290.jpg\n","c8 /content/gdrive/My Drive/Focusondriving/aws/test/c8/img_100317.jpg\n","categoria c8\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c8_res_c6_img_100317.jpg\n","c8 /content/gdrive/My Drive/Focusondriving/aws/test/c8/img_100301.jpg\n","categoria c8\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c8_res_c8_img_100301.jpg\n","c8 /content/gdrive/My Drive/Focusondriving/aws/test/c8/img_101987.jpg\n","test category c9 77 files\n","c9 /content/gdrive/My Drive/Focusondriving/aws/test/c9/img_100293.jpg\n","categoria c9\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c9_res_c9_img_100293.jpg\n","c9 /content/gdrive/My Drive/Focusondriving/aws/test/c9/img_100300.jpg\n","categoria c9\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c9_res_c7_img_100300.jpg\n","c9 /content/gdrive/My Drive/Focusondriving/aws/test/c9/img_100819.jpg\n","categoria c9\n","/content/gdrive/My Drive/Focusondriving/aws/img_results/cat_c9_res_c0_img_100819.jpg\n","c9 /content/gdrive/My Drive/Focusondriving/aws/test/c9/img_100822.jpg\n","save: /content/gdrive/My Drive/Focusondriving/aws/results/modelmodel3_test_results.xlsx\n","<pandas.io.formats.style.Styler object at 0x7fc8c05a3890>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XzZ4ZZMJwF-8"},"source":["# Crear un gif con los resultados y guardar en drive"]},{"cell_type":"code","metadata":{"id":"FXkKH7q3aRMR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624090386610,"user_tz":-120,"elapsed":7496,"user":{"displayName":"Focus2 Ondriving","photoUrl":"","userId":"16242722710693408615"}},"outputId":"7023aee3-4f0d-4574-f0f6-d0f81721cd9d"},"source":["import imageio\n","from imutils import paths\n","path_imgs = f\"{path_drive}/aws/img_results\"\n","path_save_gif = f\"{path_drive}/aws/result.gif\"\n","images = [imageio.imread(filename) for filename in paths.list_images(path_imgs)]\n","kargs = {'duration': 0.75}\n","imageio.mimsave(path_save_gif, images, **kargs)\n","print('gif created')"],"execution_count":70,"outputs":[{"output_type":"stream","text":["gif created\n"],"name":"stdout"}]}]}