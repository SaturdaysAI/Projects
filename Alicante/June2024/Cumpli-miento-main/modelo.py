# -*- coding: utf-8 -*-
"""Modelo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S2xbzUPR05RGr2AFHFSCiC-eyACJ4wY4

# **Cumpli-Miento: Detección de cumplimiento del RGPD en políticas de privacidad web**

- Autores: Daniel Carreres y Juan Sanz
- Fecha: 27/07/2024
- Versión: 1.0

# Tabla de contenidos (índice)

>[Cumpli-Miento: Detección de cumplimiento del RGPD en políticas de privacidad web](#scrollTo=xaAXRslVRqWQ)

>[Tabla de contenidos (índice)](#scrollTo=xaAXRslVRqWQ)

>>[0. Cargar los datos](#scrollTo=qgwDlksWG55_)

>>[1. Preparación de los Datos](#scrollTo=EyJFhIcuHpab)

>>[2. Definición y Entrenamiento de Modelos](#scrollTo=U-OvYGEiG9kE)

>>[3. Evaluación de Políticas de Privacidad](#scrollTo=mLk5ZVbTH6qW)

**Descripción del proyecto**

Este proyecto tiene como objetivo desarrollar una herramienta automática para evaluar si las políticas de privacidad de diferentes empresas cumplen con ciertos derechos establecidos por el Reglamento General de Protección de Datos (GDPR). Utilizando técnicas de procesamiento de lenguaje natural (NLP) y aprendizaje automático, se analiza el texto de las políticas de privacidad y se determina el cumplimiento de varios derechos específicos.

Objetivos:

Extraer y analizar el texto de documentos HTML que contienen políticas de privacidad.
Entrenar modelos de clasificación para determinar el cumplimiento de varios derechos de privacidad.
Evaluar nuevas políticas de privacidad mediante modelos entrenados y generar un informe de cumplimiento.

## 0. Cargar los datos
"""

import os
import zipfile
import requests
import pandas as pd

# Enlace directo al archivo ZIP en GitHub
zip_url = 'https://github.com/dcg64-ua/prueba/raw/main/privacy_policys.zip'  # Cambia este enlace por el correcto

# Nombre del archivo ZIP después de descargarlo
zip_filename = 'privacy_policys.zip'

# Descargar el archivo ZIP usando requests
response = requests.get(zip_url)
with open(zip_filename, 'wb') as file:
    file.write(response.content)

# Enlace directo al archivo CSV en GitHub
csv_url = 'https://github.com/dcg64-ua/prueba/raw/main/Refined_Corrected_Data.csv'  # Cambia este enlace por el correcto

# Nombre del archivo CSV después de descargarlo
csv_filename = 'Refined_Corrected_Data.csv'

# Descargar el archivo CSV usando requests
response = requests.get(csv_url)
with open(csv_filename, 'wb') as file:
    file.write(response.content)

# Leer el archivo CSV
csv_filepath = csv_filename
data = pd.read_csv(csv_filepath)

# Verificar las primeras filas del DataFrame
data.head()

import zipfile
import os


# Definir la ruta del archivo zip cargado
zip_file_path = '/content/privacy_policys.zip'  # Cambia esta ruta por la ruta correcta de tu archivo zip
extract_path = '/content/privacy_policys'  # Cambia esta ruta por la ruta donde quieres descomprimir los archivos

# Descomprimir el archivo zip
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Listar los archivos extraídos para asegurarse de que se han descomprimido correctamente
extracted_files = os.listdir(extract_path)
print("Archivos extraídos:", extracted_files[:10])  # Muestra solo los primeros 10 archivos para brevedad

"""## 1. Preparación de los Datos"""

import os
import pandas as pd

# Cargar el archivo CSV
csv_file_path = '/content/Refined_Corrected_Data.csv'
data = pd.read_csv(csv_file_path)

# Verificar los archivos en el directorio
extract_path = '/content/privacy_policys/privacy_policys (2)'
renamed_files = os.listdir(extract_path)
print("Archivos renombrados:", renamed_files[:10])

# Añadir una nueva columna 'Filename' en el DataFrame con los nombres simples de las empresas
data['Filename'] = data['Empresa'].apply(lambda x: x + '.html')

# Verificar las primeras filas del DataFrame
print(data.head())

"""## 2. Definición y Entrenamiento de Modelos"""

from bs4 import BeautifulSoup

# Definir la función para extraer texto de archivos HTML
def extract_text_from_html(file_path):
    if file_path and os.path.isfile(file_path):
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
            soup = BeautifulSoup(content, 'html.parser')
            text = soup.get_text(separator=' ', strip=True)
        return text
    return None

# Aplicar la función refinada al DataFrame
def match_and_extract_text_using_filenames(df, extract_path):
    df['Texto'] = df['Filename'].apply(lambda filename: extract_text_from_html(os.path.join(extract_path, filename)))
    return df

# Aplicar la función para extraer texto al DataFrame
data_with_text = match_and_extract_text_using_filenames(data, extract_path)

# Filtrar las filas donde el texto fue extraído correctamente
data_with_text = data_with_text.dropna(subset=['Texto'])

# Mostrar el DataFrame con el texto incluido
print(data_with_text.head())

# Verificar las columnas del DataFrame
print("Columnas del DataFrame:", data.columns)

# Añadir nuevamente la columna 'Filename' en el DataFrame
data['Filename'] = data['Empresa'].apply(lambda x: x.lower() + '.html')

# Verificar las primeras filas del DataFrame con la nueva columna
print(data.head())

# Aplicar la función refinada al DataFrame para extraer texto
def match_and_extract_text_using_filenames(df, extract_path):
    df['Texto'] = df['Filename'].apply(lambda filename: extract_text_from_html(os.path.join(extract_path, filename)))
    return df

# Aplicar la función para extraer texto al DataFrame
data_with_text = match_and_extract_text_using_filenames(data, extract_path)

# Filtrar las filas donde el texto fue extraído correctamente
data_with_text = data_with_text.dropna(subset=['Texto'])

# Mostrar el DataFrame con el texto incluido
data_with_text.head()

"""## 3. Evaluación de Políticas de Privacidad"""

import requests

# Función para extraer texto de una URL
def extract_text_from_url(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        return text
    else:
        print(f"Error al acceder a la URL: {response.status_code}")
        return None

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

# Derechos a evaluar
derechos = ['Derecho a ser informado', 'Derecho de acceso', 'Derecho de rectificación',
            'Derecho de eliminación', 'Derecho de procesamiento restringido',
            'Derecho a la portabilidad de los datos', 'Derecho de oposición',
            'Derechos relacionados con la toma de decisiones y la elaboración de perfiles automatizados']

# Diccionario para almacenar modelos y vectorizadores para cada derecho
models = {}
vectorizers = {}

# Entrenar un modelo para cada derecho
for derecho in derechos:
    X = data_with_text['Texto']
    y = data_with_text[derecho]

    # Verificar que haya al menos dos clases en los datos
    if len(y.unique()) > 1:
        # Crear un pipeline de TF-IDF Vectorizer y Logistic Regression
        pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))

        # Dividir los datos en conjuntos de entrenamiento y prueba
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Entrenar el modelo
        pipeline.fit(X_train, y_train)

        # Evaluar el modelo
        accuracy = pipeline.score(X_test, y_test)
        print(f"Accuracy for {derecho}: {accuracy}")

        # Guardar el modelo y el vectorizador
        models[derecho] = pipeline.named_steps['logisticregression']
        vectorizers[derecho] = pipeline.named_steps['tfidfvectorizer']
    else:
        print(f"No hay suficientes datos para entrenar el modelo para {derecho}")

import requests
from bs4 import BeautifulSoup

# Función para extraer texto de una URL
def extract_text_from_url(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        return text
    else:
        print(f"Error al acceder a la URL: {response.status_code}")
        return None

# Función para evaluar el texto extraído para cada derecho
def evaluate_rights_from_url(url, models, vectorizers):
    text = extract_text_from_url(url)
    if text:
        results = {}
        for derecho in derechos:
            if derecho in models:
                vectorizer = vectorizers[derecho]
                model = models[derecho]
                X_new = vectorizer.transform([text])
                prediction = model.predict(X_new)
                results[derecho] = 'Cumple' if prediction[0] else 'No cumple'
            else:
                results[derecho] = 'Sin datos suficientes'
        return results
    else:
        return None

urls = ["https://www.apple.com/legal/privacy/es/", "https://www.facebook.com/privacy/policy/?entry_point=data_policy_redirect&entry=0", "https://www.linkedin.com/legal/privacy-policy"]
# Ejemplo de URL para evaluar
for url_to_evaluate in urls:
  print("Resultados de: " + url_to_evaluate)
  url_to_evaluate = "https://en.wikipedia.org/wiki/Wikipedia:Privacy#:~:text=Overview,most%20others%20use%20a%20pseudonym."
  results = evaluate_rights_from_url(url_to_evaluate, models, vectorizers)
  print("Resultados de la evaluación:")
  for derecho, resultado in results.items():
      print(f"{derecho}: {resultado}")