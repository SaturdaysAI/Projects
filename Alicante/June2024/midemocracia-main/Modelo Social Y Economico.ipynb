{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/pdfs/pdfs_comprimidos.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1tDF5VgT96i",
        "outputId": "18cbbbb7-90a1-4566-ce4e-3039897173df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/pdfs/pdfs_comprimidos.zip\n",
            " extracting: L14/BOCG-14-B-108-1.PDF  \n",
            " extracting: L14/BOCG-14-B-137-1.PDF  \n",
            " extracting: L14/BOCG-14-B-52-1.PDF  \n",
            " extracting: L14/BOCG-14-B-65-1.PDF  \n",
            " extracting: L14/BOCG-14-B-45-1.PDF  \n",
            " extracting: L14/BOCG-14-B-85-1.PDF  \n",
            " extracting: L14/BOCG-14-B-78-1.PDF  \n",
            " extracting: L14/BOCG-14-B-100-1.PDF  \n",
            " extracting: L15/BOCG-15-B-69-1.PDF  \n",
            " extracting: L15/BOCG-15-B-32-1.PDF  \n",
            " extracting: L15/BOCG-15-B-112-1.PDF  \n",
            " extracting: L15/BOCG-15-B-121-1.PDF  \n",
            " extracting: L15/BOCG-15-B-43-1.PDF  \n",
            " extracting: L15/BOCG-15-B-47-1.PDF  \n",
            " extracting: L15/BOCG-15-B-81-1.PDF  \n",
            " extracting: L15/BOCG-15-B-59-1.PDF  \n",
            " extracting: L13/BOCG-13-B-82-1.PDF  \n",
            " extracting: L13/BOCG-13-B-51-1.PDF  \n",
            " extracting: L13/BOCG-13-B-62-1.PDF  \n",
            " extracting: L13/BOCG-13-B-71-1.PDF  \n",
            " extracting: L13/BOCG-13-B-40-1.PDF  \n",
            " extracting: L12/BOCG-12-B-100-1.PDF  \n",
            " extracting: L12/BOCG-12-B-128-1.PDF  \n",
            " extracting: L12/BOCG-12-B-20-1.PDF  \n",
            " extracting: L12/BOCG-12-B-65-1.PDF  \n",
            " extracting: L12/BOCG-12-B-89-1.PDF  \n",
            " extracting: L12/BOCG-12-B-52-1.PDF  \n",
            " extracting: L12/BOCG-12-B-41-1.PDF  \n",
            " extracting: L12/BOCG-12-B-77-1.PDF  \n",
            " extracting: L12/BOCG-12-B-112-1.PDF  \n",
            " extracting: L12/BOCG-12-B-31-1.PDF  \n",
            " extracting: L10/BOCG-10-B-97-1.PDF  \n",
            " extracting: L10/BOCG-10-B-118-1.PDF  \n",
            " extracting: L10/BOCG-10-B-108-1.PDF  \n",
            " extracting: L11/BOCG-11-B-18-1.PDF  \n",
            " extracting: L11/BOCG-11-B-27-1.PDF  \n",
            " extracting: L11/BOCG-11-B-38-1.PDF  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrmspPY-QI_R",
        "outputId": "2c4f78e9-8214-4c19-cc57-6abcb6ccc6d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Using cached PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "Collecting PyMuPDFb==1.24.6 (from pymupdf)\n",
            "  Using cached PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n",
            "Installing collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.6 pymupdf-1.24.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import fitz  # PyMuPDF para extraer texto de PDFs\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar stopwords si no están descargadas\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Función para extraer texto de un archivo PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# Función para preprocesar el texto y eliminar las 20 primeras y últimas palabras\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Eliminar números\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Eliminar caracteres especiales\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stopwords.words('spanish')]  # Eliminar stopwords\n",
        "\n",
        "    # Eliminar las 20 primeras y 20 últimas palabras\n",
        "    if len(words) > 40:  # Asegurarse de que haya al menos 40 palabras\n",
        "        words = words[20:-20]\n",
        "    else:\n",
        "        words = []  # Si no hay suficientes palabras, se deja vacío\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "# Cargar el dataset desde Excel\n",
        "dataset = pd.read_excel('/content/DatasetL11-L15.xlsx')\n",
        "\n",
        "# Crear listas para almacenar los textos preprocesados\n",
        "textos_economico = []\n",
        "textos_social = []\n",
        "\n",
        "# Iterar sobre cada fila del dataset\n",
        "for index, row in dataset.iterrows():\n",
        "    ruta_pdf = row['Ruta']\n",
        "    try:\n",
        "        # Extraer texto del PDF\n",
        "        texto_pdf = extract_text_from_pdf(ruta_pdf)\n",
        "        # Preprocesar el texto\n",
        "        texto_procesado = preprocess_text(texto_pdf)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar PDF en ruta: {ruta_pdf}. Error: {str(e)}\")\n",
        "        texto_procesado = ''  # Agregar cadena vacía en caso de error\n",
        "\n",
        "    # Verificar el tipo y agregar el texto procesado a la lista correspondiente\n",
        "    if row['Economico']:\n",
        "        textos_economico.append(texto_procesado)\n",
        "    else:\n",
        "        textos_economico.append('')\n",
        "\n",
        "    if row['Social']:\n",
        "        textos_social.append(texto_procesado)\n",
        "    else:\n",
        "        textos_social.append('')\n",
        "\n",
        "# Agregar las listas de textos preprocesados al dataset como nuevas columnas\n",
        "dataset['Texto_Economico'] = textos_economico\n",
        "dataset['Texto_Social'] = textos_social\n",
        "\n",
        "# Crear nuevos datasets solo con las columnas necesarias\n",
        "dataset_economico = dataset[['Economico', 'Ruta', 'Texto_Economico']]\n",
        "dataset_social = dataset[['Social', 'Ruta', 'Texto_Social']]\n",
        "\n",
        "# Guardar los nuevos datasets actualizados\n",
        "dataset_economico.to_excel('dataset_economico.xlsx', index=False)\n",
        "dataset_social.to_excel('dataset_social.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xDeCsSZToYy",
        "outputId": "bd1a708d-f31e-467e-939b-bb714941a89c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import random\n",
        "random_number = random.randint(1, 100)\n",
        "\n",
        "# Cargar el dataset con texto preprocesado desde Excel\n",
        "dataset = pd.read_excel('/content/dataset_economico.xlsx')\n",
        "\n",
        "# Eliminar filas con NaN en la columna 'Texto'\n",
        "dataset = dataset.dropna(subset=['Texto_Economico'])\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X = dataset['Texto_Economico']\n",
        "y = dataset['Economico']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=47)\n",
        "print(f'Semilla aleatoria utilizada en train_test_split: 47')\n",
        "\n",
        "# Crear un objeto CountVectorizer\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Utilizar n-grams de 1 y 2 palabras\n",
        "\n",
        "# Ajustar y transformar los datos de entrenamiento\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transformar los datos de prueba\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Inicializar y entrenar un modelo de Regresión Logística\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predecir sobre los datos de prueba\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Evaluar el rendimiento del modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Exactitud del modelo: {accuracy:.2f}')\n",
        "\n",
        "# Imprimir el reporte de clasificación\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Guardar el modelo entrenado para su uso posterior\n",
        "joblib.dump(model, 'modelo_bag_of_ngrams_economico_texto.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnYMLSQGbfhj",
        "outputId": "f2b26924-2f00-4d9b-fcb2-3fc2e79daa32"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semilla aleatoria utilizada en train_test_split: 47\n",
            "Exactitud del modelo: 1.00\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00         4\n",
            "   macro avg       1.00      1.00      1.00         4\n",
            "weighted avg       1.00      1.00      1.00         4\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['modelo_bag_of_ngrams_economico_texto.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "random_number = random.randint(1, 100)\n",
        "\n",
        "# Cargar el dataset con texto preprocesado desde Excel\n",
        "dataset = pd.read_excel('/content/dataset_social.xlsx')\n",
        "\n",
        "# Eliminar filas con NaN en la columna 'Texto'\n",
        "dataset = dataset.dropna(subset=['Texto_Social'])\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X = dataset['Texto_Social']\n",
        "y = dataset['Social']  # Cambiar a la columna 'Social' como variable objetivo\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=83)\n",
        "print(f'Semilla aleatoria utilizada en train_test_split: 83')\n",
        "\n",
        "# Crear un objeto CountVectorizer\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Utilizar n-grams de 1 y 2 palabras\n",
        "\n",
        "# Ajustar y transformar los datos de entrenamiento\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transformar los datos de prueba\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Inicializar y entrenar un modelo de Regresión Logística\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predecir sobre los datos de prueba\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Evaluar el rendimiento del modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Exactitud del modelo: {accuracy:.2f}')\n",
        "\n",
        "# Imprimir el reporte de clasificación\n",
        "print(classification_report(y_test, y_pred, zero_division=1))  # Establecer zero_division=1 para evitar el warning\n",
        "\n",
        "# Guardar el modelo entrenado para su uso posterior\n",
        "joblib.dump(model, 'modelo_bag_of_ngrams_social_texto.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxX-0yGoipjY",
        "outputId": "162474c6-14a5-4283-d7f6-e7a769479f21"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semilla aleatoria utilizada en train_test_split: 83\n",
            "Exactitud del modelo: 0.71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -2       1.00      0.00      0.00         1\n",
            "           2       1.00      0.00      0.00         1\n",
            "           3       0.80      1.00      0.89         4\n",
            "           4       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.71         7\n",
            "   macro avg       0.82      0.50      0.39         7\n",
            "weighted avg       0.81      0.71      0.60         7\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['modelo_bag_of_ngrams_social_texto.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF para extraer texto de PDFs\n",
        "import re\n",
        "import joblib\n",
        "\n",
        "# Función para extraer texto de un archivo PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# Función para preprocesar el texto\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Eliminar números\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Eliminar caracteres especiales\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stopwords.words('spanish')]  # Eliminar stopwords\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "modelo_path = 'modelo_bag_of_ngrams_economico_texto.pkl'\n",
        "model = joblib.load(modelo_path)\n",
        "\n",
        "# Ruta del PDF que deseas procesar\n",
        "ruta_pdf = '/content/pdfs/L11/BOCG-11-B-18-1.PDF'\n",
        "\n",
        "# Extraer texto del PDF\n",
        "try:\n",
        "    texto_pdf = extract_text_from_pdf(ruta_pdf)\n",
        "    # Preprocesar el texto\n",
        "    texto_procesado = preprocess_text(texto_pdf)\n",
        "    # Vectorizar el texto\n",
        "    texto_vec = vectorizer.transform([texto_procesado])\n",
        "    # Realizar la predicción\n",
        "    predicciones = model.predict(texto_vec)\n",
        "    print(f'Predicción para el PDF {ruta_pdf}: {predicciones}')\n",
        "except Exception as e:\n",
        "    print(f\"Error al procesar PDF en ruta: {ruta_pdf}. Error: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_7ApsHAn2fm",
        "outputId": "9b028839-811e-4c74-96d4-482a4921f53b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicción para el PDF /content/pdfs/L11/BOCG-11-B-18-1.PDF: [1]\n"
          ]
        }
      ]
    }
  ]
}