{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAREAS DE LIMPIEZA DE DATOS\n",
    "1. **Cambiar las abreviaturas -> ejem: \"u\"->you \"ur->\"your\"**\n",
    "2. Buscar  caracteres Nulls y Remplazar con valor \"0\" (El cero debe ser valor String). Igual queda en Duda y se debria preguntar al tutor y o demas compa;eros\n",
    "3. Buscar las palabras con mayor frecuencia y hacer un grafica o Datavisualitation.\n",
    "4. **Quitar stopwords ejem: the, and, that, a, any, an, be, with...**\n",
    "5. eliminar emojis, URL, hashtags, contracciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv', delimiter = ';', names = ['Conversation_Id', 'Authors', 'Class', 'Conversation' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df['Conversation'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazar | por un espacio ya que solo consideraremos las palabras presentes más no las secuencias de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: x.replace('|', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir números a palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "df_conv = df_conv.apply(lambda x: ' '.join([num2words(word) if word.isnumeric() and int(word)<1000000000 else word for message in x.split('|') for word in message.split(' ')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazar doble espacio con uno solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: ' '.join([word.lower() for word in x.split(' ') if word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar abreviaciones del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_dict={\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "    \"whats\":\"what is\",\n",
    "    \"whatre\":\"what are\",\n",
    "    \"whos\":\"who is\",\n",
    "    \"whore\":\"who are\",\n",
    "    \"wheres\":\"where is\",\n",
    "    \"wherere\":\"where are\",\n",
    "    \"whens\":\"when is\",\n",
    "    \"whenre\":\"when are\",\n",
    "    \"hows\":\"how is\",\n",
    "    \"howre\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "    \"im\":\"i am\",\n",
    "    #\"were\":\"we are\",\n",
    "    \"youre\":\"you are\",\n",
    "    \"theyre\":\"they are\",\n",
    "    \"hes\":\"he is\",\n",
    "    \"shes\":\"she is\",\n",
    "    \"thats\":\"that is\",\n",
    "    \"theres\":\"there is\",\n",
    "    \"therere\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "    \"ive\":\"i have\",\n",
    "    \"weve\":\"we have\",\n",
    "    \"youve\":\"you have\",\n",
    "    \"theyve\":\"they have\",\n",
    "    \"whove\":\"who have\",\n",
    "    \"wouldve\":\"would have\",\n",
    "    \"notve\":\"not have\",\n",
    "\n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "    \"ill\":\"i will\",\n",
    "    #\"well\":\"we will\",\n",
    "    \"youll\":\"you will\",\n",
    "    \"hell\":\"he will\",\n",
    "    \"shell\":\"she will\",\n",
    "    \"itll\":\"it will\",\n",
    "    \"theyll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\n",
    "    \"isnt\":\"is not\",\n",
    "    \"wasnt\":\"was not\",\n",
    "    \"arent\":\"are not\",\n",
    "    \"werent\":\"were not\",\n",
    "    \"cant\":\"can not\",\n",
    "    \"couldnt\":\"could not\",\n",
    "    \"dont\":\"do not\",\n",
    "    \"didnt\":\"did not\",\n",
    "    \"shouldnt\":\"should not\",\n",
    "    \"wouldnt\":\"would not\",\n",
    "    \"doesnt\":\"does not\",\n",
    "    \"havent\":\"have not\",\n",
    "    \"hasnt\":\"has not\",\n",
    "    \"hadnt\":\"had not\",\n",
    "    \"wont\":\"will not\",\n",
    "\n",
    "    \"2mrw\": \"tomorrow\",\n",
    "    \"aka\": \"also known as\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"a/s/l\": \"age sex location\",\n",
    "    \"ayt\": \"are you there\",\n",
    "    \"b4\": \"before\",\n",
    "    \"bbs\": \"be back soon\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"cmb\": \"call me back\",\n",
    "    \"cu l8r\": \"see you later\",\n",
    "    \"cul8r\": \"see you later\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"cos\": \"because\",\n",
    "    \"cwyl\": \"chat with you later\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"f2f\": \"face to face\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"ig\": \"instagram\",\n",
    "    \"fyeo\": \"for your eyes only\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"diy\": \"do it yourself\",\n",
    "    \"stfu\": \"shut the fuck up\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"ily\": \"ily\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"lmfao\": \"laughing my freaking ass off\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"ikr\": \"i know right\",\n",
    "    \"ofc\": \"of course\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"k\": \"okay\",\n",
    "    \"r\": \"are\",\n",
    "    \"n\": \"and\",\n",
    "    \"b\": \"be\",\n",
    "    \"wat\": \"what\",\n",
    "    \"ya\": \"you\",\n",
    "    \"dunno\": \"do not know\",\n",
    "    \"yea\": \"yeah\",\n",
    "    \"ok\": \"okay\",\n",
    "    \"kewl\": \"cool\",\n",
    "    \"nite\": \"night\",\n",
    "    \"yep\": \"yes\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: ' '.join([abbr_dict[word] if word in abbr_dict.keys() else word for word in x.split(' ') ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df_conv = df_conv.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar puntuación de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar valores alfanumericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: ' '.join([word for word in x.split(' ') if word.isalpha()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "df_conv = df_conv.apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in x.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey elo'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conv[6004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Conversation'] = df_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.dropna(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predators = df[df['Class'] == 1]\n",
    "df_non_predators = df[df['Class'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma = 0\n",
    "for line in df[\"Conversation\"]:\n",
    "    if line.find('age') != -1:\n",
    "        suma += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "common_words_pred = Counter(\" \".join(df_predators[\"Conversation\"]).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_non_pred = Counter(\" \".join(df_non_predators[\"Conversation\"]).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_pred = [tuple[0] for tuple in common_words_pred]\n",
    "words_non_pred = [tuple[0] for tuple in common_words_non_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words = [word for word in words_pred if word not in words_non_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(final_words).to_csv(\"Most_common_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words = pd.read_csv(\"Most_common_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df['Conversation'].apply(lambda x: ' '.join([word for word in x.split(' ') if word in final_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19264/2152944440.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'This is the first document.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'This document is the second document.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'And this is the third one.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Is this the first document?'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(len(vectorizer.get_feature_names()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1132\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m   1135\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.','This document is the second document.','And this is the third one.', 'Is this the first document?']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df1.to_list())\n",
    "#print(len(vectorizer.get_feature_names()))\n",
    "df_final = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "df_final['Class'] = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"Features_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ec4eee48a1b90125fedd080afd04f965bb6f28dbf2886da6c5cfcc343d0a26d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
