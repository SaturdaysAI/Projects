{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Preprocesamiento extraccion de sentencias.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/femIA2020/FEM_IA/blob/master/Preprocesamiento_extraccion_de_sentencias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmjJPgKXZqyk"
      },
      "source": [
        "# Proceso para extraciòn de setencias judiciales\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNnLovGoRiPB"
      },
      "source": [
        "### Instalaciòn de librerias para procesamiento de pdf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_zZQsA7lkAG",
        "outputId": "b08b11ac-9ef7-4131-d3f2-03924d096552"
      },
      "source": [
        "!pip install pyenchant\n",
        "!apt install enchant\n",
        "!sudo apt-get install myspell-es\n",
        "!pip install unidecode\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/55/810c871d9a556685553ab1ace4a6c580460ca476736829fffe8cfef32a66/pyenchant-3.1.1-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |█████▉                          | 10kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 20kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.1.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 1,310 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.1 [309 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.1 [87.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,310 kB in 2s (627 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 144793 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  hunspell libreoffice-core | openoffice.org-hunspell | openoffice.org-core\n",
            "  iceape-browser | iceweasel | icedove\n",
            "The following NEW packages will be installed:\n",
            "  myspell-es\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 201 kB of archives.\n",
            "After this operation, 1,004 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 myspell-es all 1.11-14 [201 kB]\n",
            "Fetched 201 kB in 1s (144 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package myspell-es.\n",
            "(Reading database ... 145207 files and directories currently installed.)\n",
            "Preparing to unpack .../myspell-es_1.11-14_all.deb ...\n",
            "Unpacking myspell-es (1.11-14) ...\n",
            "Setting up myspell-es (1.11-14) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n",
            "Collecting es_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2MB 647kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-cp36-none-any.whl size=16172934 sha256=268181f9d3ca4394e8da8e702b684f679d9944f7e73b844fddbb946cad7e248c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v094m0nh/wheels/05/4f/66/9d0c806f86de08e8645d67996798c49e1512f9c3a250d74242\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdZ5HaVxO3HQ"
      },
      "source": [
        "## habilitar carpeta de google drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIS68ASnkdxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735a18e3-019a-4287-f74b-fb57fdbb7b6a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqKDugRVkxV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c725d2d6-8357-44da-a9ab-679a85f0e4d2"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import nltk.data   \n",
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "from nltk.corpus import stopwords\n",
        "import string \n",
        "from collections import Counter\n",
        "from _collections import OrderedDict\n",
        "stop_words= set (stopwords.words('spanish'))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCvdF0BUt6ad"
      },
      "source": [
        "import enchant\n",
        "d = enchant.Dict(\"es_US\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOwkjnfRq-tC"
      },
      "source": [
        "##Procesamiento para convertir pdf a imagenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-m8Aij5skXh"
      },
      "source": [
        "### Instalaciòn de librererias para convertir pdf a imagen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5UzPaHjbRWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa9bb6d-bf75-4e01-bebd-4cc6fb05d36c"
      },
      "source": [
        "pip install pytesseract"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading https://files.pythonhosted.org/packages/17/4b/4dbd55388225bb6cd243d21f70e77cb3ce061e241257485936324b8e920f/pytesseract-0.3.6.tar.gz\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from pytesseract) (7.0.0)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.6-py2.py3-none-any.whl size=13629 sha256=188860f489b870116a776d41741e9b009699d69581b15ac572283582454e5359\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/71/72/b98430261d849ae631e283dfc7ccb456a3fb2ed2205714b63f\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhcjVVS0YxZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef79e51-e065-49f0-f22f-5bf2295c69a1"
      },
      "source": [
        "pip install pdf2image"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading https://files.pythonhosted.org/packages/03/62/089030fd16ab3e5c245315d63c80b29250b8f9e4579b5a09306eb7e7539c/pdf2image-1.14.0-py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from pdf2image) (7.0.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvBdE0XUYxRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7ade34-4da6-4935-e1d1-fe22b82da792"
      },
      "source": [
        "!apt-get install poppler-utils"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 154 kB of archives.\n",
            "After this operation, 613 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.10 [154 kB]\n",
            "Fetched 154 kB in 1s (123 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 145256 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.62.0-2ubuntu2.10_amd64.deb ...\n",
            "Unpacking poppler-utils (0.62.0-2ubuntu2.10) ...\n",
            "Setting up poppler-utils (0.62.0-2ubuntu2.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTxlNqFnYxLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09aa8ec5-ec30-447b-86a2-4db049eb0c69"
      },
      "source": [
        "!sudo apt install tesseract-ocr "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 3s (1,777 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 145284 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggw58U4ptf_5"
      },
      "source": [
        "###Proceso para ejecutar la conversion de archivos pdf a imagenes y guradarlos en ruta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM1Qd_qtsin-"
      },
      "source": [
        "from PIL import Image\n",
        "import pytesseract\n",
        "import os\n",
        "import sys\n",
        "from pdf2image import convert_from_path, convert_from_bytes, pdfinfo_from_path\n",
        "from shutil import copyfile, copy"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "HdZ45QogrHqy",
        "outputId": "47201fb9-ce7a-44e9-b9d6-973ff5835cc5"
      },
      "source": [
        "# Carpetas donde se encuentran las sentencias, imagenes y txt respectivamente\n",
        "dir_sent = '/content/drive/MyDrive/Colab Notebooks/saturday ia/justicia abierta/0 versiones finales/'\n",
        "\n",
        "dir_imag = \"/content/drive/MyDrive/Colab Notebooks/saturday ia/justicia abierta/imagenes_pdf/\"\n",
        "\n",
        "dir_txt = \"/content/drive/MyDrive/Colab Notebooks/saturday ia/justicia abierta/sentencias_txt/\"\n",
        "\n",
        "# Se crea una instancia de la carpeta\n",
        "local_download_path = os.path.expanduser(dir_sent)\n",
        "\n",
        "# Se itera sobre la carpeta\n",
        "for file in os.listdir(local_download_path):\n",
        "  if not file[6] == \"$\" and file.endswith(\"pdf\"):\n",
        "    print(\"Processing file: \" + file)\n",
        "\n",
        "    filename = dir_sent + file\n",
        "\n",
        "    pages = convert_from_bytes(open(filename, \"rb\").read(), dpi=500, fmt=\"jpeg\")\n",
        "\n",
        "    image_counter = 1\n",
        "    # Convertimos a imagenes\n",
        "    for page in pages:\n",
        "      filename = dir_imag + file[0:3] +\"_page_\"+str(image_counter)+\".jpg\"\n",
        "      page.save(filename, 'JPEG') \n",
        "      image_counter += 1\n",
        "\n",
        "    filelimit = image_counter-1\n",
        "\n",
        "    outfile = dir_txt + file[0:3] + \".txt\"\n",
        "\n",
        "    f = open(outfile, \"a\")\n",
        "\n",
        "    # Extraemos el texto y guardamos\n",
        "    for i in range(1, filelimit +1):\n",
        "      filename = dir_imag + file[0:3] +\"_page_\"+str(i)+\".jpg\"\n",
        "      text = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
        "      text = text.replace('-\\n', '')\n",
        "      f.write(text) \n",
        "\n",
        "    f.close()\n",
        "    \n",
        "    # Renombramos el archivo con $ para distinguir\n",
        "\n",
        "    new_name = file[0:5] + \" $ \" + file[6:]\n",
        "\n",
        "    os.rename(dir_sent + file, dir_sent + new_name)\n",
        "    print(\"The file \" + file + \"has finished\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing file: 020 a Sent. 77-2017 APIS.pdf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-180981565882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_sent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"jpeg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mimage_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mconvert_from_bytes\u001b[0;34m(pdf_file, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mpaths_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpaths_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0muse_pdftocairo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_pdftocairo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m             )\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj3s5EqFynjm"
      },
      "source": [
        "### Proceso para convertir archivos en pdf en imagenes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xu03H1111P8"
      },
      "source": [
        "local_download_path = os.path.expanduser('/content/drive/My Drive/SaturdaysAI/Sentencias')\n",
        "\n",
        "for file in os.listdir(local_download_path):\n",
        "  if file.endswith(\"pdf\"):    \n",
        "    \n",
        "    filelimit = image_counter-1\n",
        "\n",
        "    outfile = \"/content/drive/My Drive/SaturdaysAI/txt/\" + file[0:3] + \".txt\"\n",
        "\n",
        "    f = open(outfile, \"a\")\n",
        "\n",
        "    for i in range(1, filelimit +1):\n",
        "      filename = \"/content/drive/My Drive/SaturdaysAI/imagenes/\" + file[0:3] +\"_page_\"+str(i)+\".jpg\"\n",
        "      text = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
        "      text = text.replace('-\\n', '')\n",
        "      f.write(text) \n",
        "\n",
        "    print(\"The file\" + file[0:3]+ \"finish successfully\")\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqIkJkqJMpCe"
      },
      "source": [
        "**Instalar librerias para procesar formatos de *microsfot world* a txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8dNdhcYMbBn",
        "outputId": "3f274266-ff5e-4610-f79a-b02509273af6"
      },
      "source": [
        "!pip install docx2txt\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp36-none-any.whl size=3965 sha256=893a38dc1c44de558fbbb7b2f288810359e82d8071c51fec0e684e3ef3bbdd94\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf2e8ALbOOVL"
      },
      "source": [
        "import docx2txt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO7QlomcOT98"
      },
      "source": [
        "ruta_origen=\"/content/drive/My Drive/0_versiones_finales/\"\n",
        "ruta_salida_nueva='/content/drive/My Drive/sentencias_txt/'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud-7xBlAsXWq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "57625b4d-25ee-4168-9332-7569f43b2674"
      },
      "source": [
        "local_download_path = os.path.expanduser(ruta_origen)\n",
        "local_download_path\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/0_versiones_finales/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbS9i4K0Ol8k"
      },
      "source": [
        "### Proceso para convertir documentos en formato .docx o .doc a   txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGwk49EQMSQ8",
        "outputId": "52e6881a-0b6d-46a9-d551-3427c43a982a"
      },
      "source": [
        "print (local_download_path)\n",
        "fs=[]\n",
        "for file in os.listdir(local_download_path):\n",
        "   \n",
        "    if (file.endswith(\"docx\") or file.endswith(\"doc\")) and file[4]=='a':\n",
        "      try:\n",
        "        filename= ruta_origen+file\n",
        "        #print (filename)\n",
        "        result = docx2txt.process(filename)\n",
        "        archivo_salida=ruta_salida_nueva+file[0:3]+\".txt\"\n",
        "        #print (archivo_salida)\n",
        "        f = open(archivo_salida,\"a\")\n",
        "        f.write(result)\n",
        "        print (\" archivo {} se ha procesado de forma exitosa\".format(file[0:3]+\".txt\"))\n",
        "      except:\n",
        "        print(\"Fallo en procesar {}\".format(file[0:3]+\".txt\")) \n",
        "     "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/0_versiones_finales/\n",
            " archivo 004.txt se ha procesado de forma exitosa\n",
            " archivo 005.txt se ha procesado de forma exitosa\n",
            " archivo 006.txt se ha procesado de forma exitosa\n",
            " archivo 007.txt se ha procesado de forma exitosa\n",
            " archivo 012.txt se ha procesado de forma exitosa\n",
            " archivo 014.txt se ha procesado de forma exitosa\n",
            " archivo 015.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 016.txt\n",
            " archivo 017.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 023.txt\n",
            " archivo 025.txt se ha procesado de forma exitosa\n",
            " archivo 027.txt se ha procesado de forma exitosa\n",
            " archivo 022.txt se ha procesado de forma exitosa\n",
            " archivo 024.txt se ha procesado de forma exitosa\n",
            " archivo 031.txt se ha procesado de forma exitosa\n",
            " archivo 026.txt se ha procesado de forma exitosa\n",
            " archivo 028.txt se ha procesado de forma exitosa\n",
            " archivo 032.txt se ha procesado de forma exitosa\n",
            " archivo 034.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 037.txt\n",
            " archivo 043.txt se ha procesado de forma exitosa\n",
            " archivo 040.txt se ha procesado de forma exitosa\n",
            " archivo 042.txt se ha procesado de forma exitosa\n",
            " archivo 044.txt se ha procesado de forma exitosa\n",
            " archivo 052.txt se ha procesado de forma exitosa\n",
            " archivo 053.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 054.txt\n",
            " archivo 056.txt se ha procesado de forma exitosa\n",
            " archivo 049.txt se ha procesado de forma exitosa\n",
            " archivo 070.txt se ha procesado de forma exitosa\n",
            " archivo 072.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 078.txt\n",
            "Fallo en procesar 084.txt\n",
            " archivo 079.txt se ha procesado de forma exitosa\n",
            " archivo 089.txt se ha procesado de forma exitosa\n",
            " archivo 090.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 085.txt\n",
            " archivo 091.txt se ha procesado de forma exitosa\n",
            " archivo 088.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 096.txt\n",
            "Fallo en procesar 094.txt\n",
            " archivo 099.txt se ha procesado de forma exitosa\n",
            " archivo 095.txt se ha procesado de forma exitosa\n",
            " archivo 101.txt se ha procesado de forma exitosa\n",
            " archivo 103.txt se ha procesado de forma exitosa\n",
            " archivo 097.txt se ha procesado de forma exitosa\n",
            " archivo 100.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 106.txt\n",
            " archivo 107.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 117.txt\n",
            "Fallo en procesar 118.txt\n",
            " archivo 114.txt se ha procesado de forma exitosa\n",
            " archivo 127.txt se ha procesado de forma exitosa\n",
            " archivo 123.txt se ha procesado de forma exitosa\n",
            " archivo 128.txt se ha procesado de forma exitosa\n",
            "Fallo en procesar 129.txt\n",
            " archivo 136.txt se ha procesado de forma exitosa\n",
            " archivo 137.txt se ha procesado de forma exitosa\n",
            " archivo 139.txt se ha procesado de forma exitosa\n",
            " archivo 140.txt se ha procesado de forma exitosa\n",
            " archivo 135.txt se ha procesado de forma exitosa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BopGL_imHaHd"
      },
      "source": [
        "##Procesamiento para limpar y aplicar filtros de setencias.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX7zF9wmiYcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db92f172-f850-4713-f058-8bd111ad616f"
      },
      "source": [
        "procesa_setencias()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/saturday ia/justicia abierta/0 versiones finales/ /content/drive/My Drive/sentencias_txt/ /content/drive/My Drive/Colab Notebooks/saturday ia/justicia abierta/archivos_limpios/\n",
            "/content/drive/My Drive/sentencias_txt/\n",
            "/content/drive/My Drive/sentencias_txt/001.txt\n",
            "/content/drive/My Drive/sentencias_txt/008.txt\n",
            "/content/drive/My Drive/sentencias_txt/009.txt\n",
            "/content/drive/My Drive/sentencias_txt/013.txt\n",
            "/content/drive/My Drive/sentencias_txt/003.txt\n",
            "/content/drive/My Drive/sentencias_txt/018.txt\n",
            "/content/drive/My Drive/sentencias_txt/010.txt\n",
            "/content/drive/My Drive/sentencias_txt/011.txt\n",
            "/content/drive/My Drive/sentencias_txt/020.txt\n",
            "/content/drive/My Drive/sentencias_txt/019.txt\n",
            "/content/drive/My Drive/sentencias_txt/021.txt\n",
            "/content/drive/My Drive/sentencias_txt/029.txt\n",
            "/content/drive/My Drive/sentencias_txt/030.txt\n",
            "/content/drive/My Drive/sentencias_txt/033.txt\n",
            "/content/drive/My Drive/sentencias_txt/036.txt\n",
            "/content/drive/My Drive/sentencias_txt/035.txt\n",
            "/content/drive/My Drive/sentencias_txt/038.txt\n",
            "/content/drive/My Drive/sentencias_txt/039.txt\n",
            "/content/drive/My Drive/sentencias_txt/041.txt\n",
            "/content/drive/My Drive/sentencias_txt/045.txt\n",
            "/content/drive/My Drive/sentencias_txt/050.txt\n",
            "/content/drive/My Drive/sentencias_txt/046.txt\n",
            "/content/drive/My Drive/sentencias_txt/058.txt\n",
            "/content/drive/My Drive/sentencias_txt/047.txt\n",
            "/content/drive/My Drive/sentencias_txt/048.txt\n",
            "/content/drive/My Drive/sentencias_txt/055.txt\n",
            "/content/drive/My Drive/sentencias_txt/057.txt\n",
            "/content/drive/My Drive/sentencias_txt/059.txt\n",
            "/content/drive/My Drive/sentencias_txt/060.txt\n",
            "/content/drive/My Drive/sentencias_txt/064.txt\n",
            "/content/drive/My Drive/sentencias_txt/065.txt\n",
            "/content/drive/My Drive/sentencias_txt/066.txt\n",
            "/content/drive/My Drive/sentencias_txt/061.txt\n",
            "/content/drive/My Drive/sentencias_txt/068.txt\n",
            "/content/drive/My Drive/sentencias_txt/069.txt\n",
            "/content/drive/My Drive/sentencias_txt/071.txt\n",
            "/content/drive/My Drive/sentencias_txt/073.txt\n",
            "/content/drive/My Drive/sentencias_txt/074.txt\n",
            "/content/drive/My Drive/sentencias_txt/063.txt\n",
            "/content/drive/My Drive/sentencias_txt/067.txt\n",
            "/content/drive/My Drive/sentencias_txt/075.txt\n",
            "/content/drive/My Drive/sentencias_txt/080.txt\n",
            "/content/drive/My Drive/sentencias_txt/076.txt\n",
            "/content/drive/My Drive/sentencias_txt/081.txt\n",
            "/content/drive/My Drive/sentencias_txt/083.txt\n",
            "/content/drive/My Drive/sentencias_txt/082.txt\n",
            "/content/drive/My Drive/sentencias_txt/087.txt\n",
            "/content/drive/My Drive/sentencias_txt/093.txt\n",
            "/content/drive/My Drive/sentencias_txt/092.txt\n",
            "/content/drive/My Drive/sentencias_txt/098.txt\n",
            "/content/drive/My Drive/sentencias_txt/104.txt\n",
            "/content/drive/My Drive/sentencias_txt/105.txt\n",
            "/content/drive/My Drive/sentencias_txt/108.txt\n",
            "/content/drive/My Drive/sentencias_txt/102.txt\n",
            "/content/drive/My Drive/sentencias_txt/110.txt\n",
            "/content/drive/My Drive/sentencias_txt/113.txt\n",
            "/content/drive/My Drive/sentencias_txt/115.txt\n",
            "/content/drive/My Drive/sentencias_txt/111.txt\n",
            "/content/drive/My Drive/sentencias_txt/119.txt\n",
            "/content/drive/My Drive/sentencias_txt/121.txt\n",
            "/content/drive/My Drive/sentencias_txt/116.txt\n",
            "/content/drive/My Drive/sentencias_txt/120.txt\n",
            "/content/drive/My Drive/sentencias_txt/124.txt\n",
            "/content/drive/My Drive/sentencias_txt/122.txt\n",
            "/content/drive/My Drive/sentencias_txt/125.txt\n",
            "/content/drive/My Drive/sentencias_txt/126.txt\n",
            "/content/drive/My Drive/sentencias_txt/130.txt\n",
            "/content/drive/My Drive/sentencias_txt/132.txt\n",
            "/content/drive/My Drive/sentencias_txt/131.txt\n",
            "/content/drive/My Drive/sentencias_txt/138.txt\n",
            "/content/drive/My Drive/sentencias_txt/133.txt\n",
            "/content/drive/My Drive/sentencias_txt/142.txt\n",
            "/content/drive/My Drive/sentencias_txt/141.txt\n",
            "/content/drive/My Drive/sentencias_txt/143.txt\n",
            "/content/drive/My Drive/sentencias_txt/051.txt\n",
            "/content/drive/My Drive/sentencias_txt/086.txt\n",
            "/content/drive/My Drive/sentencias_txt/062.txt\n",
            "/content/drive/My Drive/sentencias_txt/004.txt\n",
            "/content/drive/My Drive/sentencias_txt/005.txt\n",
            "/content/drive/My Drive/sentencias_txt/006.txt\n",
            "/content/drive/My Drive/sentencias_txt/007.txt\n",
            "/content/drive/My Drive/sentencias_txt/012.txt\n",
            "/content/drive/My Drive/sentencias_txt/014.txt\n",
            "/content/drive/My Drive/sentencias_txt/015.txt\n",
            "/content/drive/My Drive/sentencias_txt/017.txt\n",
            "/content/drive/My Drive/sentencias_txt/025.txt\n",
            "/content/drive/My Drive/sentencias_txt/027.txt\n",
            "/content/drive/My Drive/sentencias_txt/022.txt\n",
            "/content/drive/My Drive/sentencias_txt/024.txt\n",
            "/content/drive/My Drive/sentencias_txt/031.txt\n",
            "/content/drive/My Drive/sentencias_txt/026.txt\n",
            "/content/drive/My Drive/sentencias_txt/028.txt\n",
            "/content/drive/My Drive/sentencias_txt/032.txt\n",
            "/content/drive/My Drive/sentencias_txt/034.txt\n",
            "/content/drive/My Drive/sentencias_txt/043.txt\n",
            "/content/drive/My Drive/sentencias_txt/040.txt\n",
            "/content/drive/My Drive/sentencias_txt/042.txt\n",
            "/content/drive/My Drive/sentencias_txt/044.txt\n",
            "/content/drive/My Drive/sentencias_txt/052.txt\n",
            "/content/drive/My Drive/sentencias_txt/053.txt\n",
            "/content/drive/My Drive/sentencias_txt/056.txt\n",
            "/content/drive/My Drive/sentencias_txt/049.txt\n",
            "/content/drive/My Drive/sentencias_txt/070.txt\n",
            "/content/drive/My Drive/sentencias_txt/072.txt\n",
            "/content/drive/My Drive/sentencias_txt/079.txt\n",
            "/content/drive/My Drive/sentencias_txt/089.txt\n",
            "/content/drive/My Drive/sentencias_txt/090.txt\n",
            "/content/drive/My Drive/sentencias_txt/091.txt\n",
            "/content/drive/My Drive/sentencias_txt/088.txt\n",
            "/content/drive/My Drive/sentencias_txt/099.txt\n",
            "/content/drive/My Drive/sentencias_txt/095.txt\n",
            "/content/drive/My Drive/sentencias_txt/101.txt\n",
            "/content/drive/My Drive/sentencias_txt/103.txt\n",
            "/content/drive/My Drive/sentencias_txt/097.txt\n",
            "/content/drive/My Drive/sentencias_txt/100.txt\n",
            "/content/drive/My Drive/sentencias_txt/107.txt\n",
            "/content/drive/My Drive/sentencias_txt/114.txt\n",
            "/content/drive/My Drive/sentencias_txt/127.txt\n",
            "/content/drive/My Drive/sentencias_txt/123.txt\n",
            "/content/drive/My Drive/sentencias_txt/128.txt\n",
            "/content/drive/My Drive/sentencias_txt/136.txt\n",
            "/content/drive/My Drive/sentencias_txt/137.txt\n",
            "/content/drive/My Drive/sentencias_txt/139.txt\n",
            "/content/drive/My Drive/sentencias_txt/140.txt\n",
            "/content/drive/My Drive/sentencias_txt/135.txt\n",
            "/content/drive/My Drive/sentencias_txt/001.gdoc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2142fd7f011a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocesa_setencias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-63cd81cfcbdb>\u001b[0m in \u001b[0;36mprocesa_setencias\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mruta_filtros\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab Notebooks/saturday ia/justicia abierta/archivos_limpios/'\u001b[0m \u001b[0;31m#path con sentencias archivos aplicando filtros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mruta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruta_salida\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruta_filtros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mlimpiar_sentencias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_salida\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruta_filtros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-64ada5f65a39>\u001b[0m in \u001b[0;36mlimpiar_sentencias\u001b[0;34m(ruta_txt, ruta_filtro)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_download_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mruta_txt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_txt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msentencia\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 95] Operation not supported: '/content/drive/My Drive/sentencias_txt/001.gdoc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t_kapZa9zB1"
      },
      "source": [
        "def procesa_setencias():\n",
        "  ruta=\"/content/drive/My Drive/Colab Notebooks/saturday ia/justicia abierta/0 versiones finales/\" #path con sentencias originales\n",
        "  ruta_salida = \"/content/drive/My Drive/sentencias_txt/\" #path con  sentencias en txt\n",
        "  ruta_filtros='/content/drive/My Drive/Colab Notebooks/saturday ia/justicia abierta/archivos_limpios/' #path con sentencias archivos aplicando filtros\n",
        "  print (ruta,ruta_salida,ruta_filtros)\n",
        "  limpiar_sentencias(ruta_salida,ruta_filtros)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfVszy-1dUhA"
      },
      "source": [
        "def limpiar_sentencias(ruta_txt,ruta_filtro):\n",
        "  local_download_path = os.path.expanduser(ruta_txt)\n",
        "  print (local_download_path)\n",
        "  for file in os.listdir(local_download_path):\n",
        "    print (ruta_txt+file)\n",
        "    f=open(ruta_txt+file)    \n",
        "    sentencia=f.read()\n",
        "   \n",
        "    f.close()\n",
        "    f2=open(ruta_filtro+file,\"w\") # crear archivo donde se guardan las setencias\n",
        "    word_tokens= word_tokenize(sentencia)\n",
        "    #word_tokens= suggestions(word_tokens)\n",
        "    word_tokens = list(filter (lambda token: filtro_1))\n",
        "    filtro=[]\n",
        "\n",
        "    for palabra in word_tokens:\n",
        "      if palabra.lower() not in stop_words:\n",
        "        filtro.append(palabra.lower())\n",
        "        f2.write(palabra.lower())\n",
        "        f2.write(\" \")\n",
        "    word_tokens=[]\n",
        "    f2.close()  \n",
        "    archivo_ranking=ruta_filtro+\"rank\"+file\n",
        "    guardar_ranking(filtro,archivo_ranking)\n",
        "  \n",
        "\n",
        "    \n",
        "    #todo guardar pandas dataset"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xitUzTMqhOCL"
      },
      "source": [
        "def guardar_ranking(filtro,archivo_ranking):\n",
        "  c= Counter(filtro)\n",
        "  y=OrderedDict(c.most_common(200))\n",
        "  with open(archivo_ranking,'w') as f:\n",
        "    for k,v in y.items():\n",
        "      f.write(f'{k}: {v} \\n')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yWllU9qW5jy"
      },
      "source": [
        "###Funcion para aplicar sugerencias con palabras que no tienen significado para la setencias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZGN73n76SbL"
      },
      "source": [
        "\n",
        "def suggestions(words, list_word=[]):\n",
        "  for i in range(0, len(words)):\n",
        "    if d.check(words[i]) :\n",
        "      list_word.append(words[i])\n",
        "    else:\n",
        "      if len(words[i]) > 1 and (len(d.suggest(words[i])) != 0 ):\n",
        "        # print(i)\n",
        "        list_word.append(d.suggest(words[i])[0])\n",
        "\n",
        "  return list_word"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzwx8qZeQTS0"
      },
      "source": [
        "## Procesamieanto para generar dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSZE__V3twWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b7df92-e190-4de5-a2a9-15540d92c012"
      },
      "source": [
        "ruta_filtros='/content/drive/My Drive/Colab Notebooks/saturday ia/justicia abierta/archivos_limpios/' #path con sentencias archivos aplicando filtros\n",
        "\n",
        "import pandas\n",
        "\n",
        "excel_data_df = pandas.read_excel('/content/drive/My Drive/Colab Notebooks/saturday ia/justicia abierta/result_143_sentencias.xlsx', sheet_name='BASE COMPLETA',\n",
        "                                  usecols=['Índice','Nombre','ASUNTO/TEMA','INSTANCIA','PUNTUACIÓN'], index_col=0)\n",
        "print(excel_data_df.shape)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(143, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ZvJwWJI8QfTI",
        "outputId": "83e57894-a03d-460b-ff0a-e0ea0f1dd9d2"
      },
      "source": [
        "excel_data_df.describe()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PUNTUACIÓN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>143.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.673846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.552542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.620000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.400000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PUNTUACIÓN\n",
              "count  143.000000\n",
              "mean     1.673846\n",
              "std      1.552542\n",
              "min      0.000000\n",
              "25%      0.620000\n",
              "50%      1.080000\n",
              "75%      2.000000\n",
              "max      7.400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrhQjFXUBwv"
      },
      "source": [
        "## Genereacion del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwRg46U6UFsH"
      },
      "source": [
        "## Generamos el dataset  con las salidad de las setencias que se le aplico\n",
        "\n",
        "\n",
        "*   Eliminacion de stop worlds,\n",
        "*   Eliminanciòn de puntaciones.\n",
        "*   Eliminacion de ruido.\n",
        "*   Eliminacion de  palabras sin relevancia para \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o4XHUMB6WDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215a17a2-5256-4919-c97d-d40f8037df5a"
      },
      "source": [
        "corpus=[]\n",
        "local_download_path = os.path.expanduser(ruta_filtros)\n",
        "for file in os.listdir(local_download_path):\n",
        "  if (file[0:3]!=\"ran\" and file.endswith(\"txt\")):\n",
        "   \n",
        "    filename= ruta_filtros+file\n",
        "    f=open(filename)\n",
        "    #print (filename)\n",
        "    corpus_alpha=f.read()\n",
        "    excel_data_df.at[int(file[0:3]),'CORPUS']=corpus_alpha\n",
        "\n",
        "print(len(corpus))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVRBcEbHGapK"
      },
      "source": [
        "excel_data_df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgEBMYc6Cztc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507c4724-80a4-4857-d8ce-9450eddefa35"
      },
      "source": [
        "excel_data_df[\"PUNTUACIÓN\"][10:15]>1.08"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Índice\n",
              "11    False\n",
              "12     True\n",
              "13    False\n",
              "14    False\n",
              "15    False\n",
              "Name: PUNTUACIÓN, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2iddyKTVhji"
      },
      "source": [
        "#### Generar nueva columna para tener clasificaciòn binaria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkslz0rJLelf"
      },
      "source": [
        "excel_data_df[\"score\"] = excel_data_df[\"PUNTUACIÓN\"].map(lambda x :1 if (x >1.08) else 0)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMDgMiYN9JpP"
      },
      "source": [
        "excel_data_df.to_csv (r'/content/drive/My Drive/Colab Notebooks/saturday ia/justicia abierta/csv/sentencias_dataframe.csv', index = False, header=True)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI8knCS907nW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b04ee2-6a18-430e-9b86-a29a2ad9f558"
      },
      "source": [
        "\n",
        "excel_data_df.to_csv (r'/content/drive/My Drive/Colab Notebooks/saturday ia/justicia abierta/csv/sentencias_dataframe.csv', index = False, header=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Índice  ...                      INSTANCIA\n",
            "0         1  ...              Primera Instancia\n",
            "1         2  ...              Primera instancia\n",
            "2         3  ...              Primera Instancia\n",
            "3         4  ...              Segunda Instancia\n",
            "4         5  ...  Segunda Instancia (apelación)\n",
            "..      ...  ...                            ...\n",
            "138     139  ...              Primera Instancia\n",
            "139     140  ...              Primera Instancia\n",
            "140     141  ...              Primera instancia\n",
            "141     142  ...              Primera instancia\n",
            "142     143  ...              Primera Instancia\n",
            "\n",
            "[143 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}