{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk \n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import spacy\n",
    "import pyLDAvis.gensim_models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer as lemma\n",
    "import pickle\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from dictionaries import topic_word_dictionary\n",
    "texts = pd.read_json('file.json',lines=True)\n",
    "import diccionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = texts[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-TRAINED MODEL : FASTTEXT WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "#fasttext.util.download_model('es', if_exists='ignore')  # Spanish\n",
    "ft = fasttext.load_model('cc.es.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM OF THE DOCUMENT\n",
    "The aim of this document is to implement simple topic modeling algorithms based on the results obtained from the Latent Dirichlet Allocation (LDA) proposal. During the LDA analysis, it was observed that a range of 8 to 10 topics seemed to be reasonably effective. Therefore, the objective here is to build upon those insights and develop straightforward topic modeling techniques that align with the identified number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(token_list):\n",
    "    \"\"\"\n",
    "\n",
    "    Calculate the frequency of each word in the given list of tokens.\n",
    "\n",
    "    Parameters:\n",
    "        token_list (list): A list of tokens representing words.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are words and values are their frequencies.\n",
    "\n",
    "    \"\"\"\n",
    "    word_freq = {}\n",
    "    for token in token_list:\n",
    "        if token in word_freq:\n",
    "            word_freq[token] += 1\n",
    "        else:\n",
    "            word_freq[token] = 1\n",
    "    return word_freq\n",
    "\n",
    "def top_words(word_freq_dict, n=10):\n",
    "    \"\"\"\n",
    "    Get the top 'n' words with the highest frequencies from a word frequency dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        word_freq_dict (dict): A dictionary where keys are words and values are their frequencies.\n",
    "        n (int): Number of top words to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the top 'n' words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    # Ordenar el diccionario por valor (frecuencia) en orden descendente\n",
    "    sorted_word_freq = sorted(word_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Obtener las primeras 'n' palabras\n",
    "    top_n_words = [word for word, freq in sorted_word_freq[:n]]\n",
    "    \n",
    "    return top_n_words\n",
    "\n",
    "\n",
    "def get_word_embeddings(word_list, model_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Get word embeddings for a list of words using FastText.\n",
    "\n",
    "    Parameters:\n",
    "        word_list (list): List of words for which embeddings are required.\n",
    "        model_path (str): Path to the FastText model file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping words to their corresponding word embeddings.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load FastText model\n",
    "    ft_model = fasttext.load_model(model_path)\n",
    "\n",
    "    # Initialize an empty dictionary to store word embeddings\n",
    "    embeddings = {}\n",
    "\n",
    "    # Iterate through each word in the word list\n",
    "    for word in word_list:\n",
    "        # Get the word embedding vector for the current word\n",
    "        embedding_vector = ft_model.get_word_vector(word)\n",
    "        \n",
    "        # Store the word embedding vector in the embeddings dictionary\n",
    "        embeddings[word] = embedding_vector\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def average_distance(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Calculate the average cosine distance between corresponding word embeddings in two dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings1 (dict): Dictionary mapping words to their embeddings.\n",
    "        embeddings2 (dict): Dictionary mapping words to their embeddings.\n",
    "\n",
    "    Returns:\n",
    "        float: The average cosine distance.\n",
    "    \"\"\"\n",
    "    # Extract vectors from dictionaries\n",
    "    vectors1 = list(embeddings1.values())\n",
    "    vectors2 = list(embeddings2.values())\n",
    "    \n",
    "    # Calculate pairwise cosine distances\n",
    "    distances = cosine_distances(vectors1, vectors2)\n",
    "    \n",
    "    # Calculate the average distance\n",
    "    avg_distance = np.mean(distances)\n",
    "    \n",
    "    return avg_distance\n",
    "\n",
    "def classify(embeddings_words, embeddings_topics, original_topic_dictionary):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of distances to the topics, and returns the topic with the least cosine distance.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings_words (dict): Dictionary mapping words to their embeddings.\n",
    "        embeddings_topics (dict): Dictionary mapping topics to their embeddings.\n",
    "        original_topic_dictionary (dict): Original mapping of topic indices to topic names.\n",
    "\n",
    "    Returns:\n",
    "        string : The closest topic.\n",
    "        dict : A dictionary mapping topic names to their distances to words.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store distances to topics\n",
    "    topic_distances = {}\n",
    "\n",
    "    # Iterate through each topic embedding\n",
    "    for topic_index, topic_embedding in embeddings_topics.items():\n",
    "        # Calculate the average cosine distance between word embeddings in embeddings_words and the topic embedding\n",
    "        avg_distance = average_distance(embeddings_words, {0: topic_embedding})\n",
    "        # Get the topic name from the original dictionary\n",
    "        topic_name = original_topic_dictionary.get(topic_index)\n",
    "        # Store the average distance for the topic name\n",
    "        topic_distances[topic_name] = avg_distance\n",
    "\n",
    "    # Find the topic with the smallest average distance\n",
    "    min_distance_topic = min(topic_distances, key=topic_distances.get)\n",
    "    \n",
    "    return min_distance_topic, topic_distances\n",
    "\n",
    "\n",
    "def topic_clusterer(articles_column,cosinedistance_dictionary) : \n",
    "    \n",
    "    \"\"\" \n",
    "    The aim of this function is to predict the topic of an article. Preprocessing is done before classification.\n",
    "    Input : \n",
    "        articles_columns : Df column containing articles\n",
    "        word_dictionary  : diccionary of the cosine distance of the words related to the topics\n",
    "    Output : \n",
    "        Dataframe containing the topic predicted as well as the distances to other topics.\n",
    "\n",
    "    \"\"\"\n",
    "    # Our spaCy model:\n",
    "    nlp = spacy.load(\"es_core_news_lg\")\n",
    "    # Path to the FastText model file\n",
    "    model_path = 'cc.es.300.bin'\n",
    "\n",
    "    # Tags I want to remove from the text\n",
    "    removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "    tokens = []\n",
    "    for summary in nlp.pipe(articles_column):\n",
    "        proj_tok = [token.lemma_.lower() for token in summary if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
    "        tokens.append(proj_tok)\n",
    "\n",
    "    texts = pd.DataFrame()\n",
    "    texts['Tokens'] = tokens\n",
    "    texts['Word_Frequencies'] = texts['Tokens'].apply(word_freq)\n",
    "    texts['Top_n_words']      = texts['Word_Frequencies'].apply(top_words)\n",
    "    texts['embeddings']       = texts['Top_n_words'].apply(get_word_embeddings, args=(model_path,))\n",
    "    texts['predicted_topic'], _ = zip(*texts['embeddings'].apply(classify,args=(embedding_topics,original_dict))) # embedding_topics ser치 un diccionario de los word embeddings a los topics, que guardaremos en un pickle, y original_dict ser치 el diccionario que contenga el nombre de los temas originales\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Code\n",
    "Here are examples demonstrating the usage of the functions described earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Tokens                         Word_Frequencies\n",
      "0   [this, is, a, test]  {'this': 1, 'is': 1, 'a': 1, 'test': 1}\n",
      "1       [another, test]                {'another': 1, 'test': 1}\n",
      "2  [yet, another, test]      {'yet': 1, 'another': 1, 'test': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Supongamos que tienes un DataFrame llamado df y quieres aplicar la funci칩n word_freq a cada fila de la columna 'Tokens'\n",
    "# Creamos un DataFrame de ejemplo\n",
    "data = {'Tokens': [['this', 'is', 'a', 'test'], ['another', 'test'], ['yet', 'another', 'test']]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Aplicar la funci칩n word_freq a cada fila de la columna 'Tokens'\n",
    "df['Word_Frequencies'] = df['Tokens'].apply(word_freq)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'programming', 'language', 'hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "word_frequencies = {'hello': 5, 'world': 3, 'python': 10, 'programming': 8, 'language': 7}\n",
    "top_10_words = top_words(word_frequencies)\n",
    "print(top_10_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "\n",
    "# List of words for which embeddings are required\n",
    "word_list = [\"guerra,ucrania,misil,muertos,rusia\"]\n",
    "word_list_futbol = top_words(word_freq(topic_clusterer(df111.loc[3, \"Tokens\"],dict())))\n",
    "\n",
    "\n",
    "# Path to the FastText model file\n",
    "model_path = 'cc.es.300.bin'\n",
    "\n",
    "# Get word embeddings for the word list using FastText\n",
    "word_embeddings =  get_word_embeddings(word_list_futbol , model_path)\n",
    "\n",
    "word_embeddings_dict = {}\n",
    "for i in range(10):  # Assuming you have 10 topics\n",
    "    word_embeddings_dict[i] = get_word_embeddings(topic_word_dictionary[i], model_path)\n",
    "\n",
    "\n",
    "# Print the word embeddings\n",
    "#for word, embedding in word_embeddings2.items():\n",
    "#    print(word, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine distance with topic 0 : 0.8231431\n",
      "Average cosine distance with topic 1 : 0.82431126\n",
      "Average cosine distance with topic 2 : 0.9571792\n",
      "Average cosine distance with topic 3 : 0.8269824\n",
      "Average cosine distance with topic 4 : 0.83639395\n",
      "Average cosine distance with topic 5 : 0.8159949\n",
      "Average cosine distance with topic 6 : 0.8134227\n",
      "Average cosine distance with topic 7 : 0.8082148\n",
      "Average cosine distance with topic 8 : 0.86202157\n",
      "Average cosine distance with topic 9 : 0.74208677\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "def average_distance(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Calculate the average cosine distance between corresponding word embeddings in two dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    embeddings1 (dict): Dictionary mapping words to their embeddings.\n",
    "    embeddings2 (dict): Dictionary mapping words to their embeddings.\n",
    "\n",
    "    Returns:\n",
    "    float: The average cosine distance.\n",
    "    \"\"\"\n",
    "    # Extract vectors from dictionaries\n",
    "    vectors1 = list(embeddings1.values())\n",
    "    vectors2 = list(embeddings2.values())\n",
    "    \n",
    "    # Calculate pairwise cosine distances\n",
    "    distances = cosine_distances(vectors1, vectors2)\n",
    "    \n",
    "    # Calculate the average distance\n",
    "    avg_distance = np.mean(distances)\n",
    "    \n",
    "    return avg_distance\n",
    "\n",
    "# Example usage:\n",
    "for i in range (10) : \n",
    "\n",
    "    # Calculate the average cosine distance\n",
    "    avg_distance = average_distance(word_embeddings_dict[i], word_embeddings)\n",
    "\n",
    "    print(f\"Average cosine distance with topic {i} :\", avg_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
