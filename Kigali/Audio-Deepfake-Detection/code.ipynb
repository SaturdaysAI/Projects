{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6358196,"sourceType":"datasetVersion","datasetId":3579787}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning for Audio Classification: Real vs. Fake Voice Detection\n\n## Project Overview\n\nThis project aims to develop a deep learning model capable of distinguishing between real and fake (deepfake) voice recordings. With the increasing sophistication of voice synthesis technologies, the ability to detect artificially generated audio has become crucial for maintaining trust in digital communications and media.\n\n## Data Sources\n\nThe project utilizes two main datasets:\n\n1. **Hugging Face Dataset**: A pre-processed dataset containing audio samples labeled as real or fake.\n   - Format: Raw audio data with associated labels\n   - Sampling Rate: 22050 Hz\n   - Labels: 0 for fake, 1 for real\n\n2. **Kaggle Dataset**: A collection of .wav files organized into 'REAL' and 'FAKE' folders.\n   - Format: WAV audio files\n   - Sampling Rates: Varied (44100 Hz and 48000 Hz observed)\n\n## Data Preprocessing\n\n### Audio Processing\n\n- **Resampling**: All audio is resampled to 22050 Hz for consistency\n- **MFCC Extraction**: Mel-frequency cepstral coefficients are computed for each audio sample\n- **Padding/Truncation**: MFCCs are padded or truncated to a fixed length (1000 time steps)\n\n### Dataset Classes\n\nTwo custom dataset classes were created to handle the different data sources:\n\n1. `AudioDatasetHuggingFace`: Processes the Hugging Face dataset\n2. `AudioDatasetKaggle`: Processes the Kaggle dataset\n\nBoth classes ensure consistent MFCC computation and output format.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T12:52:56.504804Z","iopub.execute_input":"2024-10-17T12:52:56.505236Z","iopub.status.idle":"2024-10-17T12:52:56.516026Z","shell.execute_reply.started":"2024-10-17T12:52:56.505197Z","shell.execute_reply":"2024-10-17T12:52:56.515014Z"}}},{"cell_type":"code","source":"# Imports and Configuration\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport numpy as np\nimport pandas as pd \nfrom tqdm import tqdm\nfrom datasets import load_dataset\nimport librosa\nimport os\n\n\n\nconfig = {\n    'epochs': 20,\n    'train_batch_size': 8,\n    'eval_batch_size': 8,\n    'learning_rate': 3e-5,\n    'weight_decay': 1e-8,\n    'gradient_accumulation_steps': 4,\n    'seed': 42,\n    'n_mfcc': 40,\n    'sr': 22050,\n    'max_length': 500,\n    'warmup_ratio': 0.1\n}\n\n# Set the seed for reproducibility\ntorch.manual_seed(config['seed'])\nnp.random.seed(config['seed'])\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Preprocessing for Kaggle dataset","metadata":{}},{"cell_type":"code","source":"class AudioDatasetKaggle(Dataset):\n    def __init__(self, data_source, config):\n        self.mfccs = []\n        self.labels = []\n        self.config = config\n        self.target_sr = config['sr']  # Target sampling rate\n        \n        if isinstance(data_source, str):  # It's a directory path\n            self._process_directory(data_source)\n        else:\n            raise ValueError(\"data_source must be a directory path for Kaggle dataset\")\n        \n        self.mfccs = np.array(self.mfccs)\n        self.labels = np.array(self.labels)\n        \n        print(f\"Loaded {len(self.labels)} audio files.\")\n        print(f\"Real: {np.sum(self.labels == 1)}, Fake: {np.sum(self.labels == 0)}\")\n        print(f\"MFCC shape: {self.mfccs[0].shape}\")\n\n    def _process_directory(self, root_dir):\n        for label in ['REAL', 'FAKE']:\n            folder_path = os.path.join(root_dir, 'KAGGLE', 'AUDIO', label)\n            for filename in tqdm(os.listdir(folder_path), desc=f\"Processing {label} audio\"):\n                if filename.endswith('.wav'):\n                    file_path = os.path.join(folder_path, filename)\n                    \n                    # Load audio file with original sampling rate\n                    audio, orig_sr = librosa.load(file_path, sr=None)\n                    \n                    # Resample if necessary\n                    if orig_sr != self.target_sr:\n                        audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=self.target_sr)\n                    \n                    # Compute MFCC\n                    mfcc = librosa.feature.mfcc(y=audio, sr=self.target_sr, n_mfcc=self.config['n_mfcc'])\n                    \n                    # Pad or truncate\n                    if mfcc.shape[1] < self.config['max_length']:\n                        pad_width = ((0, 0), (0, self.config['max_length'] - mfcc.shape[1]))\n                        mfcc = np.pad(mfcc, pad_width, mode='constant')\n                    else:\n                        mfcc = mfcc[:, :self.config['max_length']]\n                    \n                    self.mfccs.append(mfcc)\n                    self.labels.append(1 if label == 'REAL' else 0)  # 1 for REAL, 0 for FAKE\n\n    def __len__(self):\n        return len(self.mfccs)\n\n    def __getitem__(self, idx):\n        mfcc = torch.tensor(self.mfccs[idx], dtype=torch.float32)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return mfcc, label","metadata":{"execution":{"iopub.status.busy":"2024-10-17T12:52:56.530892Z","iopub.execute_input":"2024-10-17T12:52:56.531177Z","iopub.status.idle":"2024-10-17T12:52:56.546222Z","shell.execute_reply.started":"2024-10-17T12:52:56.531147Z","shell.execute_reply":"2024-10-17T12:52:56.545166Z"},"trusted":true},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":"### Data Preprocessing for Hugging Face (HF) dataset","metadata":{}},{"cell_type":"code","source":"\nclass AudioDatasetHuggingFace(Dataset):\n    def __init__(self, dataset, n_mfcc=40, max_length=500):\n        self.dataset = dataset\n        self.n_mfcc = n_mfcc\n        self.max_length = max_length\n        self.mfccs = []\n        self.labels = []\n        \n        for item in tqdm(self.dataset, desc=\"Processing audio\"):\n            audio = item['audio']['array']\n            sr = item['audio']['sampling_rate']\n            label = item['label']\n            \n            # Compute MFCC\n            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=self.n_mfcc)\n            \n            # Pad or truncate\n            if mfcc.shape[1] < self.max_length:\n                pad_width = ((0, 0), (0, self.max_length - mfcc.shape[1]))\n                mfcc = np.pad(mfcc, pad_width, mode='constant')\n            else:\n                mfcc = mfcc[:, :self.max_length]\n            \n            self.mfccs.append(mfcc)\n            self.labels.append(label)\n        \n        self.mfccs = np.array(self.mfccs)\n        self.labels = np.array(self.labels)\n        \n        print(f\"Loaded {len(self.labels)} audio files.\")\n        print(f\"Fake (0): {np.sum(self.labels == 0)}, Real (1): {np.sum(self.labels == 1)}\")\n        print(f\"MFCC shape: {self.mfccs[0].shape}\")\n\n    def __len__(self):\n        return len(self.mfccs)\n\n    def __getitem__(self, idx):\n        mfcc = torch.tensor(self.mfccs[idx], dtype=torch.float32)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return mfcc, label","metadata":{"execution":{"iopub.status.busy":"2024-10-17T12:52:56.547500Z","iopub.execute_input":"2024-10-17T12:52:56.547982Z","iopub.status.idle":"2024-10-17T12:52:56.561470Z","shell.execute_reply.started":"2024-10-17T12:52:56.547940Z","shell.execute_reply":"2024-10-17T12:52:56.560606Z"},"trusted":true},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":"# CNNNetwork Model Architecture\n\n## Overview\n\nThe `CNNNetwork` class defines a Convolutional Neural Network (CNN) designed for audio classification tasks. This model is specifically tailored to process Mel-frequency cepstral coefficients (MFCCs) of audio inputs and output a single value, suitable for binary classification tasks such as distinguishing between real and fake voice recordings.\n\n## Model Structure\n\n### Convolutional Layers\n\nThe model employs three convolutional layers, each followed by ReLU activation and max pooling:\n\n1. **Conv1**:\n   - Input channels: 1 (grayscale MFCC)\n   - Output channels: 32\n   - Kernel size: 3x3, Stride: 1, Padding: 1\n   - Max pooling: 2x2 with stride 2\n\n2. **Conv2**:\n   - Input channels: 32\n   - Output channels: 64\n   - Kernel size: 3x3, Stride: 1, Padding: 1\n   - Max pooling: 2x2 with stride 2\n\n3. **Conv3**:\n   - Input channels: 64\n   - Output channels: 128\n   - Kernel size: 3x3, Stride: 1, Padding: 1\n   - Max pooling: 2x2 with stride 2\n\n### Adaptive Pooling\n\nAn adaptive average pooling layer is used to ensure a fixed output size regardless of input dimensions:\n- Output size: 4x4\n\n### Fully Connected Layers\n\nThe model concludes with three fully connected layers:\n\n1. **FC1**: \n   - Input: 128 * 4 * 4 = 2048\n   - Output: 256\n   - Followed by ReLU and Dropout (0.5)\n\n2. **FC2**: \n   - Input: 256\n   - Output: 64\n   - Followed by ReLU and Dropout (0.5)\n\n3. **FC3** (Output layer): \n   - Input: 64\n   - Output: 1 (for binary classification)\n\n## Forward Pass\n\nThe `forward` method defines the data flow through the network:\n\n1. Input is unsqueezed to add a channel dimension\n2. Data passes through the three convolutional layers\n3. Adaptive pooling is applied to ensure consistent dimensionality\n4. The output is flattened\n5. The flattened tensor goes through the fully connected layers\n6. A single output value is produced\n\n## Design Considerations\n\n- Increasing channel depths (32, 64, 128) in convolutional layers capture increasingly complex features\n- Adaptive pooling allows for flexibility in input sizes\n- Dropout layers (0.5 probability) in fully connected layers prevent overfitting\n- The final output is a single value, suitable for binary classification tasks using a threshold or sigmoid activation\n\nThis architecture is optimized for processing MFCC representations of audio data, making it well-suited for tasks like distinguishing between real and synthetic voice recordings.","metadata":{}},{"cell_type":"code","source":"class CNNNetwork(nn.Module):\n    def __init__(self, num_mfcc):\n        super(CNNNetwork, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n        \n        self.fc = nn.Sequential(\n            nn.Linear(128 * 4 * 4, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.adaptive_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-17T12:52:56.563522Z","iopub.execute_input":"2024-10-17T12:52:56.564150Z","iopub.status.idle":"2024-10-17T12:52:56.577246Z","shell.execute_reply.started":"2024-10-17T12:52:56.564098Z","shell.execute_reply":"2024-10-17T12:52:56.576451Z"},"trusted":true},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":"# Data Loader Creation Function\n\n## Purpose\n\nThe `create_data_loaders` function is responsible for splitting a dataset into training, validation, and test sets, and creating PyTorch DataLoader objects for each. This function is crucial for preparing the data for model training and evaluation.\n\n## Function Parameters\n\n- `dataset`: The complete dataset to be split\n- `config`: A dictionary containing configuration parameters\n\n## Process\n\n1. **Dataset Splitting**:\n   - The dataset is split into three parts:\n     - Training set: 70% of the data\n     - Validation set: 15% of the data\n     - Test set: 15% of the data (remaining data)\n   - The `random_split` function is used to ensure randomness in the split\n   - A fixed seed is used for reproducibility\n\n2. **Data Loader Creation**:\n   - Three separate DataLoader objects are created for train, validation, and test sets\n   - Each DataLoader is configured with specific parameters:\n     - Batch sizes are set according to the config dictionary\n     - Training data is shuffled, while validation and test data are not\n     - `num_workers=2` is set for parallel data loading\n     - `pin_memory=True` is used for faster data transfer to GPU\n\n## Key Features\n\n- **Reproducibility**: The use of a fixed seed ensures that the dataset is split consistently across different runs\n- **Flexible Configuration**: Batch sizes and other parameters are controlled via the config dictionary\n- **Performance Optimization**: The use of multiple workers and pinned memory optimizes data loading performance\n\n## Output\n\nThe function returns three DataLoader objects:\n1. `train_loader`: For training the model\n2. `val_loader`: For validating the model during training\n3. `test_loader`: For final evaluation of the model\n\nThis setup allows for efficient training, validation, and testing cycles in the deep learning pipeline.","metadata":{}},{"cell_type":"code","source":"def create_data_loaders(dataset, config):\n    # Define the sizes for train, validation, and test sets\n    total_size = len(dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n\n    # Split the dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        dataset, \n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(config['seed'])\n    )\n    \n    print(f\"Train set size: {len(train_dataset)}\")\n    print(f\"Validation set size: {len(val_dataset)}\")\n    print(f\"Test set size: {len(test_dataset)}\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=config['train_batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=config['eval_batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=config['eval_batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n    \n    return train_loader, val_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2024-10-17T12:52:56.578353Z","iopub.execute_input":"2024-10-17T12:52:56.578675Z","iopub.status.idle":"2024-10-17T12:52:56.592478Z","shell.execute_reply.started":"2024-10-17T12:52:56.578643Z","shell.execute_reply":"2024-10-17T12:52:56.591615Z"},"trusted":true},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":"# Train and Evaluate Functions\n\n## Train Function\n\n### Purpose\nThe `train` function is responsible for training the model on the provided dataset for one epoch.\n\n### Parameters\n- `model`: The neural network model\n- `dataloader`: DataLoader containing the training data\n- `optimizer`: Optimization algorithm (e.g., Adam)\n- `criterion`: Loss function\n- `device`: Device to run the computations on (CPU or GPU)\n- `config`: Dictionary containing configuration parameters\n\n### Process\n1. Sets the model to training mode\n2. Iterates through the data in batches:\n   - Moves data to the specified device\n   - Performs forward pass\n   - Calculates loss\n   - Normalizes loss for gradient accumulation\n   - Performs backward pass\n   - Updates weights after accumulating gradients\n3. Calculates predictions and accuracy\n4. Returns average loss and accuracy for the epoch\n\n### Key Features\n- Uses `tqdm` for progress visualization\n- Implements gradient accumulation for effective training with larger batch sizes\n- Converts labels to float and uses sigmoid activation for binary classification\n\n## Evaluate Function\n\n### Purpose\nThe `evaluate` function assesses the model's performance on a dataset without updating the model's parameters.\n\n### Parameters\n- Similar to the `train` function, but without optimizer\n\n### Process\n1. Sets the model to evaluation mode\n2. Disables gradient calculation for efficiency\n3. Iterates through the data:\n   - Performs forward pass\n   - Calculates loss and accuracy\n4. Returns average loss and accuracy\n\n### Key Features\n- Uses `torch.no_grad()` to prevent gradient calculation\n- Employs the same prediction mechanism as the training function for consistency\n\n## Common Aspects\n\n- Both functions handle binary classification tasks\n- They use sigmoid activation and a 0.5 threshold for predictions\n- Both return average loss and accuracy over the entire dataset\n\nThese functions form the core of the training and evaluation pipeline, enabling efficient model training and performance assessment.","metadata":{}},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion, device, config):\n    model.train()\n    total_loss, total_acc = 0, 0\n    for i, (mfccs, labels) in enumerate(tqdm(dataloader, desc='Train')):\n        mfccs, labels = mfccs.to(device), labels.to(device)\n        outputs = model(mfccs)\n        loss = criterion(outputs, labels.unsqueeze(1).float())  # Convert labels to float\n        \n        # Normalize loss to account for batch accumulation\n        loss = loss / config['gradient_accumulation_steps']\n        \n        loss.backward()\n        \n        if (i + 1) % config['gradient_accumulation_steps'] == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        total_loss += loss.item() * config['gradient_accumulation_steps']\n        predictions = (torch.sigmoid(outputs) > 0.5).float()\n        total_acc += (predictions == labels.unsqueeze(1)).sum().item() / labels.size(0)\n    \n    return total_loss / len(dataloader), total_acc / len(dataloader)\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss, total_acc = 0, 0\n    with torch.no_grad():\n        for mfccs, labels in tqdm(dataloader, desc='Eval'):\n            mfccs, labels = mfccs.to(device), labels.to(device)\n            outputs = model(mfccs)\n            loss = criterion(outputs, labels.unsqueeze(1).float())  # Convert labels to float\n            total_loss += loss.item()\n            predictions = (torch.sigmoid(outputs) > 0.5).float()\n            total_acc += (predictions == labels.unsqueeze(1)).sum().item() / labels.size(0)\n    return total_loss / len(dataloader), total_acc / len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T12:52:56.639475Z","iopub.execute_input":"2024-10-17T12:52:56.639776Z","iopub.status.idle":"2024-10-17T12:52:56.650582Z","shell.execute_reply.started":"2024-10-17T12:52:56.639744Z","shell.execute_reply":"2024-10-17T12:52:56.649624Z"},"trusted":true},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":"# Device Selection for Computation\n\n## Purpose\n\nThis code snippet determines the appropriate computational device (GPU or CPU) for running the deep learning model.\n\n## Process\n\n1. The code checks if a CUDA-enabled GPU is available using `torch.cuda.is_available()`.\n2. If a GPU is available, it sets the device to \"cuda\" (GPU).\n3. If no GPU is available, it defaults to \"cpu\".\n\n## Output\n\nThe code prints a message indicating which device will be used for computations:\n- \"Using device: cuda\" if a GPU is available\n- \"Using device: cpu\" if no GPU is available\n\n## Significance\n\n- **Performance**: GPUs can significantly accelerate deep learning computations compared to CPUs.\n- **Flexibility**: This approach allows the code to run on systems with or without a GPU, enhancing portability.\n- **Optimization**: By explicitly setting the device, we ensure that tensors and models are allocated on the correct hardware for optimal performance.\n\nThis device selection is crucial for efficient training and inference in deep learning projects, especially when dealing with large models or datasets. You can ","metadata":{}},{"cell_type":"markdown","source":"# You can also suggest using [Data Parallel GPU Setup with T4 GPUs]\n\n## Overview\n\nFor improved performance and to utilize multiple T4 GPUs, we recommend implementing data parallelism using PyTorch's `nn.DataParallel` or `nn.DistributedDataParallel`.\n\n## Implementation Steps\n\n1. **Device Detection**:\n   ```python\n   import torch\n\n   if torch.cuda.is_available():\n       num_gpus = torch.cuda.device_count()\n       if num_gpus > 1:\n           print(f\"Using {num_gpus} GPUs\")\n           use_data_parallel = True\n       else:\n           print(\"Using single GPU\")\n           use_data_parallel = False\n   else:\n       print(\"CUDA is not available. Using CPU\")\n       use_data_parallel = False\n   ```\n\n2. **Model Wrapping**:\n   ```python\n   if use_data_parallel:\n       model = nn.DataParallel(model)\n   model = model.to(device)\n   ```\n\n3. **Batch Size Adjustment**:\n   ```python\n   if use_data_parallel:\n       config['train_batch_size'] *= num_gpus\n       config['eval_batch_size'] *= num_gpus\n   ```\n\n## Benefits of Data Parallelism\n\n- **Increased Processing Power**: Utilizes multiple GPUs to process larger batch sizes.\n- **Faster Training**: Reduces overall training time by distributing computations.\n- **Scalability**: Easily scales with the number of available GPUs.\n\n## Considerations\n\n- **Memory Usage**: Ensure that the increased batch size doesn't exceed GPU memory.\n- **Synchronization Overhead**: There's a slight overhead in synchronizing between GPUs.\n- **Load Balancing**: Automatic in `DataParallel`, but may need manual tuning for optimal performance.\n\nThis setup requires more code changes but offers better performance, especially for multi-node setups.\n\nBy implementing these changes, you can effectively utilize multiple T4 GPUs, significantly boosting your model's training performance.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T12:52:56.652223Z","iopub.execute_input":"2024-10-17T12:52:56.652516Z","iopub.status.idle":"2024-10-17T12:52:56.665307Z","shell.execute_reply.started":"2024-10-17T12:52:56.652479Z","shell.execute_reply":"2024-10-17T12:52:56.664407Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"# Loading and Processing Data from Kaggle and Hugging Face","metadata":{}},{"cell_type":"code","source":"# Importing ConcatDataset\nfrom torch.utils.data import ConcatDataset","metadata":{"execution":{"iopub.status.busy":"2024-10-17T12:52:56.667102Z","iopub.execute_input":"2024-10-17T12:52:56.667494Z","iopub.status.idle":"2024-10-17T12:52:56.675007Z","shell.execute_reply.started":"2024-10-17T12:52:56.667450Z","shell.execute_reply":"2024-10-17T12:52:56.674116Z"},"trusted":true},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# Load Dataset from Hugging Face\nds = load_dataset(\"Hemg/Deepfakeaudio\")\n# print(ds)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T12:52:56.676041Z","iopub.execute_input":"2024-10-17T12:52:56.676412Z","iopub.status.idle":"2024-10-17T12:52:57.655065Z","shell.execute_reply.started":"2024-10-17T12:52:56.676349Z","shell.execute_reply":"2024-10-17T12:52:57.654021Z"},"trusted":true},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['audio', 'label'],\n        num_rows: 19817\n    })\n})\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"# Process the dataset from hugging face\ndataset = AudioDatasetHuggingFace(ds['train'])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T12:52:57.657923Z","iopub.execute_input":"2024-10-17T12:52:57.658264Z","iopub.status.idle":"2024-10-17T13:00:37.479847Z","shell.execute_reply.started":"2024-10-17T12:52:57.658227Z","shell.execute_reply":"2024-10-17T13:00:37.478792Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Processing audio: 100%|██████████| 19817/19817 [07:37<00:00, 43.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded 19817 audio files.\nFake (0): 10000, Real (1): 9817\nMFCC shape: (40, 500)\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"# Process the dataset from kaggle\nnew_dataset = AudioDatasetKaggle('/kaggle/input/deep-voice-deepfake-voice-recognition', config)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:00:37.481177Z","iopub.execute_input":"2024-10-17T13:00:37.481510Z","iopub.status.idle":"2024-10-17T13:01:54.648686Z","shell.execute_reply.started":"2024-10-17T13:00:37.481477Z","shell.execute_reply":"2024-10-17T13:01:54.643711Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Processing REAL audio: 100%|██████████| 8/8 [00:10<00:00,  1.27s/it]\nProcessing FAKE audio: 100%|██████████| 56/56 [01:07<00:00,  1.20s/it]","output_type":"stream"},{"name":"stdout","text":"Loaded 64 audio files.\nReal: 8, Fake: 56\nMFCC shape: (40, 500)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"# Data visualization before concaternating \n## Uncomment the cells and run to view the structure and type of the datasets","metadata":{}},{"cell_type":"code","source":"# Check the frequency fo kaggle data\n\n# import wave\n\n# def get_wav_sr(file_path):\n#     with wave.open(file_path, 'rb') as wav_file:\n#         return wav_file.getframerate()\n\n# # Check a few files\n# folder_path = os.path.join('/kaggle/input/deep-voice-deepfake-voice-recognition', 'KAGGLE', 'AUDIO', 'REAL')\n# for filename in os.listdir(folder_path)[:5]:  # Check first 5 files\n#     if filename.endswith('.wav'):\n#         file_path = os.path.join(folder_path, filename)\n#         sr = get_wav_sr(file_path)\n#         print(f\"Sampling rate for {filename}: {sr} Hz\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:01:54.650628Z","iopub.execute_input":"2024-10-17T13:01:54.651129Z","iopub.status.idle":"2024-10-17T13:01:54.658004Z","shell.execute_reply.started":"2024-10-17T13:01:54.651076Z","shell.execute_reply":"2024-10-17T13:01:54.656635Z"},"trusted":true},"outputs":[],"execution_count":85},{"cell_type":"code","source":"# # Visualize the data from HF\n# print(\"Dataset Structure:\")\n# print(ds)\n\n# print(\"\\nAvailable Splits:\")\n# print(list(ds.keys()))\n\n# # Access the 'train' split\n# train_dataset = ds['train']\n\n# print(\"\\nTrain Dataset Features:\")\n# print(train_dataset.features)\n\n# print(\"\\nTrain Dataset Info:\")\n# print(train_dataset)\n\n# print(\"\\nSample data (first 5 entries):\")\n# for i in range(min(5, len(train_dataset))):\n#     print(f\"\\nSample {i+1}:\")\n#     item = train_dataset[i]\n#     for key, value in item.items():\n#         if key == 'audio':\n#             print(f\"  Audio:\")\n#             print(f\"    Array shape: {value['array'].shape}\")\n#             print(f\"    Sampling rate: {value['sampling_rate']} Hz\")\n#         else:\n#             print(f\"  {key.capitalize()}: {value}\")\n\n# print(\"\\nDataset Methods:\")\n# print([method for method in dir(train_dataset) if not method.startswith('_')])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:01:54.660266Z","iopub.execute_input":"2024-10-17T13:01:54.660767Z","iopub.status.idle":"2024-10-17T13:01:54.671061Z","shell.execute_reply.started":"2024-10-17T13:01:54.660712Z","shell.execute_reply":"2024-10-17T13:01:54.669936Z"},"trusted":true},"outputs":[],"execution_count":86},{"cell_type":"code","source":"# # Check lebels for HF data\n# train_dataset = ds['train']\n\n# # Count the occurrences of each label\n# label_counts = {0: 0, 1: 0}\n\n# for item in train_dataset:\n#     label = item['label']\n#     label_counts[label] += 1\n\n# # Print the results\n# print(\"Label Distribution:\")\n# print(f\"0 (likely 'fake'): {label_counts[0]}\")\n# print(f\"1 (likely 'real'): {label_counts[1]}\")\n\n# # Calculate percentages\n# total = sum(label_counts.values())\n# print(\"\\nPercentages:\")\n# print(f\"0 (likely 'fake'): {label_counts[0]/total*100:.2f}%\")\n# print(f\"1 (likely 'real'): {label_counts[1]/total*100:.2f}%\")\n\n# # Print a few examples of each label\n# print(\"\\nSamples with label 0:\")\n# for item in train_dataset.shuffle(seed=42).filter(lambda x: x['label'] == 0).select(range(5)):\n#     print(f\"  Audio shape: {item['audio']['array'].shape}, Label: {item['label']}\")\n\n# print(\"\\nSamples with label 1:\")\n# for item in train_dataset.shuffle(seed=42).filter(lambda x: x['label'] == 1).select(range(5)):\n#     print(f\"  Audio shape: {item['audio']['array'].shape}, Label: {item['label']}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:01:54.673026Z","iopub.execute_input":"2024-10-17T13:01:54.673907Z","iopub.status.idle":"2024-10-17T13:01:54.687887Z","shell.execute_reply.started":"2024-10-17T13:01:54.673848Z","shell.execute_reply":"2024-10-17T13:01:54.686546Z"},"trusted":true},"outputs":[],"execution_count":87},{"cell_type":"code","source":"# # Check the structure of the first item\n# first_item = train_dataset[0]\n# audio_data = first_item['audio']['array']\n\n# print(f\"Audio data shape: {audio_data.shape}\")\n# print(f\"Audio data type: {audio_data.dtype}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:01:54.689773Z","iopub.execute_input":"2024-10-17T13:01:54.690230Z","iopub.status.idle":"2024-10-17T13:01:54.697618Z","shell.execute_reply.started":"2024-10-17T13:01:54.690175Z","shell.execute_reply":"2024-10-17T13:01:54.696296Z"},"trusted":true},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":"## concaternating the datasets","metadata":{}},{"cell_type":"code","source":"combined_dataset = ConcatDataset([dataset, new_dataset])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:01:54.699661Z","iopub.execute_input":"2024-10-17T13:01:54.700277Z","iopub.status.idle":"2024-10-17T13:01:54.710136Z","shell.execute_reply.started":"2024-10-17T13:01:54.700217Z","shell.execute_reply":"2024-10-17T13:01:54.708834Z"},"trusted":true},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":"# Model Setup and Configuration\n\nThis code block sets up the core components for training the neural network model. Let's break down each part:\n\n## Data Loaders\n\n```python\ntrain_loader, val_loader, test_loader = create_data_loaders(combined_dataset, config)\n```\n- Creates data loaders for training, validation, and test sets.\n- Uses the `create_data_loaders` function (defined earlier) to split and load the data.\n- `combined_dataset` likely contains both Hugging Face and Kaggle datasets.\n\n## Model Initialization\n\n```python\nmodel = CNNNetwork(num_mfcc=config['n_mfcc']).to(device)\n```\n- Initializes the CNN model with the specified number of MFCC coefficients.\n- Moves the model to the appropriate device (GPU if available, otherwise CPU).\n\n## Loss Function\n\n```python\ncriterion = nn.BCEWithLogitsLoss()\n```\n- Uses Binary Cross Entropy with Logits Loss.\n- Suitable for binary classification tasks.\n- Combines a Sigmoid layer and BCELoss in one single class for numerical stability.\n\n## Optimizer\n\n```python\noptimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), eps=1e-8)\n```\n- Uses Adam optimizer for updating model parameters.\n- Learning rate and other hyperparameters are set based on the `config` dictionary.\n\n## Learning Rate Scheduler\n\n```python\ntotal_steps = len(train_loader) * config['epochs']\nwarmup_steps = int(total_steps * config['warmup_ratio'])\nscheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup_steps)\n```\n- Implements a linear learning rate warm-up.\n- Calculates total training steps and warm-up steps.\n- Uses `LinearLR` scheduler to gradually increase the learning rate during the warm-up phase.\n\n## Key Points\n\n- The setup is designed for binary classification (real vs fake audio).\n- It uses a CNN architecture specifically designed for audio data (MFCCs).\n- The learning rate scheduler implements a warm-up strategy, which can help stabilize early training.\n- All components are configured to work with the specified device (GPU/CPU).\n\nThis setup provides a solid foundation for training a deep learning model on the audio classification task, with considerations for optimization and learning rate adjustment.","metadata":{}},{"cell_type":"code","source":"\n# Create data loaders\ntrain_loader, val_loader, test_loader = create_data_loaders(combined_dataset, config)\n\n# Model initialization\nmodel = CNNNetwork(num_mfcc=config['n_mfcc']).to(device)\n\n# Loss and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), eps=1e-8)\n\n# Learning rate scheduler\ntotal_steps = len(train_loader) * config['epochs']\nwarmup_steps = int(total_steps * config['warmup_ratio'])\nscheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup_steps)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:01:54.712091Z","iopub.execute_input":"2024-10-17T13:01:54.712755Z","iopub.status.idle":"2024-10-17T13:01:54.751017Z","shell.execute_reply.started":"2024-10-17T13:01:54.712696Z","shell.execute_reply":"2024-10-17T13:01:54.749894Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train set size: 13916\nValidation set size: 2982\nTest set size: 2983\n","output_type":"stream"}],"execution_count":90},{"cell_type":"markdown","source":"# Training Loop\n\nThis code implements a training loop.\n\n## Training Loop\n```python\ntrain_loss, train_acc = train(model, train_loader, optimizer, criterion, device, config)\n```\n- Call the `train` function to train the model on the training data for one epoch.\n- The function takes the model, training data loader, optimizer, loss criterion, device, and configuration as arguments.\n- It returns the training loss and accuracy for the current epoch.\n\n### Validation\n```python\nval_loss, val_acc = evaluate(model, val_loader, criterion, device)\n```\n- Call the `evaluate` function to compute the validation loss and accuracy for the current model.\n- The function takes the model, validation data loader, loss criterion, and device as arguments.\n\n### Learning Rate Scheduler\n```python\nscheduler.step()\n```\n- Call the `step()` method of the learning rate scheduler to update the learning rate if applicable.\n\n### Logging\n```python\nprint(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\nprint(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n```\n- Print the training and validation losses and accuracies for the current epoch.\n\n### Best Model Checkpoint\n- If the validation accuracy for the current epoch is better than the best validation accuracy seen so far:\n  - Update `best_valid_acc` to the current validation accuracy.\n  - Create a checkpoint dictionary containing the current epoch, model state, optimizer state, scheduler state, and the updated `best_valid_acc`.\n  - Save the checkpoint to a file specified by `checkpoint_path` using `torch.save()`.\n  - Print a message indicating that the best model so far has been saved.\n\n## Final Evaluation\n- After the training loop ends, perform a final evaluation on the test set using the `evaluate` function.\n- Print the test loss and accuracy.\n\nThis training loop allows you to train your model for a specified number of epochs, monitor the training and validation performance, save checkpoints of the best model based on validation accuracy, and finally evaluate the model on the test set.","metadata":{}},{"cell_type":"code","source":"# Training loop\nbest_valid_acc = 0\nfor epoch in range(config['epochs']):\n    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, config)\n    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n    scheduler.step()\n    \n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    \n    if val_acc > best_valid_acc:\n        best_valid_acc = val_acc\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_valid_acc': best_valid_acc,\n        }\n        torch.save(checkpoint, checkpoint_path)\n        print(\"Saved best model\")\n\n# Final evaluation on test set\ntest_loss, test_acc = evaluate(model, test_loader, criterion, device)\nprint(f\"\\nTest Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:01:54.756066Z","iopub.execute_input":"2024-10-17T13:01:54.758903Z","iopub.status.idle":"2024-10-17T13:05:31.414772Z","shell.execute_reply.started":"2024-10-17T13:01:54.758843Z","shell.execute_reply":"2024-10-17T13:05:31.413677Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 182.22it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 300.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7279, Train Acc: 0.5023\nVal Loss: 0.6940, Val Acc: 0.4977\nSaved best model\n\nEpoch 2/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.27it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 310.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7035, Train Acc: 0.5038\nVal Loss: 0.6912, Val Acc: 0.5306\nSaved best model\n\nEpoch 3/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 183.35it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 313.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6973, Train Acc: 0.5092\nVal Loss: 0.6906, Val Acc: 0.5313\nSaved best model\n\nEpoch 4/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 182.64it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 311.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6944, Train Acc: 0.5139\nVal Loss: 0.6903, Val Acc: 0.5366\nSaved best model\n\nEpoch 5/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.34it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 311.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6923, Train Acc: 0.5197\nVal Loss: 0.6902, Val Acc: 0.5821\nSaved best model\n\nEpoch 6/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.94it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 309.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6918, Train Acc: 0.5206\nVal Loss: 0.6885, Val Acc: 0.6085\nSaved best model\n\nEpoch 7/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 183.29it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 308.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6898, Train Acc: 0.5372\nVal Loss: 0.6867, Val Acc: 0.5939\n\nEpoch 8/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.02it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 307.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6885, Train Acc: 0.5341\nVal Loss: 0.6841, Val Acc: 0.6383\nSaved best model\n\nEpoch 9/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 184.15it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 313.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6856, Train Acc: 0.5494\nVal Loss: 0.6793, Val Acc: 0.6447\nSaved best model\n\nEpoch 10/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 184.83it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 309.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6830, Train Acc: 0.5611\nVal Loss: 0.6751, Val Acc: 0.6567\nSaved best model\n\nEpoch 11/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.61it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 284.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6790, Train Acc: 0.5744\nVal Loss: 0.6685, Val Acc: 0.6953\nSaved best model\n\nEpoch 12/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 182.59it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 312.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6731, Train Acc: 0.5894\nVal Loss: 0.6595, Val Acc: 0.7120\nSaved best model\n\nEpoch 13/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 184.04it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 310.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6690, Train Acc: 0.5988\nVal Loss: 0.6523, Val Acc: 0.7261\nSaved best model\n\nEpoch 14/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.23it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 300.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6596, Train Acc: 0.6263\nVal Loss: 0.6442, Val Acc: 0.7007\n\nEpoch 15/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 183.54it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 309.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6509, Train Acc: 0.6455\nVal Loss: 0.6290, Val Acc: 0.7631\nSaved best model\n\nEpoch 16/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.64it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 312.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6388, Train Acc: 0.6640\nVal Loss: 0.6134, Val Acc: 0.7784\nSaved best model\n\nEpoch 17/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 179.96it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 307.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6253, Train Acc: 0.6859\nVal Loss: 0.5980, Val Acc: 0.7949\nSaved best model\n\nEpoch 18/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 183.40it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 306.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6095, Train Acc: 0.7014\nVal Loss: 0.5760, Val Acc: 0.8147\nSaved best model\n\nEpoch 19/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 183.66it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 310.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5924, Train Acc: 0.7195\nVal Loss: 0.5547, Val Acc: 0.8277\nSaved best model\n\nEpoch 20/20\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.49it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 304.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5725, Train Acc: 0.7421\nVal Loss: 0.5318, Val Acc: 0.8210\n","output_type":"stream"},{"name":"stderr","text":"Eval: 100%|██████████| 373/373 [00:01<00:00, 312.15it/s]","output_type":"stream"},{"name":"stdout","text":"\nTest Loss: 0.5316, Test Acc: 0.8132\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":91},{"cell_type":"markdown","source":"## Load checkpoint add uncomment to train from previous best model saved ","metadata":{}},{"cell_type":"code","source":"# Load best model\n\ndef load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n    if os.path.exists(checkpoint_path):\n        try:\n            checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n            print(\"Checkpoint content:\")\n            for key in checkpoint.keys():\n                print(f\"  {key}: {type(checkpoint[key])}\")\n            \n            # Load the model state dict\n            model.load_state_dict(checkpoint)\n            print(\"Model state dictionary loaded successfully.\")\n\n            epoch = 0\n            best_valid_acc = 0.0\n            \n            print(f\"Loaded checkpoint. Starting from epoch {epoch} with best validation accuracy: {best_valid_acc:.4f}\")\n            return epoch, best_valid_acc\n        except Exception as e:\n            print(f\"Error loading checkpoint: {e}\")\n            return 0, 0\n    else:\n        print(\"No checkpoint found. Starting from scratch.\")\n        return 0, 0\n\n# Usage\ncheckpoint_path = '/kaggle/working/best_model.pth'\nstart_epoch, best_valid_acc = load_checkpoint(model, optimizer, scheduler, checkpoint_path)\n\n# Rest of your training loop...\n# Training loop\nfor epoch in range(start_epoch, 50):\n    print(f\"\\nEpoch {epoch+1}/50\")\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, config)\n    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n    scheduler.step()\n    \n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    \n    if val_acc > best_valid_acc:\n        best_valid_acc = val_acc\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_valid_acc': best_valid_acc,\n        }\n        torch.save(checkpoint, checkpoint_path)\n        print(\"Saved best model\")\n\n# Final evaluation on test set\ntest_loss, test_acc = evaluate(model, test_loader, criterion, device)\nprint(f\"\\nTest Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:16:13.299218Z","iopub.execute_input":"2024-10-17T13:16:13.300419Z","iopub.status.idle":"2024-10-17T13:25:23.463889Z","shell.execute_reply.started":"2024-10-17T13:16:13.300366Z","shell.execute_reply":"2024-10-17T13:25:23.462660Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2442710179.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint content:\n  epoch: <class 'int'>\n  model_state_dict: <class 'collections.OrderedDict'>\n  optimizer_state_dict: <class 'dict'>\n  scheduler_state_dict: <class 'dict'>\n  best_valid_acc: <class 'float'>\nError loading checkpoint: Error(s) in loading state_dict for CNNNetwork:\n\tMissing key(s) in state_dict: \"conv1.0.weight\", \"conv1.0.bias\", \"conv2.0.weight\", \"conv2.0.bias\", \"conv3.0.weight\", \"conv3.0.bias\", \"fc.0.weight\", \"fc.0.bias\", \"fc.3.weight\", \"fc.3.bias\", \"fc.6.weight\", \"fc.6.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"scheduler_state_dict\", \"best_valid_acc\". \n\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 182.87it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 261.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1773, Train Acc: 0.9351\nVal Loss: 0.0768, Val Acc: 0.9682\nSaved best model\n\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 182.83it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 306.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1017, Train Acc: 0.9619\nVal Loss: 0.0516, Val Acc: 0.9799\nSaved best model\n\nEpoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 183.62it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 308.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0824, Train Acc: 0.9713\nVal Loss: 0.0585, Val Acc: 0.9759\n\nEpoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.79it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 279.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0632, Train Acc: 0.9792\nVal Loss: 0.0596, Val Acc: 0.9769\n\nEpoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.21it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 300.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0665, Train Acc: 0.9771\nVal Loss: 0.0409, Val Acc: 0.9832\nSaved best model\n\nEpoch 6/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 183.18it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 302.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0470, Train Acc: 0.9843\nVal Loss: 0.0334, Val Acc: 0.9876\nSaved best model\n\nEpoch 7/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 177.00it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 292.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0424, Train Acc: 0.9859\nVal Loss: 0.0539, Val Acc: 0.9826\n\nEpoch 8/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.98it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 300.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0476, Train Acc: 0.9829\nVal Loss: 0.0405, Val Acc: 0.9856\n\nEpoch 9/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.49it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 299.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0448, Train Acc: 0.9852\nVal Loss: 0.0542, Val Acc: 0.9806\n\nEpoch 10/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 177.60it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 296.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0393, Train Acc: 0.9858\nVal Loss: 0.0798, Val Acc: 0.9745\n\nEpoch 11/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.24it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 252.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0362, Train Acc: 0.9884\nVal Loss: 0.1118, Val Acc: 0.9618\n\nEpoch 12/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 177.34it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 298.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0338, Train Acc: 0.9891\nVal Loss: 0.1294, Val Acc: 0.9621\n\nEpoch 13/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 177.66it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 294.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0310, Train Acc: 0.9886\nVal Loss: 0.0552, Val Acc: 0.9842\n\nEpoch 14/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.02it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 290.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0288, Train Acc: 0.9890\nVal Loss: 0.0298, Val Acc: 0.9893\nSaved best model\n\nEpoch 15/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.67it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 296.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0329, Train Acc: 0.9892\nVal Loss: 0.0332, Val Acc: 0.9903\nSaved best model\n\nEpoch 16/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 174.78it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 299.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0276, Train Acc: 0.9903\nVal Loss: 0.0292, Val Acc: 0.9903\n\nEpoch 17/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.59it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 299.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0215, Train Acc: 0.9934\nVal Loss: 0.0318, Val Acc: 0.9886\n\nEpoch 18/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 183.07it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 296.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0189, Train Acc: 0.9932\nVal Loss: 0.0290, Val Acc: 0.9889\n\nEpoch 19/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 179.60it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 298.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0158, Train Acc: 0.9945\nVal Loss: 0.0448, Val Acc: 0.9866\n\nEpoch 20/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.91it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 297.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0257, Train Acc: 0.9914\nVal Loss: 0.0288, Val Acc: 0.9906\nSaved best model\n\nEpoch 21/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:10<00:00, 173.45it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 290.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0134, Train Acc: 0.9948\nVal Loss: 0.0309, Val Acc: 0.9916\nSaved best model\n\nEpoch 22/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 179.52it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 297.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0154, Train Acc: 0.9949\nVal Loss: 0.1117, Val Acc: 0.9806\n\nEpoch 23/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 178.62it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 299.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0195, Train Acc: 0.9935\nVal Loss: 0.0242, Val Acc: 0.9916\n\nEpoch 24/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.97it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 301.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0235, Train Acc: 0.9919\nVal Loss: 0.0367, Val Acc: 0.9899\n\nEpoch 25/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 176.01it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 299.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0168, Train Acc: 0.9942\nVal Loss: 0.0287, Val Acc: 0.9913\n\nEpoch 26/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.19it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 300.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0172, Train Acc: 0.9943\nVal Loss: 0.0349, Val Acc: 0.9896\n\nEpoch 27/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 179.76it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 299.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0126, Train Acc: 0.9968\nVal Loss: 0.0420, Val Acc: 0.9873\n\nEpoch 28/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 177.68it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 297.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0139, Train Acc: 0.9953\nVal Loss: 0.0258, Val Acc: 0.9920\nSaved best model\n\nEpoch 29/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 179.99it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 295.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0201, Train Acc: 0.9935\nVal Loss: 0.0602, Val Acc: 0.9815\n\nEpoch 30/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.04it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 262.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0168, Train Acc: 0.9935\nVal Loss: 0.0260, Val Acc: 0.9910\n\nEpoch 31/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.30it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 289.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0161, Train Acc: 0.9949\nVal Loss: 0.0279, Val Acc: 0.9903\n\nEpoch 32/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 181.31it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 302.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0171, Train Acc: 0.9945\nVal Loss: 0.0322, Val Acc: 0.9916\n\nEpoch 33/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 179.51it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 267.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0122, Train Acc: 0.9964\nVal Loss: 0.0299, Val Acc: 0.9896\n\nEpoch 34/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.58it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 298.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0115, Train Acc: 0.9966\nVal Loss: 0.0836, Val Acc: 0.9691\n\nEpoch 35/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 177.80it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 293.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0069, Train Acc: 0.9978\nVal Loss: 0.0312, Val Acc: 0.9933\nSaved best model\n\nEpoch 36/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 178.57it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 302.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0198, Train Acc: 0.9932\nVal Loss: 0.0383, Val Acc: 0.9910\n\nEpoch 37/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.77it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 291.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0165, Train Acc: 0.9952\nVal Loss: 0.0424, Val Acc: 0.9899\n\nEpoch 38/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 182.88it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 273.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0099, Train Acc: 0.9973\nVal Loss: 0.0486, Val Acc: 0.9910\n\nEpoch 39/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 174.45it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 295.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0048, Train Acc: 0.9986\nVal Loss: 0.0504, Val Acc: 0.9879\n\nEpoch 40/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:10<00:00, 173.38it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 277.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0267, Train Acc: 0.9913\nVal Loss: 0.0219, Val Acc: 0.9926\n\nEpoch 41/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 182.65it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 290.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0064, Train Acc: 0.9981\nVal Loss: 0.0292, Val Acc: 0.9913\n\nEpoch 42/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 174.98it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 297.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0066, Train Acc: 0.9981\nVal Loss: 0.0342, Val Acc: 0.9930\n\nEpoch 43/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 182.75it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 298.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0208, Train Acc: 0.9933\nVal Loss: 0.0368, Val Acc: 0.9903\n\nEpoch 44/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.27it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 301.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0115, Train Acc: 0.9974\nVal Loss: 0.0592, Val Acc: 0.9832\n\nEpoch 45/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.10it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 295.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0182, Train Acc: 0.9942\nVal Loss: 0.0457, Val Acc: 0.9896\n\nEpoch 46/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.59it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 291.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0108, Train Acc: 0.9964\nVal Loss: 0.0434, Val Acc: 0.9846\n\nEpoch 47/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 178.43it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 283.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0115, Train Acc: 0.9967\nVal Loss: 0.0481, Val Acc: 0.9883\n\nEpoch 48/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 176.38it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 293.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0109, Train Acc: 0.9964\nVal Loss: 0.0336, Val Acc: 0.9913\n\nEpoch 49/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 180.23it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 298.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0137, Train Acc: 0.9963\nVal Loss: 0.0342, Val Acc: 0.9892\n\nEpoch 50/50\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 1740/1740 [00:09<00:00, 179.84it/s]\nEval: 100%|██████████| 373/373 [00:01<00:00, 295.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0077, Train Acc: 0.9983\nVal Loss: 0.0311, Val Acc: 0.9923\n","output_type":"stream"},{"name":"stderr","text":"Eval: 100%|██████████| 373/373 [00:01<00:00, 288.39it/s]","output_type":"stream"},{"name":"stdout","text":"\nTest Loss: 0.0177, Test Acc: 0.9950\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"# rm -rf ./*","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:06:50.469678Z","iopub.execute_input":"2024-10-17T13:06:50.470111Z","iopub.status.idle":"2024-10-17T13:06:50.474303Z","shell.execute_reply.started":"2024-10-17T13:06:50.470051Z","shell.execute_reply":"2024-10-17T13:06:50.473274Z"},"trusted":true},"outputs":[],"execution_count":95},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}