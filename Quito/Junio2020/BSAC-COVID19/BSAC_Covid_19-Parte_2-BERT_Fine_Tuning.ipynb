{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BSAC_Covid_19-Parte_2-BERT_Fine_Tuning.ipynb","provenance":[{"file_id":"1VwrrndPer1UUdrAhSAsjC0cXkyM3jEta","timestamp":1591475561436}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Uu49G2ROnWsc","colab_type":"text"},"source":["# Parte II. Afinamiento (fine-tuning) del modelo BERT"]},{"cell_type":"markdown","metadata":{"id":"Qvr_nvbxmeoe","colab_type":"text"},"source":["**Integrantes - Grupo BSAC Covid 19 (Saturdays AI UIO)**\n","<table align=\"left\">\n","  <tr>\n","    <td>Sandra Torres</td>\n","    <td>Wendy Jara</td>    \n","  </tr>\n","  <tr>\n","    <td>Edwin Rodriguez</td>\n","    <td>Christian Pichucho</td>\n","  </tr>\n","  <tr>\n","    <td>Jorge Vargas</td>\n","    <td>Milton Fonseca</td>\n","  </tr>\n","  <tr>\n","    <td>Sebastián Ayala</td>\n","    <!--<td bgcolor=\"LightGray\">Coordinador</td>-->\n","    <td><i>*Coordinador</i></td>\n","  </tr> \n","</table>  "]},{"cell_type":"markdown","metadata":{"id":"cOlaS9afMkji","colab_type":"text"},"source":["El objetivo del presente trabajo, es realizar un afinamiento (fine-tuning) del modelo pre-entrenado **BioBERT-Base v1.1 (+ PubMed 1M)** disponible en https://github.com/dmis-lab/biobert sobre el conjunto de datos provisto por Kaggle en su reto <a href=\"https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\">COVID-19 Open Research Dataset Challenge (CORD-19)</a>.\n","\n","\n","El tipo de afinamiento que se realiza es para una tarea de <a href=\"https://en.wikipedia.org/wiki/Multi-label_classification\">Clasificación de Etiquetas Múltiples</a> (Multi-label classification). \n","\n","Para esto, se toma como datos de entrenamiento aquellos obtenidos en la *clasificación no-supervisada de tópicos* (sobre el conjunto de datos previamente mencionado), realizada en el notebook **BSAC_Covid_19-Parte_1-Interactive_Abstract_and_Expert_Finder.ipynb**, mismo que se basa en el trabajo <a href=\"https://www.kaggle.com/jdparsons/biobert-corex-topic-search\">Interactive Search using BioBERT and CorEx Topic Modeling</a> del autor John David Parsons. \n","\n","El afinamiento realizado, se basa en el artículo <a href=\"https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d\">Building a Multi-label Text Classifier using BERT and TensorFlow</a> del autor Javaid Nabi, cuyo código está disponible <a href=\"https://github.com/javaidnabi31/Multi-Label-Text-classification-Using-BERT/blob/master/multi-label-classification-bert.ipynb\">aquí</a>.\n","\n","Adicionamente, se consultaron otras fuentes como:\n","\n","|Título|Autor|Recurso|\n","|-|-|-|\n","|Multi-label Text Classification using BERT – The Mighty Transformer|Kaushal Trivedi|<a href=\"https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\">blog</a> <a href=\"https://nbviewer.jupyter.org/github/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb\">código</a>|\n","|run_classifier.py|Google AI Team|<a href=\"https://github.com/google-research/bert/blob/master/run_classifier.py\">código</a>|\n","|run_multilabels_classifier.py|Google AI Team, huangyajian|<a href=\"https://github.com/yajian/bert/blob/master/run_multilabels_classifier.py\">código</a>|"]},{"cell_type":"markdown","metadata":{"id":"X0KAXTsOnbtY","colab_type":"text"},"source":["# 1. Configuración y Carga de Datos"]},{"cell_type":"markdown","metadata":{"id":"WOH1ufBxSx5a","colab_type":"text"},"source":["## 1.1 Parámetros"]},{"cell_type":"code","metadata":{"id":"joBWSUouO8kR","colab_type":"code","colab":{}},"source":["# Nombres de Archivos a importar\n","file_corex_topics = 'corex_topic_model.pkl' # Producto de ntbk 'BioBERT + Corex Topic Search'\n","file_df_topics = 'df_final_covid_clean_topics.pkl' # Producto de ntbk 'BioBERT + Corex Topic Search'\n","\n","# Path de Directorios\n","path_folder_root = '/content/drive/My Drive/'\n","# NOTA: Modificar los siguientes nombres a directorios existentes en su Google Drive\n","path_folder_project_clustering = path_folder_root + 'NLP/projects/interactive-abstract-and-expert-finder/'\n","path_folder_project_bert = path_folder_root + 'NLP/projects/bert/'\n","path_folder_input = 'input/'\n","path_folder_output = 'output/'\n","path_models = path_folder_root + 'BERT/models/'\n","path_folder_fine_tuning = path_models + 'fine-tuned/'\n","path_folder_model_tuned = path_folder_fine_tuning + 'model/'\n","path_folder_train_output = path_folder_fine_tuning + 'train/'\n","path_folder_eval_output = path_folder_fine_tuning + 'eval/'\n","\n","# Path de Archivos\n","path_file_corex_topics = path_folder_project_clustering + path_folder_output + file_corex_topics\n","path_file_df_topics =  path_folder_project_clustering + path_folder_output + file_df_topics\n","\n","\n","# Parámetros BERT\n","# Descargar Modelo desde: https://github.com/dmis-lab/biobert\n","# Este notebook trabaja con la siguiente versión del modelo BioBert:\n","# BioBERT-Base v1.1 (+ PubMed 1M) - based on BERT-base-Cased (same vocabulary)\n","bert_model = 'biobert_v1.1_pubmed'\n","\n","dict_bert_params = {}\n","dict_bert_params['model_dir'] = bert_model\n","# Contains model vocabulary [ words to indexes mapping]\n","dict_bert_params['vocab'] = 'vocab.txt'\n","# Contains BERT model architecture.\n","dict_bert_params['config'] = 'bert_config.json'\n","# Contains weights of the pre-trained model\n","dict_bert_params['init_chkpnt'] = 'model.ckpt-1000000'\n","\n","for k, v in dict_bert_params.items():\n","  dict_bert_params[k] = path_models + bert_model + '/' + v\n","\n","\n","# Flags\n","flag_use_google_drive = True\n","\n","# Paráámetros Generales de Entrenamiento\n","ID = 'cord_uid'\n","DATA_COLUMN = 'document'\n","LABEL_COLUMNS = [] # será asignará la lista 'list_topic_words' en sección 2.4 (Matriz DTP)\n","NUM_OF_FEATURES = 0 # Número de Características o Equitetas (LABEL_COLUMNS), actualizado en sección 2.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D146O4TIROnQ","colab":{}},"source":["# Nombres de Archivos a importar\n","file_corex_topics = 'corex_topic_model.pkl' # Producto de ntbk 'BioBERT + Corex Topic Search'\n","file_df_topics = 'df_final_covid_clean_topics.pkl' # Producto de ntbk 'BioBERT + Corex Topic Search'\n","\n","# Path de Directorios\n","path_folder_root = '/content/drive/My Drive/'\n","# NOTA: Modificar los siguientes nombres a directorios existentes en su Google Drive\n","path_folder_project_clustering = path_folder_root + 'NLP/projects/interactive-abstract-and-expert-finder/'\n","path_folder_project_bert = path_folder_root + 'NLP/projects/bert/'\n","path_folder_input = 'input/'\n","path_folder_output = 'output/'\n","path_models = path_folder_root + 'BERT/models/'\n","path_folder_model_output = path_models + 'fine-tuned/' # train output\n","path_folder_eval_output = path_folder_model_output + 'eval/' # model evaluation output\n","\n","# Path de Archivos\n","path_file_corex_topics = path_folder_project_clustering + path_folder_output + file_corex_topics\n","path_file_df_topics =  path_folder_project_clustering + path_folder_output + file_df_topics\n","\n","\n","# Parámetros BERT\n","# Descargar Modelo desde: https://github.com/dmis-lab/biobert\n","# Este notebook trabaja con la siguiente versión del modelo BioBert:\n","# BioBERT-Base v1.1 (+ PubMed 1M) - based on BERT-base-Cased (same vocabulary)\n","bert_model = 'biobert_v1.1_pubmed'\n","\n","dict_bert_params = {}\n","dict_bert_params['model_dir'] = bert_model\n","# Contains model vocabulary [ words to indexes mapping]\n","dict_bert_params['vocab'] = 'vocab.txt'\n","# Contains BERT model architecture.\n","dict_bert_params['config'] = 'bert_config.json'\n","# Contains weights of the pre-trained model\n","dict_bert_params['init_chkpnt'] = 'model.ckpt-1000000'\n","\n","for k, v in dict_bert_params.items():\n","  dict_bert_params[k] = path_models + bert_model + '/' + v\n","\n","\n","# Flags\n","flag_use_google_drive = True\n","\n","# Paráámetros Generales de Entrenamiento\n","ID = 'cord_uid'\n","DATA_COLUMN = 'document'\n","LABEL_COLUMNS = [] # será asignará la lista 'list_topic_words' en sección 2.4 (Matriz DTP)\n","NUM_OF_FEATURES = 0 # Número de Características o Equitetas (LABEL_COLUMNS), actualizado en sección 2.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrHIIjVMw5rG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"status":"ok","timestamp":1592601130679,"user_tz":300,"elapsed":3818,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"073bb3ea-5a51-4f57-d49f-fd1a6974b41f"},"source":["# Construir Diccionario de Parámetros y mostrarlos\n","# https://thispointer.com/python-filter-a-dictionary-by-conditions-on-keys-or-values/\n","#dict_params = dict(filter(lambda item: item[0].find('file') != -1, globals().items()))\n","\n","# Construir Diccionario de Parámetros y mostrarlos\n","# https://stackoverflow.com/questions/6531482/how-to-check-if-a-string-contains-an-element-from-a-list-in-python\n","paramToCheck = ['file', 'folder', 'bert', 'flag']\n","\n","dict_params = dict(\n","    filter(lambda item:  any(item[0].find(paramType) != -1 for paramType in paramToCheck)\n","                  and isinstance(item[1], (str, bool)),\n","           globals().items())\n","    )\n","\n","print(len(dict_params))\n","\n","for param in sorted(dict_params):\n","  print( str(param + ':\\t').rjust(30) + str(dict_params[param]))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["16\n","                  bert_model:\tbiobert_v1.1_pubmed\n","           file_corex_topics:\tcorex_topic_model.pkl\n","              file_df_topics:\tdf_final_covid_clean_topics.pkl\n","       flag_use_google_drive:\tTrue\n","      path_file_corex_topics:\t/content/drive/My Drive/NLP/projects/interactive-abstract-and-expert-finder/output/corex_topic_model.pkl\n","         path_file_df_topics:\t/content/drive/My Drive/NLP/projects/interactive-abstract-and-expert-finder/output/df_final_covid_clean_topics.pkl\n","     path_folder_eval_output:\t/content/drive/My Drive/BERT/models/fine-tuned/eval/\n","     path_folder_fine_tuning:\t/content/drive/My Drive/BERT/models/fine-tuned/\n","           path_folder_input:\tinput/\n","    path_folder_model_output:\t/content/drive/My Drive/BERT/models/fine-tuned/\n","     path_folder_model_tuned:\t/content/drive/My Drive/BERT/models/fine-tuned/model/\n","          path_folder_output:\toutput/\n","    path_folder_project_bert:\t/content/drive/My Drive/NLP/projects/bert/\n","path_folder_project_clustering:\t/content/drive/My Drive/NLP/projects/interactive-abstract-and-expert-finder/\n","            path_folder_root:\t/content/drive/My Drive/\n","    path_folder_train_output:\t/content/drive/My Drive/BERT/models/fine-tuned/train/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OYwJoTBhgwo9","colab_type":"text"},"source":["## 1.2 Instalar Dependencias"]},{"cell_type":"markdown","metadata":{"id":"CJpDPOLFr-RN","colab_type":"text"},"source":["#### 1.2.1 CorEx"]},{"cell_type":"code","metadata":{"id":"amxYIF0Tr-li","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1592601135075,"user_tz":300,"elapsed":8205,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"9b106a2e-1b6e-4fcc-a3d0-d644a1952533"},"source":["# CorEx topic modeling dependencies\n","# https://github.com/gregversteeg/corex_topic\n","!pip install 'corextopic'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting corextopic\n","  Downloading https://files.pythonhosted.org/packages/64/1d/b2a320090c91e67dd93a7e6715794a126b1a56c643824444c952b303dc0a/corextopic-1.0.6-py3-none-any.whl\n","Installing collected packages: corextopic\n","Successfully installed corextopic-1.0.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y6tUe93Fpq3y","colab_type":"text"},"source":["#### 1.2.2 BERT"]},{"cell_type":"code","metadata":{"id":"WdT23Dfppu7q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1592601139071,"user_tz":300,"elapsed":12190,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"d5cf1be2-f801-4057-fbb2-2dec2e0f5496"},"source":["##install bert if not already done\n","!pip install bert-tensorflow"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |████▉                           | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"whbxvJ6Co1Zy","colab_type":"text"},"source":["## 1.3 Importar Librerías"]},{"cell_type":"code","metadata":{"id":"ADKn8dH5DWlQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592601139073,"user_tz":300,"elapsed":12184,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"f2e27672-a28d-4685-94be-9639477db572"},"source":["# 1) Bajar (Downgrade) la versión de Tensorflow en Google Colab de 2.x a 1.x\n","# (por compatibilidad con algunas librerías de BERT y Bert as a Service)\n","\n","# NOTA: Al ejecutar `import tensorflow` se importará (en Google Colab) por defecto la versión 2.x\n","# (actualmente).\n","# Se puede utilizar la versión 1.x al ejecutar una celda con el comando 'mágico' `tensorflow_version`\n","# antes de ejecutar `import tensorflow`.\n","\n","# TensorFlow versions in Colab\n","# https://colab.research.google.com/notebooks/tensorflow_version.ipynb#scrollTo=8UvRkm1JGUrk\n","\n","# *How to downgrade tensorflow version in colab?*  \n","# https://stackoverflow.com/questions/51888118/how-to-downgrade-tensorflow-version-in-colab/54445624\n","%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"otNS2nKFo0Dr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1592601144194,"user_tz":300,"elapsed":17294,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"926ebf0e-0700-4bac-ef3a-027bcb12da1f"},"source":["%%time\n","\n","# 2) Importar Librerías\n","import bert\n","import collections\n","import numpy as np\n","import os\n","import pandas as pd\n","import pickle\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","# Corex Imports\n","from corextopic import corextopic as ct\n","\n","# BERT Imports\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","from bert import modeling\n","\n","# Other general Imports\n","from datetime import datetime"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","CPU times: user 1.14 s, sys: 218 ms, total: 1.36 s\n","Wall time: 4.97 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ox-d53-to5Yy","colab_type":"text"},"source":["## 1.4 Montar Google Drive"]},{"cell_type":"code","metadata":{"id":"XlUBHv9XnXLT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1592601168859,"user_tz":300,"elapsed":41952,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"4f743872-ed1c-4dc2-e484-ac0ff09bb8c8"},"source":["# Mount Google Drive to this Notebook instance.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6QS8EH58pDs7","colab_type":"text"},"source":["## 1.5 Importar la data resultante del anterior Procesamiento"]},{"cell_type":"markdown","metadata":{"id":"pJUy_vHgYLRb","colab_type":"text"},"source":["### 1.5.1 Leer DataFrame de Abstact de Artículos y Tópicos"]},{"cell_type":"markdown","metadata":{"id":"_unRDU-aYLAu","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"73lXGbA2pEn4","colab_type":"code","colab":{}},"source":["# 1) Leer DataFrame con el Abstract de los Artíículos y Tópicos Clasificados\n","# Archivo: df_final_covid_clean_topics.pkl\n","# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_pickle.html\n","df_covid = pd.read_pickle(path_file_df_topics)\n","\n","# 2) Establecer como índice la columna 'index'\n","#df_covid.set_index('cord_uid', inplace=True)\n","\n","# 3) Elminar columnas que no se necesitan para el procesamiento requerido\n","df_covid.drop(columns=\n","['index',\n"," 'sha',\n"," 'source_x',\n"," 'title',\n"," 'doi',\n"," 'pmcid',\n"," 'pubmed_id',\n"," 'license',\n"," 'abstract',\n"," 'publish_time',\n"," 'authors',\n"," 'journal',\n"," 'Microsoft Academic Paper ID',\n"," 'WHO #Covidence',\n"," 'arxiv_id',\n"," 'has_pdf_parse',\n"," 'has_pmc_xml_parse',\n"," 'full_text_file',\n"," 'url'], inplace=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCYV-g52saWn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":590},"executionInfo":{"status":"ok","timestamp":1592601175321,"user_tz":300,"elapsed":48402,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"57270e90-5c8e-457b-e8b2-b27b96f01ae8"},"source":["print(\"df_covid datatype:\", type(df_covid))\n","print(\"df_covid shape:\", df_covid.shape)\n","df_covid.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["df_covid datatype: <class 'pandas.core.frame.DataFrame'>\n","df_covid shape: (48510, 24)\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 48510 entries, 0 to 48509\n","Data columns (total 24 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   cord_uid     48510 non-null  object \n"," 1   document     48510 non-null  object \n"," 2   clean        48510 non-null  object \n"," 3   clean_tfidf  48510 non-null  object \n"," 4   topic_0      48510 non-null  float64\n"," 5   topic_1      48510 non-null  float64\n"," 6   topic_2      48510 non-null  float64\n"," 7   topic_3      48510 non-null  float64\n"," 8   topic_4      48510 non-null  float64\n"," 9   topic_5      48510 non-null  float64\n"," 10  topic_6      48510 non-null  float64\n"," 11  topic_7      48510 non-null  float64\n"," 12  topic_8      48510 non-null  float64\n"," 13  topic_9      48510 non-null  float64\n"," 14  topic_10     48510 non-null  float64\n"," 15  topic_11     48510 non-null  float64\n"," 16  topic_12     48510 non-null  float64\n"," 17  topic_13     48510 non-null  float64\n"," 18  topic_14     48510 non-null  float64\n"," 19  topic_15     48510 non-null  float64\n"," 20  topic_16     48510 non-null  float64\n"," 21  topic_17     48510 non-null  float64\n"," 22  topic_18     48510 non-null  float64\n"," 23  topic_19     48510 non-null  float64\n","dtypes: float64(20), object(4)\n","memory usage: 8.9+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iAZDRmKejpEu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"status":"ok","timestamp":1592601175323,"user_tz":300,"elapsed":48396,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"76365582-8bfb-4e93-881c-84af9d2ca024"},"source":["df_covid.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cord_uid</th>\n","      <th>document</th>\n","      <th>clean</th>\n","      <th>clean_tfidf</th>\n","      <th>topic_0</th>\n","      <th>topic_1</th>\n","      <th>topic_2</th>\n","      <th>topic_3</th>\n","      <th>topic_4</th>\n","      <th>topic_5</th>\n","      <th>topic_6</th>\n","      <th>topic_7</th>\n","      <th>topic_8</th>\n","      <th>topic_9</th>\n","      <th>topic_10</th>\n","      <th>topic_11</th>\n","      <th>topic_12</th>\n","      <th>topic_13</th>\n","      <th>topic_14</th>\n","      <th>topic_15</th>\n","      <th>topic_16</th>\n","      <th>topic_17</th>\n","      <th>topic_18</th>\n","      <th>topic_19</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>zjufx4fo</td>\n","      <td>Sequence requirements for RNA strand transfer ...</td>\n","      <td>[[sequence, requirement, rna, strand, transfer...</td>\n","      <td>[sequence, requirement, rna, strand, transfer,...</td>\n","      <td>-11.967215</td>\n","      <td>-14.365350</td>\n","      <td>-4.232822e-05</td>\n","      <td>-6.038780</td>\n","      <td>0.000000</td>\n","      <td>-9.256285</td>\n","      <td>-8.990463</td>\n","      <td>-10.888360</td>\n","      <td>-8.524362</td>\n","      <td>-4.914860</td>\n","      <td>-17.746458</td>\n","      <td>-10.139457</td>\n","      <td>-8.976088</td>\n","      <td>-8.913227</td>\n","      <td>-8.710551</td>\n","      <td>-5.554590</td>\n","      <td>-5.697667</td>\n","      <td>-0.000089</td>\n","      <td>-5.325501</td>\n","      <td>-1.484620</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ymceytj3</td>\n","      <td>Crystal structure of murine sCEACAM1a[1,4]: a ...</td>\n","      <td>[[crystal_structure, murine, sceacama, coronav...</td>\n","      <td>[crystal_structure, murine, coronavirus, recep...</td>\n","      <td>-13.910713</td>\n","      <td>-14.805321</td>\n","      <td>-3.573142e-12</td>\n","      <td>-7.631686</td>\n","      <td>-8.760851</td>\n","      <td>-9.256256</td>\n","      <td>-10.317028</td>\n","      <td>-10.889253</td>\n","      <td>-14.412434</td>\n","      <td>-0.000010</td>\n","      <td>-25.626166</td>\n","      <td>-5.738624</td>\n","      <td>-8.000439</td>\n","      <td>-8.900729</td>\n","      <td>-8.678459</td>\n","      <td>-2.377468</td>\n","      <td>-6.530519</td>\n","      <td>-0.000012</td>\n","      <td>-9.027035</td>\n","      <td>-5.345502</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>wzj2glte</td>\n","      <td>Synthesis of a novel hepatitis C virus protein...</td>\n","      <td>[[synthesis, novel, hepatitis, virus, protein,...</td>\n","      <td>[synthesis, novel, hepatitis, ribosomal_frames...</td>\n","      <td>-13.910713</td>\n","      <td>-12.204447</td>\n","      <td>-1.059500e-07</td>\n","      <td>-10.775814</td>\n","      <td>0.000000</td>\n","      <td>-9.256337</td>\n","      <td>-8.045243</td>\n","      <td>-10.887869</td>\n","      <td>-6.730414</td>\n","      <td>-4.399364</td>\n","      <td>-25.626187</td>\n","      <td>-4.930299</td>\n","      <td>-8.975826</td>\n","      <td>-5.811124</td>\n","      <td>-8.505479</td>\n","      <td>-0.304766</td>\n","      <td>-5.277167</td>\n","      <td>-0.229326</td>\n","      <td>-5.505994</td>\n","      <td>-0.820358</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2sfqsfm1</td>\n","      <td>Structure of coronavirus main proteinase revea...</td>\n","      <td>[[structure, coronavirus, main, proteinase, re...</td>\n","      <td>[structure, coronavirus, main, proteinase, rev...</td>\n","      <td>-13.911672</td>\n","      <td>-13.899673</td>\n","      <td>0.000000e+00</td>\n","      <td>-6.329999</td>\n","      <td>-0.107050</td>\n","      <td>-5.852967</td>\n","      <td>-4.255949</td>\n","      <td>-10.853199</td>\n","      <td>-4.850234</td>\n","      <td>-8.392439</td>\n","      <td>-25.625044</td>\n","      <td>-10.143355</td>\n","      <td>-5.797064</td>\n","      <td>-2.433380</td>\n","      <td>-7.356429</td>\n","      <td>-5.216935</td>\n","      <td>-6.513124</td>\n","      <td>-0.000084</td>\n","      <td>-5.068582</td>\n","      <td>-4.267154</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i0zym7iq</td>\n","      <td>Discontinuous and non-discontinuous subgenomic...</td>\n","      <td>[[discontinuous, nondiscontinuous, subgenomic_...</td>\n","      <td>[discontinuous, subgenomic_rna, transcription,...</td>\n","      <td>-10.473175</td>\n","      <td>-13.169311</td>\n","      <td>-7.774288e-02</td>\n","      <td>-11.078761</td>\n","      <td>0.000000</td>\n","      <td>-9.256224</td>\n","      <td>-4.648812</td>\n","      <td>-10.888558</td>\n","      <td>-6.800841</td>\n","      <td>-5.802712</td>\n","      <td>-25.626739</td>\n","      <td>-10.143009</td>\n","      <td>-8.356926</td>\n","      <td>-8.898013</td>\n","      <td>-8.326373</td>\n","      <td>-3.828599</td>\n","      <td>-6.539708</td>\n","      <td>-0.460640</td>\n","      <td>-1.997524</td>\n","      <td>-0.005377</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   cord_uid  ...  topic_19\n","0  zjufx4fo  ... -1.484620\n","1  ymceytj3  ... -5.345502\n","2  wzj2glte  ... -0.820358\n","3  2sfqsfm1  ... -4.267154\n","4  i0zym7iq  ... -0.005377\n","\n","[5 rows x 24 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"szagPrPbYRtZ","colab_type":"text"},"source":["### 1.5.2 Leer Tópicos CorEx"]},{"cell_type":"code","metadata":{"id":"svGkiHdgsbaF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1592601176066,"user_tz":300,"elapsed":49130,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"d0ca60e0-7c1e-4a8f-d909-a6181bc50676"},"source":["# Leer Numpy Array de: corex_topic_model.pkl\n","# Conjunto de tópicos clasificados con CorEx\n","corex_topics = pd.read_pickle(path_file_corex_topics)\n","\n","print(\"corex_topics datatype:\", type(corex_topics))\n","print(\"len(corex_topics.words):\", len(corex_topics.words))\n","print(\"len(corex_topics.get_topics()):\", len(corex_topics.get_topics()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["corex_topics datatype: <class 'corextopic.corextopic.Corex'>\n","len(corex_topics.words): 9089\n","len(corex_topics.get_topics()): 20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K8qiuv5x5fOo","colab_type":"text"},"source":["# 2. Procesamiento de Datos"]},{"cell_type":"markdown","metadata":{"id":"cRFNByEKkb4V","colab_type":"text"},"source":["## 2.1 Extraer Información de los DataFrames"]},{"cell_type":"markdown","metadata":{"id":"XkAnNfKt9dEB","colab_type":"text"},"source":["### 2.1.1 Extraer los Términos Frecuentes de Cada Documento"]},{"cell_type":"code","metadata":{"id":"KSsUcfoQkgDI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1592601176068,"user_tz":300,"elapsed":49122,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"ba06fe23-b0cd-4f39-c243-7fa82d2044aa"},"source":["# 1) Cargar los Términos utilizados en los Documentos, ordenados por frecuencia\n","doc_terms = df_covid['clean_tfidf']\n","doc_terms.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [sequence, requirement, rna, strand, transfer,...\n","1    [crystal_structure, murine, coronavirus, recep...\n","2    [synthesis, novel, hepatitis, ribosomal_frames...\n","3    [structure, coronavirus, main, proteinase, rev...\n","4    [discontinuous, subgenomic_rna, transcription,...\n","Name: clean_tfidf, dtype: object"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"79EwjOCFu63m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1592601176069,"user_tz":300,"elapsed":49116,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"6d0ee9e5-7d7f-49c8-d2ca-a25379900e17"},"source":["# 2) Convertir Columna de Frecuencia de Términos a Lista\n","doc_terms = doc_terms.tolist()\n","print(\"doc_terms type:\", type(doc_terms))\n","print(\"len(doc_terms):\", len(doc_terms))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["doc_terms type: <class 'list'>\n","len(doc_terms): 48510\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1hPZBbDl999k","colab_type":"text"},"source":["### 2.1.2 Extraer los Tópicos CorEx"]},{"cell_type":"code","metadata":{"id":"S2yZy68Fte2Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1592601176553,"user_tz":300,"elapsed":49594,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"e3425928-7e57-468f-eac2-c0bef95375b1"},"source":["# 1) Obtener Tópicos\n","#help(corex_topics.get_topics)\n","topics = corex_topics.get_topics()  # Default: n_words=10\n","print(\"type(topics):\", type(topics))\n","print(\"len(topics):\", len(topics))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["type(topics): <class 'list'>\n","len(topics): 20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B2HqpMkmuNog","colab_type":"code","colab":{}},"source":["# 2) Generar Lista y Diccionario de Tópicos\n","# 2.1.a) Generar Lista de Tópicos\n","topic_list = []\n","# 2.1.b) Generar Diccionario Tópicos\n","topic_dict = {}\n","\n","for n, topic in enumerate(topics):\n","    topic_words, scores = zip(*topic)\n","    #print(\"type(topic_words):\", type(topic_words))\n","    #print(\"type(scores):\", type(scores))\n","    #print(f\"({topic_words} | {scores})\")\n","    #print(\"type(topic):\", type(topic))\n","    #print(topic)\n","    \n","    #print('{}: '.format(n) + ','.join(topic_words))\n","    #topic_list.append('topic_' + str(n) + ': ' + ', '.join(topic_words))\n","    topic_id = 'topic_' + str(n)\n","    topic_list.append(topic_id + ': ' + str(topic))\n","    topic_dict[topic_id] = topic\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gsWKMQyVDjfZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"status":"ok","timestamp":1592601176555,"user_tz":300,"elapsed":49587,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"b5d44b0f-1afa-4386-bdad-dd867886c90c"},"source":["# 2.2.a) Verificar Lista de Tópicos\n","print(len(topic_list))\n","topic_list"],"execution_count":null,"outputs":[{"output_type":"stream","text":["20\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[\"topic_0: [('health', 0.07819868863594989), ('public_health', 0.047379193897883566), ('national', 0.04701427268345163), ('risk', 0.046775351041916026), ('policy', 0.0439273024375846), ('international', 0.04352759900681843), ('care', 0.04023434762464224), ('practice', 0.036697040974408186), ('medical', 0.036401359257311756), ('measure', 0.035782878395272784)]\",\n"," \"topic_1: [('patient', 0.15485459288984565), ('respiratory', 0.12153850117832599), ('child', 0.07954655572121214), ('clinical', 0.07797813266884773), ('acute', 0.07133336096940021), ('pneumonia', 0.061411567951009195), ('symptom', 0.06026015650904005), ('respiratory_tract', 0.05267492814517904), ('respiratory_syncytial', 0.04918432880240683), ('rhinovirus', 0.04347807305923954)]\",\n"," \"topic_2: [('binding', 0.08487981563257542), ('activity', 0.07399665259932148), ('inhibit', 0.06402164406873423), ('inhibitor', 0.060225820339119746), ('membrane', 0.05039574144437599), ('domain', 0.050373571524239394), ('inhibition', 0.04665430929710614), ('target', 0.045797138315003656), ('structural', 0.0426937692347906), ('residue', 0.04162287904718621)]\",\n"," \"topic_3: [('activation', 0.07666129935367358), ('mechanism', 0.06382218698126185), ('role', 0.06226149362680084), ('pathway', 0.06046644226789522), ('cellular', 0.05897557273613863), ('response', 0.04917575785677037), ('function', 0.045826752936287525), ('receptor', 0.04498026183605892), ('cytokine', 0.04448547803402678), ('signal', 0.04442379605233788)]\",\n"," \"topic_4: [('sequence', 0.1906456491425223), ('genome', 0.10828039663121958), ('gene', 0.08817601197550268), ('rna', 0.06848777828095587), ('nucleotide', 0.05628328344457879), ('strain', 0.04336962181934096), ('genomic', 0.04325487305813427), ('mutation', 0.04294041328890361), ('phylogenetic_analysis', 0.037963234446286936), ('genetic', 0.03643079197921046)]\",\n"," \"topic_5: [('diarrhea', 0.06834567290437742), ('pig', 0.04336636108166872), ('calf', 0.03850553256952589), ('animal', 0.036783064424007424), ('cat', 0.035139927800586376), ('porcine_epidemic', 0.03428184089250712), ('feline', 0.03151504370054608), ('central_nervous_system', 0.031500916182657504), ('pedv', 0.030910901134548127), ('intestinal', 0.029751785964779984)]\",\n"," \"topic_6: [('model', 0.04461808964218917), ('approach', 0.04124722387436872), ('base', 0.033548766188396424), ('apply', 0.03190297443174978), ('tool', 0.03155913215532163), ('application', 0.030158128252056167), ('prediction', 0.028088005141710014), ('dynamic', 0.027345653472299274), ('simulation', 0.025995983292174663), ('technology', 0.02531674062655536)]\",\n"," \"topic_7: [('detection', 0.1413935097736406), ('sample', 0.12098635037811856), ('detect', 0.11438516451252756), ('test', 0.07862354538719718), ('pcr', 0.07597925247361681), ('positive', 0.07395096131554244), ('specimen', 0.06289213773425406), ('assay', 0.05441054468350665), ('rtpcr', 0.04646549099878162), ('diagnostic', 0.04340269017977028)]\",\n"," \"topic_8: [('severe_acute_respiratory', 0.15692786005250672), ('sarscov', 0.13187096342136734), ('covid', 0.13134982796330152), ('syndrome_coronavirus', 0.0996827407958), ('outbreak', 0.09527176639403911), ('coronavirus', 0.08357595270890306), ('sars', 0.08008765627625088), ('syndrome_sars', 0.07377011122056248), ('epidemic', 0.06853313805443259), ('middle_east_respiratory', 0.06786998339341299)]\",\n"," \"topic_9: [('mouse', 0.11970569594774687), ('expression', 0.11508286210617305), ('express', 0.07156423185741558), ('induced', 0.06345714370549886), ('ifn', 0.05417051382327697), ('induce', 0.04358167621552012), ('immune_response', 0.042520527878747265), ('induction', 0.04142682938426067), ('demonstrate', 0.04006683144974022), ('vivo', 0.039144021945674755)]\",\n"," \"topic_10: [('que', 0.03875980293362739), ('de', 0.03252103303056897), ('le', 0.03140335855954469), ('est', 0.026714164428681755), ('une', 0.026711142142746714), ('rsum', 0.02648828336863945), ('la', 0.02506946634698258), ('por', 0.0238276037646052), ('los', 0.023365592401220277), ('una', 0.022895561332766358)]\",\n"," \"topic_11: [('antibody', 0.1449280015193484), ('vaccine', 0.0940489677182339), ('antigen', 0.06262813891999358), ('serum', 0.05655350701216889), ('epitope', 0.05364568007068753), ('igg', 0.043159213957664345), ('neutralize_antibody', 0.040377581790695734), ('neutralize', 0.03992181452531947), ('recombinant', 0.03716608719016976), ('elisa', 0.03386999062063644)]\",\n"," \"topic_12: [('conclusion', 0.2738108944289679), ('background', 0.18549668413130557), ('objective', 0.09816568479460276), ('hospital', 0.0517790576992272), ('conduct', 0.0447820128770132), ('aim', 0.04165661531759413), ('data', 0.03187041969565813), ('participant', 0.03133730167251555), ('enrol', 0.02897799788493811), ('confidence_interval', 0.027281813288867307)]\",\n"," \"topic_13: [('research', 0.052191966261661955), ('need', 0.039906879645009725), ('global', 0.038185682436344244), ('infectious_disease', 0.037714794533098235), ('current', 0.032229854174215374), ('issue', 0.032184518895597446), ('future', 0.03208196996790178), ('threat', 0.02834707697562825), ('article', 0.026513241104638492), ('effort', 0.02639807588399775)]\",\n"," \"topic_14: [('age', 0.0837598498342901), ('day', 0.07921348728282235), ('total', 0.05457928394113005), ('period', 0.05272531051313418), ('year', 0.05225135898114198), ('rate', 0.047781836243786936), ('collect', 0.04684941339280518), ('month', 0.04453378533737183), ('group', 0.03637648682317136), ('week', 0.03605400423965926)]\",\n"," \"topic_15: [('review', 0.07393997533740193), ('understand', 0.045645347230285606), ('development', 0.04211329743193815), ('emerge', 0.03957379864506567), ('focus', 0.03880977637992266), ('recent', 0.037673212130249496), ('new', 0.0359155054799627), ('many', 0.03304866482999514), ('therapeutic', 0.026090836238773597), ('pathogen', 0.025477224255413496)]\",\n"," \"topic_16: [('country', 0.07588399751528094), ('population', 0.055966182447346254), ('surveillance', 0.03718800992467997), ('estimate', 0.03559810439141312), ('europe', 0.025222123863439305), ('africa', 0.023670826289576414), ('area', 0.01947716550958368), ('across', 0.018456094557891163), ('asia', 0.01737872509558131), ('endemic', 0.015179372970636004)]\",\n"," \"topic_17: [('structure', 0.05166805921525199), ('complex', 0.041454150034632826), ('interaction', 0.04029616164194388), ('process', 0.03867837518686396), ('property', 0.02172623535931837), ('biological', 0.019501934453142737), ('particle', 0.019414756025838927), ('component', 0.016688888480192336), ('generate', 0.015197290305060845), ('efficient', 0.015084867174072846)]\",\n"," \"topic_18: [('significantly', 0.06871736799242417), ('increase', 0.05981921897872112), ('effect', 0.058143155138169846), ('level', 0.05542146224513062), ('decrease', 0.05158132506192958), ('compare', 0.04567843843670922), ('reduce', 0.03973731486389893), ('low', 0.038949865442928895), ('observe', 0.02953853304555213), ('electronic_supplementary_material_online', 0.024376030237925825)]\",\n"," \"topic_19: [('replication', 0.10710271686825507), ('vitro', 0.04526886069376699), ('cell_line', 0.03557634693877297), ('mrna', 0.03399284949603686), ('culture', 0.0322954188181541), ('transcription', 0.0264087769469055), ('translation', 0.025550173798668106), ('infect', 0.024036293979745334), ('replicate', 0.02233384728043602), ('porcine_reproductive_respiratory', 0.02067749929959266)]\"]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"r_ZkW0YCD1VB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592601176556,"user_tz":300,"elapsed":49582,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"21c5b3c1-92c5-4d4d-e8fc-293f4c947c41"},"source":["# 2.2.b) Verificar Diccionario de Tópicos\n","print(len(topic_dict))\n","#topic_dict"],"execution_count":null,"outputs":[{"output_type":"stream","text":["20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jXFj0cZTyHSA","colab_type":"text"},"source":["## 2.2 Determinar el tópico más representativo de cada documento"]},{"cell_type":"code","metadata":{"id":"sG4p06ny3kjY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1592601176556,"user_tz":300,"elapsed":49574,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"5767e825-44e9-430b-c799-f099dab1c46c"},"source":["# 1) De entre las columnas de los tópicos (topic_), obtener el\n","# 'nombre de la columna' con el valor más representativo\n","#https://thispointer.com/pandas-find-maximum-values-position-in-columns-or-rows-of-a-dataframe/\n","corex_cols = [col for col in df_covid if col.startswith('topic_')]\n","print(corex_cols)\n","\n","# Determinar el tópico más representativo para cada documento\n","df_covid['best_topic'] = df_covid[corex_cols].idxmax(axis=1)\n","df_covid['best_topic'].describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10', 'topic_11', 'topic_12', 'topic_13', 'topic_14', 'topic_15', 'topic_16', 'topic_17', 'topic_18', 'topic_19']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["count       48510\n","unique         20\n","top       topic_2\n","freq         5894\n","Name: best_topic, dtype: object"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"lJ7yYXO0yULy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1592601177076,"user_tz":300,"elapsed":50087,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"8e4d6c3f-8e95-41a7-a311-d59e66e591a6"},"source":["# 2) De entre las columnas de los tópicos (topic_), obtener el\n","# valor (puntaje) más repesentativo.\n","# NOTA: Este valor corresponde al de la columna cuyo nombre consta\n","# bajo 'best_topic'.\n","df_covid['best_topic_score'] = df_covid[corex_cols].max(axis=1)\n","df_covid['best_topic_score'].describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    4.851000e+04\n","mean    -5.363184e-02\n","std      4.337930e-01\n","min     -5.411805e+00\n","25%     -2.085213e-08\n","50%     -6.217249e-14\n","75%      0.000000e+00\n","max      0.000000e+00\n","Name: best_topic_score, dtype: float64"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"FErETGTnbaKR","colab_type":"code","colab":{}},"source":["# 3) Eliminar filas con 'best_topic_score' <= 0\n","#df.drop(df[condition].index, axis=0, inplace=True)\n","#df_covid.drop(df_covid[df_covid.best_topic_score <= 0].index, axis=0, inplace=True)\n","#df_covid.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zs8mVoe77NBF","colab_type":"text"},"source":["## 2.3 Determinar las N palabras más representativas de cada Tópico"]},{"cell_type":"code","metadata":{"id":"ZbX_yqrU7Li_","colab_type":"code","colab":{}},"source":["# 1) Determinar el Número N de Palabras a utilizar\n","# NOTA: \n","# De acuerdo a la descripción de la función get_topics(), la misma retorna\n","# por defecto 10 palabras por cada tópico, ordenadas por su relevancia dentro\n","# del tópico.\n","#   help(corex_topics.get_topics) # Default: n_words=10\n","# De estas 10 palabras, nosotros arbitrariamente seleccionaremos las N primeras\n","# procurando que no sean muchas, lo cual demoraría el entrenamiento (afinamiento)\n","# posterior del modelo BERT. \n","top_n_words = 3\n","\n","# Queda pendiente determinar una metodología apropiada para no seleccionar este\n","# número de manera arbitraria"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eoy-axUZCpDu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":192},"executionInfo":{"status":"ok","timestamp":1592601177078,"user_tz":300,"elapsed":50079,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"a9589232-1e23-4dc8-faa4-14b21ee2ad45"},"source":["# 2) Crear columnas para las N palabras Top\n","\n","# 2.a) Determinar nombres de las columnas\n","word_cols = []\n","for i in range(0, top_n_words):\n","  columna = \"word_\" + str(i)\n","  #print(f\"Iteracion {i}, columna '{columna}\")\n","  word_cols.append(columna)\n","\n","print(word_cols)\n","\n","# 2.b) Crear las columnas en el DataFrame\n","for i, col in enumerate(word_cols):\n","  df_covid[col] = \"\"\n","\n","df_covid[word_cols].describe()  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["['word_0', 'word_1', 'word_2']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word_0</th>\n","      <th>word_1</th>\n","      <th>word_2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>48510</td>\n","      <td>48510</td>\n","      <td>48510</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>48510</td>\n","      <td>48510</td>\n","      <td>48510</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       word_0 word_1 word_2\n","count   48510  48510  48510\n","unique      1      1      1\n","top                        \n","freq    48510  48510  48510"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"It7kYo1YEEc1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1592601177079,"user_tz":300,"elapsed":50074,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"b5b4ad51-878a-4e20-d05b-71f6533e0bf9"},"source":["# 3) Actualizar columnas Nuevas (N  Palabras Top de Tópicos)\n","#https://thispointer.com/pandas-apply-a-function-to-single-or-selected-columns-or-rows-in-dataframe/ \n","#https://datatofish.com/if-condition-in-pandas-dataframe/\n","def get_topic_word(topic_dictionary, topic_id, word_index):\n","  # Dado un diccionario de tópicos 'topic_dictionary' obtener del\n","  # tópico 'topic_id' la palabra ubicada enl índice 'word_index'\n","  word, weight = topic_dictionary[topic_id][word_index]\n","  return word\n","\n","for i in range(0, top_n_words):\n","  col = word_cols[i] # Nombre de Columna actual\n","  topic_id = 'topic_' + str(i)\n","  df_covid[ col ] =  df_covid['best_topic'].apply(lambda x: get_topic_word(topic_dict, x, i) )\n","\n","df_covid[word_cols].describe() "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word_0</th>\n","      <th>word_1</th>\n","      <th>word_2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>48510</td>\n","      <td>48510</td>\n","      <td>48510</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>20</td>\n","      <td>20</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>binding</td>\n","      <td>activity</td>\n","      <td>inhibit</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>5894</td>\n","      <td>5894</td>\n","      <td>5894</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         word_0    word_1   word_2\n","count     48510     48510    48510\n","unique       20        20       20\n","top     binding  activity  inhibit\n","freq       5894      5894     5894"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"qu6EigQ-D0z2","colab_type":"text"},"source":["## 2.4 Construir Matriz 'Documento-Tópico-Palabras'"]},{"cell_type":"markdown","metadata":{"id":"S9HfRpQyjE51","colab_type":"text"},"source":["### 2.4.1 Construir Lista de palabras relevantes"]},{"cell_type":"code","metadata":{"id":"n1LUsBwmEJ86","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1592601177079,"user_tz":300,"elapsed":50066,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"1863f86e-b10d-47a0-95cf-4d0e816e3182"},"source":["# 1) Construir Lista de todas las palabras relevantes de todos los documentos\n","set_topic_words = set()\n","\n","for i in range(0, top_n_words):\n","  col = word_cols[i] # Nombre de Columna actual\n","  \n","  # Obtener datos de columna (word_{i}) como Lista  \n","  list_words = df_covid[col].to_list()\n","  #print(f\"list_words {i}: \", len(list_words))\n","  \n","  set_words = set(list_words)\n","  #print(f\"set_words: \", len(set_words))\n","\n","  # Agregar Lista a Set de Palabras\n","  set_topic_words = set_topic_words.union(set_words)\n","\n","list_topic_words = sorted(set_topic_words)\n","print( len(list_topic_words) )\n","\n","# Asignar como lista de Etiquetas (Labels) la lista de palabras calculada\n","LABEL_COLUMNS = list_topic_words \n","# Asignar como número de Características, el número de palabras encontradas\n","NUM_OF_FEATURES = len(list_topic_words)\n","\n","print(\"NUM_OF_FEATURES:\", NUM_OF_FEATURES)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["60\n","NUM_OF_FEATURES: 60\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"szI_TfJIjgxS","colab_type":"text"},"source":["### 2.4.2 Construir Matriz"]},{"cell_type":"code","metadata":{"id":"TG1OzocmEBig","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1592601264061,"user_tz":300,"elapsed":137042,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"5a9dcf7e-338d-420d-a1d8-ea56a99cc2c4"},"source":["# 1) Agregar a la matriz las palabras y asignar como valor de la celda:\n","# 1 cuando haya ocurrencia de la palabra en el documento\n","# 0 cuando no haya ocurrencia de la palabra en el documento\n","# NOTA: Esta rutina demora alrededor de 3 minutos!\n","%%time\n","for i, word in enumerate( list_topic_words ):\n","  # https://thispointer.com/pandas-apply-a-function-to-single-or-selected-columns-or-rows-in-dataframe/\n","  df_covid[word] = \\\n","    df_covid[word_cols].apply(lambda row: 1 if any([word == row[ word_cols[i] ] for i in range(len(word_cols))]) else 0, axis='columns')\n","\n","#df_doc_topic_words.info() "],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 1min 25s, sys: 35 ms, total: 1min 25s\n","Wall time: 1min 26s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YzwjNCGQJISc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592601264062,"user_tz":300,"elapsed":137037,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"96c028ee-addc-42c4-82e3-588d93f6a49d"},"source":["# 2) Comprobar presencia de registros con score positivo\n","len(df_covid[df_covid.best_topic_score != 0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["27874"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"SdntgWT1c4FK","colab_type":"code","colab":{}},"source":["# 3) Comprobar el número de ocurrencias en el documento (palabras con valor 1)\n","# NOTA: debe ser igual al valor del parámetro 'top_n_words'\n","#https://stackoverflow.com/questions/25748683/pandas-sum-dataframe-rows-for-given-columns\n","df_covid['count'] = df_covid.loc[:,list_topic_words].sum(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQ57qMtTUVk0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592601264064,"user_tz":300,"elapsed":137030,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"0fc2fca1-32d9-4c20-cdab-7d10d0c82334"},"source":["# 4) Mostrar información del nuevo dataframe\n","df_covid.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 48510 entries, 0 to 48509\n","Data columns (total 90 columns):\n"," #   Column                    Non-Null Count  Dtype  \n","---  ------                    --------------  -----  \n"," 0   cord_uid                  48510 non-null  object \n"," 1   document                  48510 non-null  object \n"," 2   clean                     48510 non-null  object \n"," 3   clean_tfidf               48510 non-null  object \n"," 4   topic_0                   48510 non-null  float64\n"," 5   topic_1                   48510 non-null  float64\n"," 6   topic_2                   48510 non-null  float64\n"," 7   topic_3                   48510 non-null  float64\n"," 8   topic_4                   48510 non-null  float64\n"," 9   topic_5                   48510 non-null  float64\n"," 10  topic_6                   48510 non-null  float64\n"," 11  topic_7                   48510 non-null  float64\n"," 12  topic_8                   48510 non-null  float64\n"," 13  topic_9                   48510 non-null  float64\n"," 14  topic_10                  48510 non-null  float64\n"," 15  topic_11                  48510 non-null  float64\n"," 16  topic_12                  48510 non-null  float64\n"," 17  topic_13                  48510 non-null  float64\n"," 18  topic_14                  48510 non-null  float64\n"," 19  topic_15                  48510 non-null  float64\n"," 20  topic_16                  48510 non-null  float64\n"," 21  topic_17                  48510 non-null  float64\n"," 22  topic_18                  48510 non-null  float64\n"," 23  topic_19                  48510 non-null  float64\n"," 24  best_topic                48510 non-null  object \n"," 25  best_topic_score          48510 non-null  float64\n"," 26  word_0                    48510 non-null  object \n"," 27  word_1                    48510 non-null  object \n"," 28  word_2                    48510 non-null  object \n"," 29  activation                48510 non-null  int64  \n"," 30  activity                  48510 non-null  int64  \n"," 31  age                       48510 non-null  int64  \n"," 32  antibody                  48510 non-null  int64  \n"," 33  antigen                   48510 non-null  int64  \n"," 34  approach                  48510 non-null  int64  \n"," 35  background                48510 non-null  int64  \n"," 36  base                      48510 non-null  int64  \n"," 37  binding                   48510 non-null  int64  \n"," 38  calf                      48510 non-null  int64  \n"," 39  cell_line                 48510 non-null  int64  \n"," 40  child                     48510 non-null  int64  \n"," 41  complex                   48510 non-null  int64  \n"," 42  conclusion                48510 non-null  int64  \n"," 43  country                   48510 non-null  int64  \n"," 44  covid                     48510 non-null  int64  \n"," 45  day                       48510 non-null  int64  \n"," 46  de                        48510 non-null  int64  \n"," 47  detect                    48510 non-null  int64  \n"," 48  detection                 48510 non-null  int64  \n"," 49  development               48510 non-null  int64  \n"," 50  diarrhea                  48510 non-null  int64  \n"," 51  effect                    48510 non-null  int64  \n"," 52  express                   48510 non-null  int64  \n"," 53  expression                48510 non-null  int64  \n"," 54  gene                      48510 non-null  int64  \n"," 55  genome                    48510 non-null  int64  \n"," 56  global                    48510 non-null  int64  \n"," 57  health                    48510 non-null  int64  \n"," 58  increase                  48510 non-null  int64  \n"," 59  inhibit                   48510 non-null  int64  \n"," 60  interaction               48510 non-null  int64  \n"," 61  le                        48510 non-null  int64  \n"," 62  mechanism                 48510 non-null  int64  \n"," 63  model                     48510 non-null  int64  \n"," 64  mouse                     48510 non-null  int64  \n"," 65  national                  48510 non-null  int64  \n"," 66  need                      48510 non-null  int64  \n"," 67  objective                 48510 non-null  int64  \n"," 68  patient                   48510 non-null  int64  \n"," 69  pig                       48510 non-null  int64  \n"," 70  population                48510 non-null  int64  \n"," 71  public_health             48510 non-null  int64  \n"," 72  que                       48510 non-null  int64  \n"," 73  replication               48510 non-null  int64  \n"," 74  research                  48510 non-null  int64  \n"," 75  respiratory               48510 non-null  int64  \n"," 76  review                    48510 non-null  int64  \n"," 77  role                      48510 non-null  int64  \n"," 78  sample                    48510 non-null  int64  \n"," 79  sarscov                   48510 non-null  int64  \n"," 80  sequence                  48510 non-null  int64  \n"," 81  severe_acute_respiratory  48510 non-null  int64  \n"," 82  significantly             48510 non-null  int64  \n"," 83  structure                 48510 non-null  int64  \n"," 84  surveillance              48510 non-null  int64  \n"," 85  total                     48510 non-null  int64  \n"," 86  understand                48510 non-null  int64  \n"," 87  vaccine                   48510 non-null  int64  \n"," 88  vitro                     48510 non-null  int64  \n"," 89  count                     48510 non-null  int64  \n","dtypes: float64(21), int64(61), object(8)\n","memory usage: 33.3+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wwxsYbbldXz-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":638},"executionInfo":{"status":"ok","timestamp":1592601264066,"user_tz":300,"elapsed":137023,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"bb2fa59e-2da3-4160-aa70-305555a8b529"},"source":["# 5) Desplegar muestra de N registros de 'df_doc_topic_words'\n","df_covid.sample(n=5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cord_uid</th>\n","      <th>document</th>\n","      <th>clean</th>\n","      <th>clean_tfidf</th>\n","      <th>topic_0</th>\n","      <th>topic_1</th>\n","      <th>topic_2</th>\n","      <th>topic_3</th>\n","      <th>topic_4</th>\n","      <th>topic_5</th>\n","      <th>topic_6</th>\n","      <th>topic_7</th>\n","      <th>topic_8</th>\n","      <th>topic_9</th>\n","      <th>topic_10</th>\n","      <th>topic_11</th>\n","      <th>topic_12</th>\n","      <th>topic_13</th>\n","      <th>topic_14</th>\n","      <th>topic_15</th>\n","      <th>topic_16</th>\n","      <th>topic_17</th>\n","      <th>topic_18</th>\n","      <th>topic_19</th>\n","      <th>best_topic</th>\n","      <th>best_topic_score</th>\n","      <th>word_0</th>\n","      <th>word_1</th>\n","      <th>word_2</th>\n","      <th>activation</th>\n","      <th>activity</th>\n","      <th>age</th>\n","      <th>antibody</th>\n","      <th>antigen</th>\n","      <th>approach</th>\n","      <th>background</th>\n","      <th>base</th>\n","      <th>binding</th>\n","      <th>calf</th>\n","      <th>cell_line</th>\n","      <th>...</th>\n","      <th>diarrhea</th>\n","      <th>effect</th>\n","      <th>express</th>\n","      <th>expression</th>\n","      <th>gene</th>\n","      <th>genome</th>\n","      <th>global</th>\n","      <th>health</th>\n","      <th>increase</th>\n","      <th>inhibit</th>\n","      <th>interaction</th>\n","      <th>le</th>\n","      <th>mechanism</th>\n","      <th>model</th>\n","      <th>mouse</th>\n","      <th>national</th>\n","      <th>need</th>\n","      <th>objective</th>\n","      <th>patient</th>\n","      <th>pig</th>\n","      <th>population</th>\n","      <th>public_health</th>\n","      <th>que</th>\n","      <th>replication</th>\n","      <th>research</th>\n","      <th>respiratory</th>\n","      <th>review</th>\n","      <th>role</th>\n","      <th>sample</th>\n","      <th>sarscov</th>\n","      <th>sequence</th>\n","      <th>severe_acute_respiratory</th>\n","      <th>significantly</th>\n","      <th>structure</th>\n","      <th>surveillance</th>\n","      <th>total</th>\n","      <th>understand</th>\n","      <th>vaccine</th>\n","      <th>vitro</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>19425</th>\n","      <td>y0s0yydl</td>\n","      <td>Mutation of Host Δ9 Fatty Acid Desaturase Inhi...</td>\n","      <td>[[mutation, host, fatty_acid, desaturase, inhi...</td>\n","      <td>[mutation, host, fatty_acid, inhibits, brome_m...</td>\n","      <td>-12.795788</td>\n","      <td>-1.203499e+01</td>\n","      <td>0.000000e+00</td>\n","      <td>-2.512689</td>\n","      <td>0.000000e+00</td>\n","      <td>-9.256283</td>\n","      <td>-10.317463</td>\n","      <td>-5.819684</td>\n","      <td>-8.509904</td>\n","      <td>-5.635860</td>\n","      <td>-25.626276</td>\n","      <td>-10.141650</td>\n","      <td>-5.752181</td>\n","      <td>-4.667865</td>\n","      <td>-7.916734</td>\n","      <td>-6.725431</td>\n","      <td>-5.676054</td>\n","      <td>-0.000002</td>\n","      <td>-0.078273</td>\n","      <td>-0.034047</td>\n","      <td>topic_2</td>\n","      <td>0.000000e+00</td>\n","      <td>binding</td>\n","      <td>activity</td>\n","      <td>inhibit</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>10334</th>\n","      <td>mkegxh7v</td>\n","      <td>Adaptive memory and evolution of the human nat...</td>\n","      <td>[[adaptive, memory, evolution, human, naturali...</td>\n","      <td>[adaptive, memory, evolution, mind, insight, m...</td>\n","      <td>-5.937405</td>\n","      <td>-6.827415e+00</td>\n","      <td>-8.997992e+00</td>\n","      <td>-13.069386</td>\n","      <td>-5.844794e-07</td>\n","      <td>-8.283356</td>\n","      <td>-4.392781</td>\n","      <td>-10.888228</td>\n","      <td>-8.524486</td>\n","      <td>-2.138001</td>\n","      <td>-25.626353</td>\n","      <td>-10.016427</td>\n","      <td>-2.975826</td>\n","      <td>-0.000001</td>\n","      <td>-2.559081</td>\n","      <td>-0.000295</td>\n","      <td>-4.545586</td>\n","      <td>-3.734556</td>\n","      <td>-0.146203</td>\n","      <td>-5.366926</td>\n","      <td>topic_4</td>\n","      <td>-5.844794e-07</td>\n","      <td>sequence</td>\n","      <td>genome</td>\n","      <td>gene</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4337</th>\n","      <td>b0tlco4t</td>\n","      <td>Etiology of Severe Childhood Pneumonia in The ...</td>\n","      <td>[[etiology, severe, childhood, pneumonia, gamb...</td>\n","      <td>[etiology, severe, childhood, pneumonia, west_...</td>\n","      <td>-13.911759</td>\n","      <td>-8.401102e-11</td>\n","      <td>-1.012527e+01</td>\n","      <td>-12.228490</td>\n","      <td>-1.116279e+01</td>\n","      <td>-8.057314</td>\n","      <td>-10.320849</td>\n","      <td>-0.903339</td>\n","      <td>-10.353499</td>\n","      <td>-8.389869</td>\n","      <td>-25.625982</td>\n","      <td>-5.839084</td>\n","      <td>-8.969200</td>\n","      <td>-6.121042</td>\n","      <td>-6.111404</td>\n","      <td>-6.848467</td>\n","      <td>-4.358565</td>\n","      <td>-4.580810</td>\n","      <td>-6.391552</td>\n","      <td>-3.628143</td>\n","      <td>topic_1</td>\n","      <td>-8.401102e-11</td>\n","      <td>patient</td>\n","      <td>respiratory</td>\n","      <td>child</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>14099</th>\n","      <td>fduv4yk2</td>\n","      <td>Ibis T5000: a universal biosensor approach for...</td>\n","      <td>[[ibis, universal, biosensor, approach, microb...</td>\n","      <td>[universal, biosensor, approach, microbiology,...</td>\n","      <td>-13.910713</td>\n","      <td>-1.286583e+01</td>\n","      <td>-7.771083e+00</td>\n","      <td>-11.816980</td>\n","      <td>-7.409468e+00</td>\n","      <td>-7.670176</td>\n","      <td>-0.005939</td>\n","      <td>-0.001621</td>\n","      <td>-10.352767</td>\n","      <td>-8.392398</td>\n","      <td>-25.626002</td>\n","      <td>-10.143011</td>\n","      <td>-8.975376</td>\n","      <td>-5.200953</td>\n","      <td>-7.938087</td>\n","      <td>-0.442373</td>\n","      <td>-6.543736</td>\n","      <td>-3.936592</td>\n","      <td>-5.893250</td>\n","      <td>-5.379325</td>\n","      <td>topic_7</td>\n","      <td>-1.621377e-03</td>\n","      <td>detection</td>\n","      <td>sample</td>\n","      <td>detect</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>10930</th>\n","      <td>ebx603bn</td>\n","      <td>Peptide-conjugate antisense based splice-corre...</td>\n","      <td>[[peptideconjugate, antisense, base, splicecor...</td>\n","      <td>[antisense, base, muscular, dystrophy, muscula...</td>\n","      <td>-13.910659</td>\n","      <td>-7.406791e+00</td>\n","      <td>-2.583267e-12</td>\n","      <td>-8.202380</td>\n","      <td>-5.233654e+00</td>\n","      <td>-3.751229</td>\n","      <td>-4.724675</td>\n","      <td>-7.177901</td>\n","      <td>-7.956844</td>\n","      <td>-1.543536</td>\n","      <td>-25.625987</td>\n","      <td>-10.142098</td>\n","      <td>-6.296149</td>\n","      <td>-0.006447</td>\n","      <td>-6.713429</td>\n","      <td>-0.001542</td>\n","      <td>-4.960554</td>\n","      <td>-0.000001</td>\n","      <td>-1.993565</td>\n","      <td>-2.957728</td>\n","      <td>topic_2</td>\n","      <td>-2.583267e-12</td>\n","      <td>binding</td>\n","      <td>activity</td>\n","      <td>inhibit</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 90 columns</p>\n","</div>"],"text/plain":["       cord_uid  ... count\n","19425  y0s0yydl  ...     3\n","10334  mkegxh7v  ...     3\n","4337   b0tlco4t  ...     3\n","14099  fduv4yk2  ...     3\n","10930  ebx603bn  ...     3\n","\n","[5 rows x 90 columns]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"-BIK501ZXChd","colab_type":"text"},"source":["# 3. Fine-Tuning (Afinamiento) de modelo BERT"]},{"cell_type":"markdown","metadata":{"id":"R4-oyeHy6E7G","colab_type":"text"},"source":["## 3.1 Preparación de Programas y Configuraciones"]},{"cell_type":"markdown","metadata":{"id":"JrEQZxKHFkAZ","colab_type":"text"},"source":["### 3.1.1 Definir Clases y Funciones utilitarias para entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"UcKXxD6tFrM6","colab_type":"text"},"source":["A continuacióón se necesita convertir la data en el formato que BERT requiere para interpretarla. Algunas clases y funciones utilitarias se provee para ello."]},{"cell_type":"markdown","metadata":{"id":"cdcipgCYJB2e","colab_type":"text"},"source":["#### 3.1.1.1 InputExample"]},{"cell_type":"markdown","metadata":{"id":"btY4K9TiJO6g","colab_type":"text"},"source":["Clase que representa una muestra de entrenamiento / prueba para un modelo de clasificación de secuencia simple."]},{"cell_type":"code","metadata":{"id":"XtmT0T7pImIx","colab_type":"code","colab":{}},"source":["class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, labels=None):\n","        \"\"\"Constructs a InputExample.\n","\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single\n","            sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence.\n","            Only must be specified for sequence pair tasks.\n","            labels: (Optional) [string]. The label of the example. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.labels = labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cFbm2cX6KSeM","colab_type":"text"},"source":["#### 3.1.1.2 InputFeatures"]},{"cell_type":"markdown","metadata":{"id":"IKg1Zx6BKYXI","colab_type":"text"},"source":["Clase que representa un set de características (variables) de datos"]},{"cell_type":"code","metadata":{"id":"KBnv97W2KVcd","colab_type":"code","colab":{}},"source":["class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self, input_ids, input_mask, segment_ids, label_ids, is_real_example=True):\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids,\n","        self.is_real_example=is_real_example"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZCVH6djK--8","colab_type":"text"},"source":["#### 3.1.1.3 create_examples()"]},{"cell_type":"markdown","metadata":{"id":"nQ9jDjWgLCJt","colab_type":"text"},"source":["`create_examples()`, lee el dataframe y carga el texto de entrada y sus correspondientes etiquetas objetivo (clases) en el objeto `InputExample`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oQb6R64ZZC84","colab":{}},"source":["def create_examples(df, idx_column_index, idx_column_text, idx_columns_labels):\n","    \"\"\"Creates examples for the training and dev sets.\"\"\"\n","    examples = []\n","    for (i, row) in enumerate(df.values):\n","        #print(\"row:\\n\",row)\n","        #print(\"index_column:\", idx_column_index)\n","        #print(\"row[idx_column_index]:\", row[idx_column_index])\n","        #print(\"row[0]:\", row[0])\n","        guid = row[idx_column_index]\n","        text_a = row[idx_column_text]\n","        labels = row[idx_columns_labels]\n","        # Agregar una instancia de 'InputExample' a la lista de muestras\n","        examples.append(\n","            InputExample(guid=guid, text_a=text_a, labels=labels))\n","    return examples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IeyabQZMaRhL","colab_type":"text"},"source":["#### 3.1.1.4 convert_examples_to_features()"]},{"cell_type":"markdown","metadata":{"id":"moV8-LGvXgri","colab_type":"text"},"source":["Usando `tokenizer`, llamaremos al método **`convert_examples_to_features`** en nuestras muestras para convertirlas en características que BERT entienda.\n","\n","Este método agrega los tokens especiales **\"CLS\"** y **\"SEP\"** utilizados por BERT para identificar el *inicio* y el *final* de la oración. También agrega tokens de **\"index\"** (índice) y **\"segment\"** (segmento) a cada entrada. Por lo tanto, esta función realiza todo el trabajo de formateo de entrada según el BERT.\n","\n","<img src=\"https://miro.medium.com/max/1400/1*IA45-w25Ach4LcgsBABYKA.png\" alt=\"Representación de las entradas de BERT\" width=\"60%\" height=\"60%\" />  \n","*Representación de entradas de BERT. Las incrustaciones de entrada (input embeddings) son la suma de las incrustaciones de: token, segmentación y posición.*"]},{"cell_type":"code","metadata":{"id":"YbtwU5mULFIR","colab_type":"code","colab":{}},"source":["def convert_examples_to_features(examples,  max_seq_length, tokenizer):\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        print(example.text_a)\n","        tokens_a = tokenizer.tokenize(example.text_a)\n","\n","        tokens_b = None\n","        if example.text_b:\n","            tokens_b = tokenizer.tokenize(example.text_b)\n","            # Modifies `tokens_a` and `tokens_b` in place so that the total\n","            # length is less than the specified length.\n","            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n","        else:\n","            # Account for [CLS] and [SEP] with \"- 2\"\n","            if len(tokens_a) > max_seq_length - 2:\n","                tokens_a = tokens_a[:(max_seq_length - 2)]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids: 0   0   0   0  0     0 0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambigiously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens += tokens_b + [\"[SEP]\"]\n","            segment_ids += [1] * (len(tokens_b) + 1)\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding = [0] * (max_seq_length - len(input_ids))\n","        input_ids += padding\n","        input_mask += padding\n","        segment_ids += padding\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        \n","        labels_ids = []\n","        for label in example.labels:\n","            labels_ids.append(int(label))\n","\n","        if ex_index < 0:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\" % (example.guid))\n","            logger.info(\"tokens: %s\" % \" \".join(\n","                    [str(x) for x in tokens]))\n","            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n","            logger.info(\n","                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n","            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n","\n","        features.append(\n","                InputFeatures(input_ids=input_ids,\n","                              input_mask=input_mask,\n","                              segment_ids=segment_ids,\n","                              label_ids=labels_ids))\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCqBeQrkbRsU","colab_type":"text"},"source":["#### 3.1.1.5 Crear el Modelo: create_model()"]},{"cell_type":"markdown","metadata":{"id":"-dnS61shbYv0","colab_type":"text"},"source":["Se utiliza el modelo BERT (Bio BERT) previamente entrenado y lo afinamos para nuestra tarea de clasificación.\n","\n","Básicamente cargamos el modelo pre-entrenado y luego entrenamos la última capa para la tarea de clasificación."]},{"cell_type":"code","metadata":{"id":"KUNMHJeFaolb","colab_type":"code","colab":{}},"source":["def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n","                 labels, num_labels, use_one_hot_embeddings):\n","    \"\"\"Creates a classification model.\"\"\"\n","    model = modeling.BertModel(\n","        config=bert_config,\n","        is_training=is_training,\n","        input_ids=input_ids,\n","        input_mask=input_mask,\n","        token_type_ids=segment_ids,\n","        use_one_hot_embeddings=use_one_hot_embeddings)\n","\n","    # In the demo, we are doing a simple classification task on the entire\n","    # segment.\n","    #\n","    # If you want to use the token-level output, use model.get_sequence_output()\n","    # instead.\n","    output_layer = model.get_pooled_output()\n","    print(\"\\ntype(output_layer):\", type(output_layer))\n","    print(\"output_layer:\\n\", output_layer)\n","\n","    hidden_size = output_layer.shape[-1].value\n","    print(\"\\ntype(hidden_size):\", type(hidden_size))\n","    print(\"hidden_size:\\n\", hidden_size)\n","\n","    output_weights = tf.get_variable(\n","        \"output_weights\", [num_labels, hidden_size],\n","        initializer=tf.truncated_normal_initializer(stddev=0.02))\n","    print(\"\\ntype(output_weights):\\n\", type(output_weights))\n","    print(\"output_weights:\\n\", output_weights)\n","\n","    output_bias = tf.get_variable(\n","        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","    with tf.variable_scope(\"loss\"):\n","        if is_training:\n","            # I.e., 0.1 dropout\n","            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","            print(\"\\ntype(output_layer):\", type(output_layer))\n","            print(\"output_layer:\", output_layer)\n","\n","        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","        logits = tf.nn.bias_add(logits, output_bias)\n","        print(\"\\ntype(logits):\", type(logits))\n","        print(\"logits:\", logits)\n","\n","        # probabilities = tf.nn.softmax(logits, axis=-1) ### multiclass case\n","        probabilities = tf.nn.sigmoid(logits)#### multi-label case\n","        \n","        labels = tf.cast(labels, tf.float32)\n","        tf.logging.info(\"\\nnum_labels:{};\\nlogits:{};\\nlabels:{}\".format(num_labels, logits, labels))\n","        per_example_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n","        loss = tf.reduce_mean(per_example_loss)\n","\n","        # probabilities = tf.nn.softmax(logits, axis=-1)\n","        # log_probs = tf.nn.log_softmax(logits, axis=-1)\n","        #\n","        # one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","        #\n","        # per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","        # loss = tf.reduce_mean(per_example_loss)\n","\n","        return (loss, per_example_loss, logits, probabilities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qwjr1zEQcXtv","colab_type":"code","colab":{}},"source":["def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n","                     num_train_steps, num_warmup_steps, use_tpu,\n","                     use_one_hot_embeddings):\n","    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","        #tf.logging.info(\"*** Features ***\")\n","        #for name in sorted(features.keys()):\n","        #    tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n","\n","        input_ids = features[\"input_ids\"]\n","        input_mask = features[\"input_mask\"]\n","        segment_ids = features[\"segment_ids\"]\n","        label_ids = features[\"label_ids\"]\n","        is_real_example = None\n","        if \"is_real_example\" in features:\n","             is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n","        else:\n","             is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n","\n","        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","\n","        (total_loss, per_example_loss, logits, probabilities) = create_model(\n","            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n","            num_labels, use_one_hot_embeddings)\n","\n","        tvars = tf.trainable_variables()\n","        initialized_variable_names = {}\n","        scaffold_fn = None\n","        if init_checkpoint:\n","            (assignment_map, initialized_variable_names\n","             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n","            if use_tpu:\n","\n","                def tpu_scaffold():\n","                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","                    return tf.train.Scaffold()\n","\n","                scaffold_fn = tpu_scaffold\n","            else:\n","                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","\n","        tf.logging.info(\"**** Trainable Variables ****\")\n","        for var in tvars:\n","            init_string = \"\"\n","            if var.name in initialized_variable_names:\n","                init_string = \", *INIT_FROM_CKPT*\"\n","            #tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,init_string)\n","\n","        output_spec = None\n","        if mode == tf.estimator.ModeKeys.TRAIN:\n","\n","            train_op = optimization.create_optimizer(\n","                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n","\n","            output_spec = tf.estimator.EstimatorSpec(\n","                mode=mode,\n","                loss=total_loss,\n","                train_op=train_op,\n","                scaffold=scaffold_fn)\n","        elif mode == tf.estimator.ModeKeys.EVAL:\n","\n","            def metric_fn(per_example_loss, label_ids, probabilities, is_real_example):\n","\n","                logits_split = tf.split(probabilities, num_labels, axis=-1)\n","                label_ids_split = tf.split(label_ids, num_labels, axis=-1)\n","                # metrics change to auc of every class\n","                eval_dict = {}\n","                for j, logits in enumerate(logits_split):\n","                    label_id_ = tf.cast(label_ids_split[j], dtype=tf.int32)\n","                    current_auc, update_op_auc = tf.metrics.auc(label_id_, logits)\n","                    eval_dict[str(j)] = (current_auc, update_op_auc)\n","                eval_dict['eval_loss'] = tf.metrics.mean(values=per_example_loss)\n","                return eval_dict\n","\n","                ## original eval metrics\n","                # predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","                # accuracy = tf.metrics.accuracy(\n","                #     labels=label_ids, predictions=predictions, weights=is_real_example)\n","                # loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n","                # return {\n","                #     \"eval_accuracy\": accuracy,\n","                #     \"eval_loss\": loss,\n","                # }\n","\n","            eval_metrics = metric_fn(per_example_loss, label_ids, probabilities, is_real_example)\n","            output_spec = tf.estimator.EstimatorSpec(\n","                mode=mode,\n","                loss=total_loss,\n","                eval_metric_ops=eval_metrics,\n","                scaffold=scaffold_fn)\n","        else:\n","            print(\"mode:\", mode,\"probabilities:\", probabilities)\n","            output_spec = tf.estimator.EstimatorSpec(\n","                mode=mode,\n","                predictions={\"probabilities\": probabilities},\n","                scaffold=scaffold_fn)\n","        return output_spec\n","\n","    return model_fn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K68KI3Shad3W","colab_type":"text"},"source":["#### 3.1.1.6 Clases y Funciones utilitarias varias"]},{"cell_type":"markdown","metadata":{"id":"DhMwsFpNrrtC","colab_type":"text"},"source":["##### class `PaddingInputExample`"]},{"cell_type":"code","metadata":{"id":"-9owbpInrSfz","colab_type":"code","colab":{}},"source":["class PaddingInputExample(object):\n","    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n","    When running eval/predict on the TPU, we need to pad the number of examples\n","    to be a multiple of the batch size, because the TPU requires a fixed batch\n","    size. The alternative is to drop the last batch, which is bad because it means\n","    the entire output data won't be generated.\n","    We use this class instead of `None` because treating `None` as padding\n","    battches could cause silent errors.\n","    \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3dNKagKar1E6","colab_type":"text"},"source":["##### `convert_single_example`"]},{"cell_type":"code","metadata":{"id":"oPXu1zDZrXc9","colab_type":"code","colab":{}},"source":["def convert_single_example(ex_index, example, max_seq_length,\n","                           tokenizer):\n","    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n","  \n","    if isinstance(example, PaddingInputExample):\n","        return InputFeatures(\n","            input_ids=[0] * max_seq_length,\n","            input_mask=[0] * max_seq_length,\n","            segment_ids=[0] * max_seq_length,\n","            label_ids=0,\n","            is_real_example=False)\n","\n","    tokens_a = tokenizer.tokenize(example.text_a)\n","    tokens_b = None\n","    if example.text_b:\n","        tokens_b = tokenizer.tokenize(example.text_b)\n","\n","    if tokens_b:\n","        # Modifies `tokens_a` and `tokens_b` in place so that the total\n","        # length is less than the specified length.\n","        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n","    else:\n","        # Account for [CLS] and [SEP] with \"- 2\"\n","        if len(tokens_a) > max_seq_length - 2:\n","            tokens_a = tokens_a[0:(max_seq_length - 2)]\n","\n","    # The convention in BERT is:\n","    # (a) For sequence pairs:\n","    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n","    # (b) For single sequences:\n","    #  tokens:   [CLS] the dog is hairy . [SEP]\n","    #  type_ids: 0     0   0   0  0     0 0\n","    #\n","    # Where \"type_ids\" are used to indicate whether this is the first\n","    # sequence or the second sequence. The embedding vectors for `type=0` and\n","    # `type=1` were learned during pre-training and are added to the wordpiece\n","    # embedding vector (and position vector). This is not *strictly* necessary\n","    # since the [SEP] token unambiguously separates the sequences, but it makes\n","    # it easier for the model to learn the concept of sequences.\n","    #\n","    # For classification tasks, the first vector (corresponding to [CLS]) is\n","    # used as the \"sentence vector\". Note that this only makes sense because\n","    # the entire model is fine-tuned.\n","    tokens = []\n","    segment_ids = []\n","    tokens.append(\"[CLS]\")\n","    segment_ids.append(0)\n","    for token in tokens_a:\n","        tokens.append(token)\n","        segment_ids.append(0)\n","    tokens.append(\"[SEP]\")\n","    segment_ids.append(0)\n","\n","    if tokens_b:\n","        for token in tokens_b:\n","            tokens.append(token)\n","            segment_ids.append(1)\n","        tokens.append(\"[SEP]\")\n","        segment_ids.append(1)\n","\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","    # tokens are attended to.\n","    input_mask = [1] * len(input_ids)\n","\n","    # Zero-pad up to the sequence length.\n","    while len(input_ids) < max_seq_length:\n","        input_ids.append(0)\n","        input_mask.append(0)\n","        segment_ids.append(0)\n","\n","    assert len(input_ids) == max_seq_length\n","    assert len(input_mask) == max_seq_length\n","    assert len(segment_ids) == max_seq_length\n","\n","    labels_ids = []\n","    for label in example.labels:\n","        labels_ids.append(int(label))\n","\n","\n","    feature = InputFeatures(\n","        input_ids=input_ids,\n","        input_mask=input_mask,\n","        segment_ids=segment_ids,\n","        label_ids=labels_ids,\n","        is_real_example=True)\n","    return feature"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ir2FT3r-r7tw","colab_type":"text"},"source":["##### `file_based_convert_examples_to_features`"]},{"cell_type":"code","metadata":{"id":"8I-Igaw2rebx","colab_type":"code","colab":{}},"source":["def file_based_convert_examples_to_features(\n","        examples, max_seq_length, tokenizer, output_file):\n","    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n","\n","    writer = tf.python_io.TFRecordWriter(output_file)\n","\n","    for (ex_index, example) in enumerate(examples):\n","        #if ex_index % 10000 == 0:\n","            #tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","\n","        feature = convert_single_example(ex_index, example,\n","                                         max_seq_length, tokenizer)\n","\n","        def create_int_feature(values):\n","            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n","            return f\n","\n","        features = collections.OrderedDict()\n","        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n","        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n","        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n","        features[\"is_real_example\"] = create_int_feature(\n","            [int(feature.is_real_example)])\n","        if isinstance(feature.label_ids, list):\n","            label_ids = feature.label_ids\n","        else:\n","            label_ids = feature.label_ids[0]\n","        features[\"label_ids\"] = create_int_feature(label_ids)\n","\n","        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n","        writer.write(tf_example.SerializeToString())\n","    writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZA23ckpr_um","colab_type":"text"},"source":["##### `file_based_input_fn_builder`"]},{"cell_type":"code","metadata":{"id":"xqgkp6DSadIM","colab_type":"code","colab":{}},"source":["def file_based_input_fn_builder(input_file, seq_length, is_training,\n","                                drop_remainder, number_of_features):\n","    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","    name_to_features = {\n","        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n","        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n","        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n","        \"label_ids\": tf.FixedLenFeature([number_of_features], tf.int64),\n","        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n","    }\n","\n","    def _decode_record(record, name_to_features):\n","        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n","        example = tf.parse_single_example(record, name_to_features)\n","\n","        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n","        # So cast all int64 to int32.\n","        for name in list(example.keys()):\n","            t = example[name]\n","            if t.dtype == tf.int64:\n","                t = tf.to_int32(t)\n","            example[name] = t\n","\n","        return example\n","\n","    def input_fn(params):\n","        \"\"\"The actual input function.\"\"\"\n","        batch_size = params[\"batch_size\"]\n","\n","        # For training, we want a lot of parallel reading and shuffling.\n","        # For eval, we want no shuffling and parallel reading doesn't matter.\n","        d = tf.data.TFRecordDataset(input_file)\n","        if is_training:\n","            d = d.repeat()\n","            d = d.shuffle(buffer_size=100)\n","\n","        d = d.apply(\n","            tf.contrib.data.map_and_batch(\n","                lambda record: _decode_record(record, name_to_features),\n","                batch_size=batch_size,\n","                drop_remainder=drop_remainder))\n","\n","        return d\n","\n","    return input_fn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"34WCOAemsC-e","colab_type":"text"},"source":["fn `_truncate_seq_pair`"]},{"cell_type":"code","metadata":{"id":"1zPERQskrOb0","colab_type":"code","colab":{}},"source":["def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n","    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n","\n","    # This is a simple heuristic which will always truncate the longer sequence\n","    # one token at a time. This makes more sense than truncating an equal percent\n","    # of tokens from each, since if one sequence is very short then each token\n","    # that's truncated likely contains more information than a longer sequence.\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) > len(tokens_b):\n","            tokens_a.pop()\n","        else:\n","            tokens_b.pop()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aSqWyKC5uk6j","colab_type":"text"},"source":["### 3.1.2 Crear Tokenizador"]},{"cell_type":"code","metadata":{"id":"WyhozC9sAbB5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1592601265288,"user_tz":300,"elapsed":138215,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"e37cf0db-4005-4422-b111-badfe21407d6"},"source":["print(dict_bert_params['init_chkpnt'])\n","print(dict_bert_params['vocab'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/BERT/models/biobert_v1.1_pubmed/model.ckpt-1000000\n","/content/drive/My Drive/BERT/models/biobert_v1.1_pubmed/vocab.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YZWqbRp0ud0Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"status":"ok","timestamp":1592601266196,"user_tz":300,"elapsed":139117,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"18688589-0ccf-42e4-9eaf-6c0b34cb2e96"},"source":["# 1) Validar Tokenizacion del Modelo\n","tokenization.validate_case_matches_checkpoint(True, dict_bert_params['init_chkpnt'])\n","\n","# 2) Crear Tokenizador\n","tokenizer = tokenization.FullTokenizer(vocab_file=dict_bert_params['vocab'], do_lower_case=True)\n","\n","# 3) Probar Tokenizador con oración Dummy (en inglés)\n","tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'be',\n"," '##rt',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"HXTmithcFSoK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592601266197,"user_tz":300,"elapsed":139112,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"229c5ebd-bfec-467a-9a1b-9a02ac5a632f"},"source":["# 4) Verificar Longitud actual del Vocabulario\n","tokens_vocabulario_inicial = len(tokenizer.vocab)\n","print(\"Tokens Vocabulario Inicial:\", tokens_vocabulario_inicial)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tokens Vocabulario Inicial: 28996\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JLCeoCxxm9CY","colab_type":"text"},"source":["## 3.2 Preparación de Datos"]},{"cell_type":"markdown","metadata":{"id":"2DK4N8YOnPm5","colab_type":"text"},"source":["Se utiliza el modelo BERT (Bio BERT) previamente entrenado y lo afinamos para nuestra tarea de clasificación.\n","\n","Básicamente cargamos el modelo pre-entrenado y luego entrenamos la última capa para la tarea de clasificación."]},{"cell_type":"markdown","metadata":{"id":"b5yGKOX56Qkf","colab_type":"text"},"source":["### 3.2.1 Dividir data para Entrenamiento, Validación y Prueba"]},{"cell_type":"code","metadata":{"id":"8Jrx_ix8udpl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1592601266198,"user_tz":300,"elapsed":139107,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"9d960abc-5b8e-481a-bb07-74c4ece9e275"},"source":["# Dividir 75% Entrenamiento y 25% Test\n","# https://pythonhealthcare.org/2018/12/22/112-splitting-data-set-into-training-and-test-sets-using-pandas-dataframes-methods/\n","print(\"# registros Dataset Original:\", len(df_covid), \"\\n\")\n","\n","# 1) Crear dataframe de Entrenamiento (train)\n","train = df_covid.sample(frac=0.75, random_state=0)\n","#print(\"# registros Dataset Train:\", len(train))\n","\n","# 2) Crear dataframe para Prueba (test)\n","x_test = df_covid.drop(train.index)\n","print(\"# registros Dataset Test:\", len(x_test), \"shape:\", x_test.shape )\n","\n","# 3) Crear dataframe de Validación (validation), tomando el 20% del de 'train'\n","x_validation = train.sample(frac=0.1, random_state=0)\n","print(\"# registros Dataset Validación:\", len(x_validation), \"shape:\", x_validation.shape)\n","\n","x_train = train.drop(x_validation.index)\n","print(\"# registros Dataset Train:\", len(x_train), \"shape:\", x_train.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# registros Dataset Original: 48510 \n","\n","# registros Dataset Test: 12128 shape: (12128, 90)\n","# registros Dataset Validación: 3638 shape: (3638, 90)\n","# registros Dataset Train: 32744 shape: (32744, 90)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K8mZpQ-pWuKj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1592601266199,"user_tz":300,"elapsed":139101,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"d1a2d03a-2bf9-4567-81b6-acaa9fde2135"},"source":["print(x_train.shape)\n","print(x_validation.shape)\n","print(x_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(32744, 90)\n","(3638, 90)\n","(12128, 90)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fbMbMB_MYjDL","colab_type":"text"},"source":["### 3.2.2 Crear Muestras de Entrenamiento en formato BERT: `InputExample`"]},{"cell_type":"code","metadata":{"id":"OZRrd66r0V-2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1592601266200,"user_tz":300,"elapsed":139097,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"f2995f82-68b5-49a0-f86e-4566da7a7391"},"source":["#def create_examples(df, idx_column_index, idx_column_text, idx_columns_labels):\n","print(\"ID\", ID)\n","\n","idx_column_text =  df_covid.columns.get_loc(DATA_COLUMN)\n","idx_columns_labels = [[df_covid.columns.get_loc(col)] for col in LABEL_COLUMNS] \n","idx_column_index = df_covid.columns.get_loc(ID)\n","idx_columns_labels = [df_covid.columns.get_loc(col) for col in LABEL_COLUMNS] \n","\n","print(\"idx_column_index:\", idx_column_index)\n","print(\"idx_column_text:\", idx_column_text)\n","print(\"LABEL_COLUMNS:\", LABEL_COLUMNS)\n","print(\"idx_columns_labels:\", idx_columns_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ID cord_uid\n","idx_column_index: 0\n","idx_column_text: 1\n","LABEL_COLUMNS: ['activation', 'activity', 'age', 'antibody', 'antigen', 'approach', 'background', 'base', 'binding', 'calf', 'cell_line', 'child', 'complex', 'conclusion', 'country', 'covid', 'day', 'de', 'detect', 'detection', 'development', 'diarrhea', 'effect', 'express', 'expression', 'gene', 'genome', 'global', 'health', 'increase', 'inhibit', 'interaction', 'le', 'mechanism', 'model', 'mouse', 'national', 'need', 'objective', 'patient', 'pig', 'population', 'public_health', 'que', 'replication', 'research', 'respiratory', 'review', 'role', 'sample', 'sarscov', 'sequence', 'severe_acute_respiratory', 'significantly', 'structure', 'surveillance', 'total', 'understand', 'vaccine', 'vitro']\n","idx_columns_labels: [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lr_Lq8h3nAdi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592601268407,"user_tz":300,"elapsed":141298,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"90fdfa21-cec5-4cbc-9cbe-457781ee9604"},"source":["##%%time\n","#def create_examples(df, index_column, text_column, label_columns):\n","train_examples = create_examples(x_train, idx_column_index, idx_column_text, idx_columns_labels)\n","len(train_examples)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32744"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"W-sYXjdYpF_a","colab_type":"text"},"source":["### 3.2.3 Parámetros de Entrenamiento "]},{"cell_type":"code","metadata":{"id":"ZTqAtVBDwc_Q","colab_type":"code","colab":{}},"source":["# We'll set sequences to be at most 128 tokens long.\n","MAX_SEQ_LENGTH = 128\n","\n","# Compute train and warmup steps from batch size\n","# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n","BATCH_SIZE = 32\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 1.0\n","# Warmup is a period of time where hte learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 1000\n","SAVE_SUMMARY_STEPS = 500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8CSMUntis0cT","colab_type":"text"},"source":["## 3.3 Entrenar el modelo (Train)"]},{"cell_type":"code","metadata":{"id":"kbjBzPfas6Ov","colab_type":"code","colab":{}},"source":["# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_examples) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lb5WiQk8tEYx","colab_type":"code","colab":{}},"source":["# Abrir archivo para la salida del proceso de entrenamiento\n","train_file = os.path.join(path_folder_train_output, \"train.tf_record\")\n","\n","# Crear el archivo si no existe\n","if not os.path.exists(train_file):\n","    open(train_file, 'w').close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TejpjBJuUFd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1592601391492,"user_tz":300,"elapsed":264371,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"a67cfc10-a360-4d8d-8c88-ab150ecdfe01"},"source":["# Convertir las Muestras a Características (Features)\n","%%time\n","file_based_convert_examples_to_features(\n","            train_examples, MAX_SEQ_LENGTH, tokenizer, train_file)\n","tf.logging.info(\"***** Running training *****\")\n","tf.logging.info(\"  Num examples = %d\", len(train_examples))\n","tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n","tf.logging.info(\"  Num steps = %d\", num_train_steps)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:***** Running training *****\n","INFO:tensorflow:  Num examples = 32744\n","INFO:tensorflow:  Batch size = 32\n","INFO:tensorflow:  Num steps = 1023\n","CPU times: user 2min 1s, sys: 150 ms, total: 2min 2s\n","Wall time: 2min 4s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"USIeHFeBuXgH","colab_type":"code","colab":{}},"source":["# Creates an `input_fn` closure to be passed to TPUEstimator\n","train_input_fn = file_based_input_fn_builder(\n","    input_file=train_file,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=True,\n","    number_of_features=NUM_OF_FEATURES)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"urrqeRQo2u49","colab_type":"text"},"source":["#### 3.3.1 Configuración de TensorFlow Estimator"]},{"cell_type":"code","metadata":{"id":"fMgTVwYw2p35","colab_type":"code","colab":{}},"source":["# Specify output directory and number of checkpoint steps to save\n","run_config = tf.estimator.RunConfig(\n","    model_dir= path_folder_model_tuned,\n","    save_summary_steps = SAVE_SUMMARY_STEPS,\n","    keep_checkpoint_max = 1,\n","    save_checkpoints_steps = SAVE_CHECKPOINTS_STEPS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQFvmY123ROx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"ok","timestamp":1592601391893,"user_tz":300,"elapsed":264763,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"39571fa1-b67b-4539-b210-3e884c55dc34"},"source":["# Cargar archivo de configuración de BERT\n","bert_config = modeling.BertConfig.from_json_file( dict_bert_params['config'] )\n","\n","model_fn = model_fn_builder(\n","  bert_config= bert_config,\n","  num_labels= NUM_OF_FEATURES, #len(LABEL_COLUMNS)\n","  init_checkpoint= dict_bert_params['init_chkpnt'] ,\n","  learning_rate= LEARNING_RATE,\n","  num_train_steps= num_train_steps,\n","  num_warmup_steps= num_warmup_steps,\n","  use_tpu= False,\n","  use_one_hot_embeddings= False)\n","\n","estimator = tf.estimator.Estimator(\n","  model_fn= model_fn,\n","  config= run_config,\n","  params={\"batch_size\": BATCH_SIZE})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': '/content/drive/My Drive/BERT/models/fine-tuned/model/', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8c5f680f60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N57gFbQ5EL08","colab_type":"text"},"source":["### 3.3.2 Afinar Modelo"]},{"cell_type":"code","metadata":{"id":"uhxWiYddER1X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1592601395892,"user_tz":300,"elapsed":268757,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"0d469c60-3ebe-432c-f6a4-67941c016627"},"source":["print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Beginning Training!\n","INFO:tensorflow:Skipping training since max_steps has already saved.\n","Training took time  0:00:03.755697\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"edwf_AEjExQ4","colab_type":"text"},"source":["## 3.4 Evaluar el modelo (Validate)"]},{"cell_type":"code","metadata":{"id":"RnWoXQ3nE0Qg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1592601410859,"user_tz":300,"elapsed":283719,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"08c34595-68fc-403e-f347-cffbb69d17ed"},"source":["%%time\n","eval_file = os.path.join(path_folder_eval_output, \"eval.tf_record\")\n","#filename = Path(train_file)\n","if not os.path.exists(eval_file):\n","    open(eval_file, 'w').close()\n","\n","eval_examples = create_examples(x_validation, idx_column_index, idx_column_text, idx_columns_labels)\n","file_based_convert_examples_to_features(\n","    eval_examples, MAX_SEQ_LENGTH, tokenizer, eval_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 13.6 s, sys: 21.9 ms, total: 13.6 s\n","Wall time: 14.8 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fLw4ak8HE1I-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592601485016,"user_tz":300,"elapsed":357868,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"2018f767-de41-4f82-9ff2-a4fc6f553dd9"},"source":["# This tells the estimator to run through the entire set.\n","eval_steps = None\n","eval_drop_remainder = False\n","\n","eval_input_fn = file_based_input_fn_builder(\n","    input_file=eval_file,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=eval_drop_remainder,\n","    number_of_features=NUM_OF_FEATURES)\n","\n","print(\"Evaluate\")\n","result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Evaluate\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-40-2bc270ee49eb>:42: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.experimental.map_and_batch(...)`.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n","WARNING:tensorflow:Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f8c5f468950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function file_based_input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f8c5f468950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING:tensorflow:From <ipython-input-40-2bc270ee49eb>:22: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","\n","type(output_layer): <class 'tensorflow.python.framework.ops.Tensor'>\n","output_layer:\n"," Tensor(\"bert/pooler/dense/Tanh:0\", shape=(?, 768), dtype=float32)\n","\n","type(hidden_size): <class 'int'>\n","hidden_size:\n"," 768\n","\n","type(output_weights):\n"," <class 'tensorflow.python.ops.variables.RefVariable'>\n","output_weights:\n"," <tf.Variable 'output_weights:0' shape=(60, 768) dtype=float32_ref>\n","\n","type(logits): <class 'tensorflow.python.framework.ops.Tensor'>\n","logits: Tensor(\"loss/BiasAdd:0\", shape=(?, 60), dtype=float32)\n","INFO:tensorflow:\n","num_labels:60;\n","logits:Tensor(\"loss/BiasAdd:0\", shape=(?, 60), dtype=float32);\n","labels:Tensor(\"loss/Cast:0\", shape=(?, 60), dtype=float32)\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","INFO:tensorflow:**** Trainable Variables ****\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-06-19T21:16:59Z\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/BERT/models/fine-tuned/model/model.ckpt-1023\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Finished evaluation at 2020-06-19-21:18:02\n","INFO:tensorflow:Saving dict for global step 1023: 0 = 0.96334314, 1 = 0.97171515, 10 = 0.60036504, 11 = 0.9630305, 12 = 0.78747934, 13 = 0.84463394, 14 = 0.7683457, 15 = 0.9156571, 16 = 0.59959704, 17 = 0.9762473, 18 = 0.9494256, 19 = 0.9440703, 2 = 0.48952392, 20 = 0.7793925, 21 = 0.9708494, 22 = 0.49743307, 23 = 0.9190171, 24 = 0.89971155, 25 = 0.96246517, 26 = 0.96388084, 27 = 0.84461254, 28 = 0.9744985, 29 = 0.52944237, 3 = 0.9504507, 30 = 0.97148883, 31 = 0.73147434, 32 = 0.9743613, 33 = 0.9640601, 34 = 0.96746325, 35 = 0.9145992, 36 = 0.9720437, 37 = 0.83398944, 38 = 0.73240006, 39 = 0.96469635, 4 = 0.95986015, 40 = 0.9681299, 41 = 0.7071643, 42 = 0.9743399, 43 = 0.97156864, 44 = 0.65396065, 45 = 0.8459904, 46 = 0.96248746, 47 = 0.77618706, 48 = 0.96671516, 49 = 0.94760144, 5 = 0.96425986, 50 = 0.909461, 51 = 0.9611664, 52 = 0.9219429, 53 = 0.52132434, 54 = 0.75347084, 55 = 0.7764268, 56 = 0.5324786, 57 = 0.77090496, 58 = 0.95804656, 59 = 0.5699552, 6 = 0.7996618, 7 = 0.96758693, 8 = 0.97189426, 9 = 0.9735312, eval_loss = 0.12462726, global_step = 1023, loss = 0.124662966\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1023: /content/drive/My Drive/BERT/models/fine-tuned/model/model.ckpt-1023\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wrTNGDE9RqYC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592601485908,"user_tz":300,"elapsed":358754,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"c63c3cf4-055c-4eba-e865-ae95b3f6f067"},"source":["# Show the results of the Evaluation\n","output_eval_file = os.path.join(path_folder_eval_output, \"eval_results.txt\")\n","with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n","    tf.logging.info(\"***** Eval results *****\")\n","    for key in sorted(result.keys()):\n","        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n","        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:***** Eval results *****\n","INFO:tensorflow:  0 = 0.96334314\n","INFO:tensorflow:  1 = 0.97171515\n","INFO:tensorflow:  10 = 0.60036504\n","INFO:tensorflow:  11 = 0.9630305\n","INFO:tensorflow:  12 = 0.78747934\n","INFO:tensorflow:  13 = 0.84463394\n","INFO:tensorflow:  14 = 0.7683457\n","INFO:tensorflow:  15 = 0.9156571\n","INFO:tensorflow:  16 = 0.59959704\n","INFO:tensorflow:  17 = 0.9762473\n","INFO:tensorflow:  18 = 0.9494256\n","INFO:tensorflow:  19 = 0.9440703\n","INFO:tensorflow:  2 = 0.48952392\n","INFO:tensorflow:  20 = 0.7793925\n","INFO:tensorflow:  21 = 0.9708494\n","INFO:tensorflow:  22 = 0.49743307\n","INFO:tensorflow:  23 = 0.9190171\n","INFO:tensorflow:  24 = 0.89971155\n","INFO:tensorflow:  25 = 0.96246517\n","INFO:tensorflow:  26 = 0.96388084\n","INFO:tensorflow:  27 = 0.84461254\n","INFO:tensorflow:  28 = 0.9744985\n","INFO:tensorflow:  29 = 0.52944237\n","INFO:tensorflow:  3 = 0.9504507\n","INFO:tensorflow:  30 = 0.97148883\n","INFO:tensorflow:  31 = 0.73147434\n","INFO:tensorflow:  32 = 0.9743613\n","INFO:tensorflow:  33 = 0.9640601\n","INFO:tensorflow:  34 = 0.96746325\n","INFO:tensorflow:  35 = 0.9145992\n","INFO:tensorflow:  36 = 0.9720437\n","INFO:tensorflow:  37 = 0.83398944\n","INFO:tensorflow:  38 = 0.73240006\n","INFO:tensorflow:  39 = 0.96469635\n","INFO:tensorflow:  4 = 0.95986015\n","INFO:tensorflow:  40 = 0.9681299\n","INFO:tensorflow:  41 = 0.7071643\n","INFO:tensorflow:  42 = 0.9743399\n","INFO:tensorflow:  43 = 0.97156864\n","INFO:tensorflow:  44 = 0.65396065\n","INFO:tensorflow:  45 = 0.8459904\n","INFO:tensorflow:  46 = 0.96248746\n","INFO:tensorflow:  47 = 0.77618706\n","INFO:tensorflow:  48 = 0.96671516\n","INFO:tensorflow:  49 = 0.94760144\n","INFO:tensorflow:  5 = 0.96425986\n","INFO:tensorflow:  50 = 0.909461\n","INFO:tensorflow:  51 = 0.9611664\n","INFO:tensorflow:  52 = 0.9219429\n","INFO:tensorflow:  53 = 0.52132434\n","INFO:tensorflow:  54 = 0.75347084\n","INFO:tensorflow:  55 = 0.7764268\n","INFO:tensorflow:  56 = 0.5324786\n","INFO:tensorflow:  57 = 0.77090496\n","INFO:tensorflow:  58 = 0.95804656\n","INFO:tensorflow:  59 = 0.5699552\n","INFO:tensorflow:  6 = 0.7996618\n","INFO:tensorflow:  7 = 0.96758693\n","INFO:tensorflow:  8 = 0.97189426\n","INFO:tensorflow:  9 = 0.9735312\n","INFO:tensorflow:  eval_loss = 0.12462726\n","INFO:tensorflow:  global_step = 1023\n","INFO:tensorflow:  loss = 0.124662966\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"18kVHEKaE1rI","colab_type":"text"},"source":["## 3.5 Probar el modelo (Test)"]},{"cell_type":"markdown","metadata":{"id":"GiDUPEfJTAzd","colab_type":"text"},"source":["### 3.5.1 Predecir una muestra de ejemplos "]},{"cell_type":"code","metadata":{"id":"ZbXThPV8E8lg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592601485908,"user_tz":300,"elapsed":358747,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"2a65b3dd-edce-40e9-bf78-5dc13d52a351"},"source":["# Print Current length of the Test Set\n","print(len(x_test))\n","\n","# Testing only a small sample\n","# https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n","x_test = x_test.sample(frac=0.2).reset_index(drop=True)\n","\n","# Convert the data example instances of class InputExample\n","predict_examples = create_examples(x_test, idx_column_index, idx_column_text, idx_columns_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12128\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8tIaLq96RFIL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1NPKG1K93_bv4Sdb7GhnD6sJZ5EDOmCIo"},"executionInfo":{"status":"ok","timestamp":1592601498559,"user_tz":300,"elapsed":371382,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"897c6209-ab18-41be-c25f-ea5044d2e9f9"},"source":["# Convert Examples to Features\n","test_features = convert_examples_to_features(\n","    predict_examples,\n","    MAX_SEQ_LENGTH,\n","    tokenizer)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"wr_GPwpVRumD","colab_type":"code","colab":{}},"source":["def input_fn_builder(features, seq_length, is_training, drop_remainder):\n","  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","  all_input_ids = []\n","  all_input_mask = []\n","  all_segment_ids = []\n","  all_label_ids = []\n","\n","  for feature in features:\n","    all_input_ids.append(feature.input_ids)\n","    all_input_mask.append(feature.input_mask)\n","    all_segment_ids.append(feature.segment_ids)\n","    all_label_ids.append(feature.label_ids)\n","\n","  def input_fn(params):\n","    \"\"\"The actual input function.\"\"\"\n","    batch_size = params[\"batch_size\"]\n","\n","    num_examples = len(features)\n","\n","    # This is for demo purposes and does NOT scale to large data sets. We do\n","    # not use Dataset.from_generator() because that uses tf.py_func which is\n","    # not TPU compatible. The right way to load data is with TFRecordReader.\n","    d = tf.data.Dataset.from_tensor_slices({\n","        \"input_ids\":\n","            tf.constant(\n","                all_input_ids, shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"input_mask\":\n","            tf.constant(\n","                all_input_mask,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"segment_ids\":\n","            tf.constant(\n","                all_segment_ids,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"label_ids\":\n","            tf.constant(all_label_ids, shape=[num_examples, len(LABEL_COLUMNS)], dtype=tf.int32),\n","    })\n","\n","    if is_training:\n","      d = d.repeat()\n","      d = d.shuffle(buffer_size=100)\n","\n","    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","    return d\n","\n","  return input_fn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SkemAQmSRIRc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1592601721628,"user_tz":300,"elapsed":943,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"1734ff42-ec83-40b0-86b4-e7d6cdcae38a"},"source":["%%time\n","print('Beginning Predictions!')\n","current_time = datetime.now()\n","\n","predict_input_fn = input_fn_builder(features=test_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n","predictions = estimator.predict(predict_input_fn)\n","print(\"Prediction took time \", datetime.now() - current_time)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Beginning Predictions!\n","Prediction took time  0:00:00.001317\n","CPU times: user 1.51 ms, sys: 0 ns, total: 1.51 ms\n","Wall time: 1.48 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JNgLSl8nRQ3p","colab_type":"code","colab":{}},"source":["def create_output(predictions):\n","    probabilities = []\n","    for (i, prediction) in enumerate(predictions):\n","        preds = prediction[\"probabilities\"]\n","        probabilities.append(preds)\n","    dff = pd.DataFrame(probabilities)\n","    dff.columns = LABEL_COLUMNS\n","    \n","    return dff"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbVR47rXVHV_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592601727111,"user_tz":300,"elapsed":596,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"0a31b761-af69-403a-a1a0-cfd78588e157"},"source":["path_folder_output_dataframes = path_folder_root + 'NLP/projects/bert/output/'\n","path_folder_output_dataframes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/NLP/projects/bert/output/'"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"kbSmW9pXRcgb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":650},"executionInfo":{"status":"ok","timestamp":1592601746510,"user_tz":300,"elapsed":17323,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"349d2c6e-57ce-4994-e3e0-a8ca94bd30ea"},"source":["# Generar DataFrame con Predicciones\n","output_df = create_output(predictions)\n","output_df.sample(3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","\n","type(output_layer): <class 'tensorflow.python.framework.ops.Tensor'>\n","output_layer:\n"," Tensor(\"bert/pooler/dense/Tanh:0\", shape=(?, 768), dtype=float32)\n","\n","type(hidden_size): <class 'int'>\n","hidden_size:\n"," 768\n","\n","type(output_weights):\n"," <class 'tensorflow.python.ops.variables.RefVariable'>\n","output_weights:\n"," <tf.Variable 'output_weights:0' shape=(60, 768) dtype=float32_ref>\n","\n","type(logits): <class 'tensorflow.python.framework.ops.Tensor'>\n","logits: Tensor(\"loss/BiasAdd:0\", shape=(?, 60), dtype=float32)\n","INFO:tensorflow:\n","num_labels:60;\n","logits:Tensor(\"loss/BiasAdd:0\", shape=(?, 60), dtype=float32);\n","labels:Tensor(\"loss/Cast:0\", shape=(?, 60), dtype=float32)\n","INFO:tensorflow:**** Trainable Variables ****\n","mode: infer probabilities: Tensor(\"loss/Sigmoid:0\", shape=(?, 60), dtype=float32)\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/BERT/models/fine-tuned/model/model.ckpt-1023\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>activation</th>\n","      <th>activity</th>\n","      <th>age</th>\n","      <th>antibody</th>\n","      <th>antigen</th>\n","      <th>approach</th>\n","      <th>background</th>\n","      <th>base</th>\n","      <th>binding</th>\n","      <th>calf</th>\n","      <th>cell_line</th>\n","      <th>child</th>\n","      <th>complex</th>\n","      <th>conclusion</th>\n","      <th>country</th>\n","      <th>covid</th>\n","      <th>day</th>\n","      <th>de</th>\n","      <th>detect</th>\n","      <th>detection</th>\n","      <th>development</th>\n","      <th>diarrhea</th>\n","      <th>effect</th>\n","      <th>express</th>\n","      <th>expression</th>\n","      <th>gene</th>\n","      <th>genome</th>\n","      <th>global</th>\n","      <th>health</th>\n","      <th>increase</th>\n","      <th>inhibit</th>\n","      <th>interaction</th>\n","      <th>le</th>\n","      <th>mechanism</th>\n","      <th>model</th>\n","      <th>mouse</th>\n","      <th>national</th>\n","      <th>need</th>\n","      <th>objective</th>\n","      <th>patient</th>\n","      <th>pig</th>\n","      <th>population</th>\n","      <th>public_health</th>\n","      <th>que</th>\n","      <th>replication</th>\n","      <th>research</th>\n","      <th>respiratory</th>\n","      <th>review</th>\n","      <th>role</th>\n","      <th>sample</th>\n","      <th>sarscov</th>\n","      <th>sequence</th>\n","      <th>severe_acute_respiratory</th>\n","      <th>significantly</th>\n","      <th>structure</th>\n","      <th>surveillance</th>\n","      <th>total</th>\n","      <th>understand</th>\n","      <th>vaccine</th>\n","      <th>vitro</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1405</th>\n","      <td>0.041591</td>\n","      <td>0.109260</td>\n","      <td>0.025052</td>\n","      <td>0.046383</td>\n","      <td>0.033486</td>\n","      <td>0.028322</td>\n","      <td>0.019834</td>\n","      <td>0.036013</td>\n","      <td>0.139845</td>\n","      <td>0.087812</td>\n","      <td>0.040006</td>\n","      <td>0.049463</td>\n","      <td>0.033714</td>\n","      <td>0.023504</td>\n","      <td>0.032044</td>\n","      <td>0.055475</td>\n","      <td>0.014140</td>\n","      <td>0.014198</td>\n","      <td>0.037706</td>\n","      <td>0.042095</td>\n","      <td>0.043863</td>\n","      <td>0.114097</td>\n","      <td>0.017213</td>\n","      <td>0.040571</td>\n","      <td>0.056200</td>\n","      <td>0.438958</td>\n","      <td>0.428051</td>\n","      <td>0.043518</td>\n","      <td>0.035001</td>\n","      <td>0.035014</td>\n","      <td>0.115531</td>\n","      <td>0.037769</td>\n","      <td>0.012798</td>\n","      <td>0.060211</td>\n","      <td>0.031389</td>\n","      <td>0.046463</td>\n","      <td>0.027652</td>\n","      <td>0.030082</td>\n","      <td>0.030493</td>\n","      <td>0.035361</td>\n","      <td>0.068078</td>\n","      <td>0.036574</td>\n","      <td>0.035385</td>\n","      <td>0.013365</td>\n","      <td>0.035915</td>\n","      <td>0.022329</td>\n","      <td>0.036405</td>\n","      <td>0.044741</td>\n","      <td>0.065566</td>\n","      <td>0.045612</td>\n","      <td>0.026918</td>\n","      <td>0.466280</td>\n","      <td>0.063731</td>\n","      <td>0.026788</td>\n","      <td>0.031047</td>\n","      <td>0.030203</td>\n","      <td>0.024833</td>\n","      <td>0.051557</td>\n","      <td>0.027521</td>\n","      <td>0.052949</td>\n","    </tr>\n","    <tr>\n","      <th>942</th>\n","      <td>0.120588</td>\n","      <td>0.123509</td>\n","      <td>0.028187</td>\n","      <td>0.133583</td>\n","      <td>0.161950</td>\n","      <td>0.028609</td>\n","      <td>0.022093</td>\n","      <td>0.028761</td>\n","      <td>0.135795</td>\n","      <td>0.138604</td>\n","      <td>0.050350</td>\n","      <td>0.043997</td>\n","      <td>0.035669</td>\n","      <td>0.025159</td>\n","      <td>0.020950</td>\n","      <td>0.043600</td>\n","      <td>0.017941</td>\n","      <td>0.022145</td>\n","      <td>0.030287</td>\n","      <td>0.047734</td>\n","      <td>0.049557</td>\n","      <td>0.136181</td>\n","      <td>0.032763</td>\n","      <td>0.145850</td>\n","      <td>0.138025</td>\n","      <td>0.074183</td>\n","      <td>0.061698</td>\n","      <td>0.036155</td>\n","      <td>0.031009</td>\n","      <td>0.030791</td>\n","      <td>0.147415</td>\n","      <td>0.035165</td>\n","      <td>0.018388</td>\n","      <td>0.124654</td>\n","      <td>0.027663</td>\n","      <td>0.138030</td>\n","      <td>0.025215</td>\n","      <td>0.032055</td>\n","      <td>0.027865</td>\n","      <td>0.029104</td>\n","      <td>0.119084</td>\n","      <td>0.031828</td>\n","      <td>0.022492</td>\n","      <td>0.011466</td>\n","      <td>0.060252</td>\n","      <td>0.022397</td>\n","      <td>0.034016</td>\n","      <td>0.042538</td>\n","      <td>0.115440</td>\n","      <td>0.031001</td>\n","      <td>0.040533</td>\n","      <td>0.055019</td>\n","      <td>0.038724</td>\n","      <td>0.029098</td>\n","      <td>0.045538</td>\n","      <td>0.026290</td>\n","      <td>0.020308</td>\n","      <td>0.044891</td>\n","      <td>0.142554</td>\n","      <td>0.047108</td>\n","    </tr>\n","    <tr>\n","      <th>1841</th>\n","      <td>0.045517</td>\n","      <td>0.121265</td>\n","      <td>0.024840</td>\n","      <td>0.044935</td>\n","      <td>0.033735</td>\n","      <td>0.027962</td>\n","      <td>0.019414</td>\n","      <td>0.037384</td>\n","      <td>0.151539</td>\n","      <td>0.083295</td>\n","      <td>0.040427</td>\n","      <td>0.049053</td>\n","      <td>0.034777</td>\n","      <td>0.023272</td>\n","      <td>0.032105</td>\n","      <td>0.055671</td>\n","      <td>0.014383</td>\n","      <td>0.014278</td>\n","      <td>0.036297</td>\n","      <td>0.041366</td>\n","      <td>0.044049</td>\n","      <td>0.104777</td>\n","      <td>0.017221</td>\n","      <td>0.041872</td>\n","      <td>0.055703</td>\n","      <td>0.444396</td>\n","      <td>0.428081</td>\n","      <td>0.044775</td>\n","      <td>0.033921</td>\n","      <td>0.034308</td>\n","      <td>0.127341</td>\n","      <td>0.037962</td>\n","      <td>0.012976</td>\n","      <td>0.064073</td>\n","      <td>0.031934</td>\n","      <td>0.047638</td>\n","      <td>0.026953</td>\n","      <td>0.030479</td>\n","      <td>0.030346</td>\n","      <td>0.034076</td>\n","      <td>0.063247</td>\n","      <td>0.035908</td>\n","      <td>0.035312</td>\n","      <td>0.013286</td>\n","      <td>0.035517</td>\n","      <td>0.022349</td>\n","      <td>0.034891</td>\n","      <td>0.047031</td>\n","      <td>0.069399</td>\n","      <td>0.044435</td>\n","      <td>0.026245</td>\n","      <td>0.466940</td>\n","      <td>0.060374</td>\n","      <td>0.026370</td>\n","      <td>0.031505</td>\n","      <td>0.028839</td>\n","      <td>0.024809</td>\n","      <td>0.049349</td>\n","      <td>0.027159</td>\n","      <td>0.052755</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      activation  activity       age  ...  understand   vaccine     vitro\n","1405    0.041591  0.109260  0.025052  ...    0.051557  0.027521  0.052949\n","942     0.120588  0.123509  0.028187  ...    0.044891  0.142554  0.047108\n","1841    0.045517  0.121265  0.024840  ...    0.049349  0.027159  0.052755\n","\n","[3 rows x 60 columns]"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"markdown","metadata":{"id":"MSc_jJ8aTJG2","colab_type":"text"},"source":["### 3.5.2 Verificar Precisión de la Predicción"]},{"cell_type":"code","metadata":{"id":"XcRHL66AdIad","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":475},"executionInfo":{"status":"ok","timestamp":1592601751087,"user_tz":300,"elapsed":689,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"d9029841-7261-4608-9d2d-d9fc08e9b22c"},"source":["# 1. Agregar columna 'cord_uid' a dataset de predicciones\n","output_df = pd.concat([x_test['cord_uid'], output_df], axis=1)\n","\n","# 2. Establecer como índice de 'output_df' la columna 'cord_uid'\n","output_df.set_index('cord_uid')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>activation</th>\n","      <th>activity</th>\n","      <th>age</th>\n","      <th>antibody</th>\n","      <th>antigen</th>\n","      <th>approach</th>\n","      <th>background</th>\n","      <th>base</th>\n","      <th>binding</th>\n","      <th>calf</th>\n","      <th>cell_line</th>\n","      <th>child</th>\n","      <th>complex</th>\n","      <th>conclusion</th>\n","      <th>country</th>\n","      <th>covid</th>\n","      <th>day</th>\n","      <th>de</th>\n","      <th>detect</th>\n","      <th>detection</th>\n","      <th>development</th>\n","      <th>diarrhea</th>\n","      <th>effect</th>\n","      <th>express</th>\n","      <th>expression</th>\n","      <th>gene</th>\n","      <th>genome</th>\n","      <th>global</th>\n","      <th>health</th>\n","      <th>increase</th>\n","      <th>inhibit</th>\n","      <th>interaction</th>\n","      <th>le</th>\n","      <th>mechanism</th>\n","      <th>model</th>\n","      <th>mouse</th>\n","      <th>national</th>\n","      <th>need</th>\n","      <th>objective</th>\n","      <th>patient</th>\n","      <th>pig</th>\n","      <th>population</th>\n","      <th>public_health</th>\n","      <th>que</th>\n","      <th>replication</th>\n","      <th>research</th>\n","      <th>respiratory</th>\n","      <th>review</th>\n","      <th>role</th>\n","      <th>sample</th>\n","      <th>sarscov</th>\n","      <th>sequence</th>\n","      <th>severe_acute_respiratory</th>\n","      <th>significantly</th>\n","      <th>structure</th>\n","      <th>surveillance</th>\n","      <th>total</th>\n","      <th>understand</th>\n","      <th>vaccine</th>\n","      <th>vitro</th>\n","    </tr>\n","    <tr>\n","      <th>cord_uid</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ibwkkeud</th>\n","      <td>0.028667</td>\n","      <td>0.027395</td>\n","      <td>0.023224</td>\n","      <td>0.044958</td>\n","      <td>0.033098</td>\n","      <td>0.022080</td>\n","      <td>0.032954</td>\n","      <td>0.013139</td>\n","      <td>0.027864</td>\n","      <td>0.176477</td>\n","      <td>0.030703</td>\n","      <td>0.418374</td>\n","      <td>0.016769</td>\n","      <td>0.032697</td>\n","      <td>0.020871</td>\n","      <td>0.091439</td>\n","      <td>0.025780</td>\n","      <td>0.015459</td>\n","      <td>0.065483</td>\n","      <td>0.057762</td>\n","      <td>0.024293</td>\n","      <td>0.187056</td>\n","      <td>0.028501</td>\n","      <td>0.026803</td>\n","      <td>0.036035</td>\n","      <td>0.056498</td>\n","      <td>0.056821</td>\n","      <td>0.025014</td>\n","      <td>0.045406</td>\n","      <td>0.026315</td>\n","      <td>0.026052</td>\n","      <td>0.022519</td>\n","      <td>0.014705</td>\n","      <td>0.036504</td>\n","      <td>0.017958</td>\n","      <td>0.025156</td>\n","      <td>0.029943</td>\n","      <td>0.020735</td>\n","      <td>0.032864</td>\n","      <td>0.434070</td>\n","      <td>0.208711</td>\n","      <td>0.027325</td>\n","      <td>0.030699</td>\n","      <td>0.022605</td>\n","      <td>0.023384</td>\n","      <td>0.023339</td>\n","      <td>0.378751</td>\n","      <td>0.022428</td>\n","      <td>0.029190</td>\n","      <td>0.050150</td>\n","      <td>0.074052</td>\n","      <td>0.044608</td>\n","      <td>0.075881</td>\n","      <td>0.030168</td>\n","      <td>0.024626</td>\n","      <td>0.026151</td>\n","      <td>0.019880</td>\n","      <td>0.031329</td>\n","      <td>0.037387</td>\n","      <td>0.028950</td>\n","    </tr>\n","    <tr>\n","      <th>xeaqfxqj</th>\n","      <td>0.031622</td>\n","      <td>0.034656</td>\n","      <td>0.034714</td>\n","      <td>0.014270</td>\n","      <td>0.025053</td>\n","      <td>0.069526</td>\n","      <td>0.032488</td>\n","      <td>0.064269</td>\n","      <td>0.034312</td>\n","      <td>0.030154</td>\n","      <td>0.023169</td>\n","      <td>0.033309</td>\n","      <td>0.025140</td>\n","      <td>0.034914</td>\n","      <td>0.028059</td>\n","      <td>0.076332</td>\n","      <td>0.035276</td>\n","      <td>0.018100</td>\n","      <td>0.029244</td>\n","      <td>0.023362</td>\n","      <td>0.048411</td>\n","      <td>0.024911</td>\n","      <td>0.020315</td>\n","      <td>0.026599</td>\n","      <td>0.022781</td>\n","      <td>0.035283</td>\n","      <td>0.029257</td>\n","      <td>0.094938</td>\n","      <td>0.417678</td>\n","      <td>0.015700</td>\n","      <td>0.029135</td>\n","      <td>0.026257</td>\n","      <td>0.025072</td>\n","      <td>0.040976</td>\n","      <td>0.073902</td>\n","      <td>0.029083</td>\n","      <td>0.488574</td>\n","      <td>0.095009</td>\n","      <td>0.040243</td>\n","      <td>0.036545</td>\n","      <td>0.036948</td>\n","      <td>0.029299</td>\n","      <td>0.461766</td>\n","      <td>0.020021</td>\n","      <td>0.019074</td>\n","      <td>0.143071</td>\n","      <td>0.049306</td>\n","      <td>0.054881</td>\n","      <td>0.032436</td>\n","      <td>0.025182</td>\n","      <td>0.069045</td>\n","      <td>0.024590</td>\n","      <td>0.071571</td>\n","      <td>0.019571</td>\n","      <td>0.027153</td>\n","      <td>0.034595</td>\n","      <td>0.023888</td>\n","      <td>0.044110</td>\n","      <td>0.020255</td>\n","      <td>0.022392</td>\n","    </tr>\n","    <tr>\n","      <th>jwcgf3op</th>\n","      <td>0.031740</td>\n","      <td>0.058197</td>\n","      <td>0.035980</td>\n","      <td>0.017096</td>\n","      <td>0.032305</td>\n","      <td>0.149114</td>\n","      <td>0.022616</td>\n","      <td>0.144578</td>\n","      <td>0.053514</td>\n","      <td>0.054769</td>\n","      <td>0.040866</td>\n","      <td>0.028896</td>\n","      <td>0.050778</td>\n","      <td>0.037108</td>\n","      <td>0.049245</td>\n","      <td>0.065606</td>\n","      <td>0.034802</td>\n","      <td>0.019007</td>\n","      <td>0.032973</td>\n","      <td>0.022633</td>\n","      <td>0.083712</td>\n","      <td>0.036522</td>\n","      <td>0.023969</td>\n","      <td>0.032815</td>\n","      <td>0.036137</td>\n","      <td>0.073311</td>\n","      <td>0.078480</td>\n","      <td>0.105515</td>\n","      <td>0.104124</td>\n","      <td>0.024314</td>\n","      <td>0.049374</td>\n","      <td>0.053752</td>\n","      <td>0.026354</td>\n","      <td>0.031908</td>\n","      <td>0.152984</td>\n","      <td>0.033228</td>\n","      <td>0.105098</td>\n","      <td>0.112298</td>\n","      <td>0.036444</td>\n","      <td>0.022421</td>\n","      <td>0.051372</td>\n","      <td>0.042156</td>\n","      <td>0.082446</td>\n","      <td>0.023972</td>\n","      <td>0.026365</td>\n","      <td>0.112591</td>\n","      <td>0.019507</td>\n","      <td>0.101137</td>\n","      <td>0.038805</td>\n","      <td>0.027919</td>\n","      <td>0.044274</td>\n","      <td>0.072099</td>\n","      <td>0.062128</td>\n","      <td>0.030498</td>\n","      <td>0.050538</td>\n","      <td>0.049096</td>\n","      <td>0.032893</td>\n","      <td>0.093189</td>\n","      <td>0.019133</td>\n","      <td>0.027454</td>\n","    </tr>\n","    <tr>\n","      <th>2h8yfriz</th>\n","      <td>0.422114</td>\n","      <td>0.120696</td>\n","      <td>0.026155</td>\n","      <td>0.038293</td>\n","      <td>0.061696</td>\n","      <td>0.022371</td>\n","      <td>0.020204</td>\n","      <td>0.023885</td>\n","      <td>0.135023</td>\n","      <td>0.094671</td>\n","      <td>0.030747</td>\n","      <td>0.035876</td>\n","      <td>0.034473</td>\n","      <td>0.018403</td>\n","      <td>0.022015</td>\n","      <td>0.065028</td>\n","      <td>0.022227</td>\n","      <td>0.017724</td>\n","      <td>0.020309</td>\n","      <td>0.030704</td>\n","      <td>0.040903</td>\n","      <td>0.089578</td>\n","      <td>0.029620</td>\n","      <td>0.088471</td>\n","      <td>0.066863</td>\n","      <td>0.060130</td>\n","      <td>0.057775</td>\n","      <td>0.027480</td>\n","      <td>0.037521</td>\n","      <td>0.029344</td>\n","      <td>0.128517</td>\n","      <td>0.028267</td>\n","      <td>0.014021</td>\n","      <td>0.402095</td>\n","      <td>0.025927</td>\n","      <td>0.081736</td>\n","      <td>0.023447</td>\n","      <td>0.031335</td>\n","      <td>0.020003</td>\n","      <td>0.032259</td>\n","      <td>0.069239</td>\n","      <td>0.034502</td>\n","      <td>0.033964</td>\n","      <td>0.008540</td>\n","      <td>0.041718</td>\n","      <td>0.024934</td>\n","      <td>0.029741</td>\n","      <td>0.043482</td>\n","      <td>0.428025</td>\n","      <td>0.028063</td>\n","      <td>0.045402</td>\n","      <td>0.044760</td>\n","      <td>0.034346</td>\n","      <td>0.025786</td>\n","      <td>0.030872</td>\n","      <td>0.019079</td>\n","      <td>0.019356</td>\n","      <td>0.032553</td>\n","      <td>0.052100</td>\n","      <td>0.025562</td>\n","    </tr>\n","    <tr>\n","      <th>yz9yhnbx</th>\n","      <td>0.078960</td>\n","      <td>0.047074</td>\n","      <td>0.031817</td>\n","      <td>0.028189</td>\n","      <td>0.032203</td>\n","      <td>0.026648</td>\n","      <td>0.040866</td>\n","      <td>0.030593</td>\n","      <td>0.045312</td>\n","      <td>0.047382</td>\n","      <td>0.024783</td>\n","      <td>0.083727</td>\n","      <td>0.021833</td>\n","      <td>0.022293</td>\n","      <td>0.025748</td>\n","      <td>0.265174</td>\n","      <td>0.026135</td>\n","      <td>0.014316</td>\n","      <td>0.034065</td>\n","      <td>0.031791</td>\n","      <td>0.022722</td>\n","      <td>0.044324</td>\n","      <td>0.032725</td>\n","      <td>0.025346</td>\n","      <td>0.028998</td>\n","      <td>0.033580</td>\n","      <td>0.037104</td>\n","      <td>0.027040</td>\n","      <td>0.083454</td>\n","      <td>0.023276</td>\n","      <td>0.041405</td>\n","      <td>0.020224</td>\n","      <td>0.015597</td>\n","      <td>0.081360</td>\n","      <td>0.033784</td>\n","      <td>0.032933</td>\n","      <td>0.058154</td>\n","      <td>0.027203</td>\n","      <td>0.018250</td>\n","      <td>0.089469</td>\n","      <td>0.046211</td>\n","      <td>0.026701</td>\n","      <td>0.090906</td>\n","      <td>0.015312</td>\n","      <td>0.026168</td>\n","      <td>0.026969</td>\n","      <td>0.096431</td>\n","      <td>0.028088</td>\n","      <td>0.080594</td>\n","      <td>0.025568</td>\n","      <td>0.321814</td>\n","      <td>0.028325</td>\n","      <td>0.254216</td>\n","      <td>0.031538</td>\n","      <td>0.018335</td>\n","      <td>0.020674</td>\n","      <td>0.021570</td>\n","      <td>0.031888</td>\n","      <td>0.031802</td>\n","      <td>0.023979</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>kwikwxge</th>\n","      <td>0.025401</td>\n","      <td>0.024342</td>\n","      <td>0.046884</td>\n","      <td>0.031886</td>\n","      <td>0.036740</td>\n","      <td>0.086367</td>\n","      <td>0.043152</td>\n","      <td>0.100205</td>\n","      <td>0.033800</td>\n","      <td>0.068094</td>\n","      <td>0.032854</td>\n","      <td>0.044785</td>\n","      <td>0.031866</td>\n","      <td>0.040469</td>\n","      <td>0.041602</td>\n","      <td>0.284086</td>\n","      <td>0.043751</td>\n","      <td>0.017538</td>\n","      <td>0.042516</td>\n","      <td>0.031039</td>\n","      <td>0.033869</td>\n","      <td>0.061107</td>\n","      <td>0.030733</td>\n","      <td>0.021752</td>\n","      <td>0.031646</td>\n","      <td>0.043779</td>\n","      <td>0.048261</td>\n","      <td>0.047891</td>\n","      <td>0.102448</td>\n","      <td>0.030701</td>\n","      <td>0.030012</td>\n","      <td>0.038065</td>\n","      <td>0.021656</td>\n","      <td>0.029374</td>\n","      <td>0.141926</td>\n","      <td>0.027040</td>\n","      <td>0.092238</td>\n","      <td>0.059617</td>\n","      <td>0.037885</td>\n","      <td>0.044384</td>\n","      <td>0.072103</td>\n","      <td>0.041043</td>\n","      <td>0.097543</td>\n","      <td>0.023713</td>\n","      <td>0.026036</td>\n","      <td>0.048353</td>\n","      <td>0.052555</td>\n","      <td>0.040428</td>\n","      <td>0.028612</td>\n","      <td>0.026593</td>\n","      <td>0.347442</td>\n","      <td>0.046008</td>\n","      <td>0.322206</td>\n","      <td>0.046490</td>\n","      <td>0.029372</td>\n","      <td>0.031823</td>\n","      <td>0.038471</td>\n","      <td>0.051011</td>\n","      <td>0.025200</td>\n","      <td>0.031354</td>\n","    </tr>\n","    <tr>\n","      <th>uznmbvom</th>\n","      <td>0.038476</td>\n","      <td>0.099524</td>\n","      <td>0.030632</td>\n","      <td>0.034564</td>\n","      <td>0.035498</td>\n","      <td>0.057381</td>\n","      <td>0.020539</td>\n","      <td>0.071617</td>\n","      <td>0.105384</td>\n","      <td>0.075409</td>\n","      <td>0.041713</td>\n","      <td>0.034644</td>\n","      <td>0.046918</td>\n","      <td>0.027436</td>\n","      <td>0.034486</td>\n","      <td>0.048266</td>\n","      <td>0.017091</td>\n","      <td>0.015509</td>\n","      <td>0.039171</td>\n","      <td>0.037551</td>\n","      <td>0.063660</td>\n","      <td>0.072040</td>\n","      <td>0.016928</td>\n","      <td>0.043230</td>\n","      <td>0.053790</td>\n","      <td>0.349606</td>\n","      <td>0.375680</td>\n","      <td>0.065441</td>\n","      <td>0.039654</td>\n","      <td>0.030657</td>\n","      <td>0.091175</td>\n","      <td>0.045166</td>\n","      <td>0.016111</td>\n","      <td>0.053700</td>\n","      <td>0.068404</td>\n","      <td>0.053290</td>\n","      <td>0.036223</td>\n","      <td>0.050875</td>\n","      <td>0.033998</td>\n","      <td>0.021949</td>\n","      <td>0.057529</td>\n","      <td>0.038790</td>\n","      <td>0.038341</td>\n","      <td>0.014936</td>\n","      <td>0.036947</td>\n","      <td>0.035089</td>\n","      <td>0.022336</td>\n","      <td>0.063851</td>\n","      <td>0.059813</td>\n","      <td>0.039550</td>\n","      <td>0.025163</td>\n","      <td>0.384988</td>\n","      <td>0.056649</td>\n","      <td>0.028686</td>\n","      <td>0.040190</td>\n","      <td>0.034352</td>\n","      <td>0.027581</td>\n","      <td>0.063193</td>\n","      <td>0.025094</td>\n","      <td>0.046353</td>\n","    </tr>\n","    <tr>\n","      <th>e3zx2kh5</th>\n","      <td>0.407488</td>\n","      <td>0.208614</td>\n","      <td>0.019842</td>\n","      <td>0.023646</td>\n","      <td>0.038713</td>\n","      <td>0.021210</td>\n","      <td>0.015643</td>\n","      <td>0.024563</td>\n","      <td>0.224348</td>\n","      <td>0.046965</td>\n","      <td>0.024613</td>\n","      <td>0.025459</td>\n","      <td>0.030425</td>\n","      <td>0.013605</td>\n","      <td>0.019392</td>\n","      <td>0.058503</td>\n","      <td>0.018359</td>\n","      <td>0.013780</td>\n","      <td>0.015165</td>\n","      <td>0.021001</td>\n","      <td>0.033749</td>\n","      <td>0.046721</td>\n","      <td>0.024560</td>\n","      <td>0.060282</td>\n","      <td>0.044387</td>\n","      <td>0.053567</td>\n","      <td>0.048548</td>\n","      <td>0.024818</td>\n","      <td>0.034688</td>\n","      <td>0.024203</td>\n","      <td>0.215641</td>\n","      <td>0.022753</td>\n","      <td>0.010248</td>\n","      <td>0.396061</td>\n","      <td>0.023182</td>\n","      <td>0.055573</td>\n","      <td>0.027013</td>\n","      <td>0.027214</td>\n","      <td>0.016064</td>\n","      <td>0.025348</td>\n","      <td>0.038488</td>\n","      <td>0.024786</td>\n","      <td>0.038241</td>\n","      <td>0.006880</td>\n","      <td>0.033173</td>\n","      <td>0.024790</td>\n","      <td>0.022037</td>\n","      <td>0.040448</td>\n","      <td>0.431774</td>\n","      <td>0.020704</td>\n","      <td>0.040193</td>\n","      <td>0.039901</td>\n","      <td>0.033855</td>\n","      <td>0.017466</td>\n","      <td>0.027061</td>\n","      <td>0.015767</td>\n","      <td>0.019572</td>\n","      <td>0.027575</td>\n","      <td>0.032675</td>\n","      <td>0.020129</td>\n","    </tr>\n","    <tr>\n","      <th>99q1bhba</th>\n","      <td>0.322154</td>\n","      <td>0.260354</td>\n","      <td>0.017034</td>\n","      <td>0.023164</td>\n","      <td>0.036604</td>\n","      <td>0.017511</td>\n","      <td>0.014398</td>\n","      <td>0.022295</td>\n","      <td>0.264370</td>\n","      <td>0.044094</td>\n","      <td>0.023943</td>\n","      <td>0.023461</td>\n","      <td>0.029588</td>\n","      <td>0.012370</td>\n","      <td>0.018057</td>\n","      <td>0.047451</td>\n","      <td>0.017136</td>\n","      <td>0.012865</td>\n","      <td>0.015253</td>\n","      <td>0.019307</td>\n","      <td>0.032827</td>\n","      <td>0.048885</td>\n","      <td>0.022740</td>\n","      <td>0.057086</td>\n","      <td>0.046284</td>\n","      <td>0.052824</td>\n","      <td>0.056590</td>\n","      <td>0.024075</td>\n","      <td>0.025389</td>\n","      <td>0.022675</td>\n","      <td>0.265787</td>\n","      <td>0.022887</td>\n","      <td>0.009638</td>\n","      <td>0.302947</td>\n","      <td>0.020468</td>\n","      <td>0.055049</td>\n","      <td>0.021600</td>\n","      <td>0.024383</td>\n","      <td>0.014684</td>\n","      <td>0.023592</td>\n","      <td>0.038412</td>\n","      <td>0.021314</td>\n","      <td>0.029068</td>\n","      <td>0.006823</td>\n","      <td>0.033861</td>\n","      <td>0.021422</td>\n","      <td>0.019690</td>\n","      <td>0.037892</td>\n","      <td>0.347338</td>\n","      <td>0.019967</td>\n","      <td>0.033686</td>\n","      <td>0.041852</td>\n","      <td>0.031453</td>\n","      <td>0.015452</td>\n","      <td>0.027819</td>\n","      <td>0.014654</td>\n","      <td>0.017950</td>\n","      <td>0.026743</td>\n","      <td>0.032769</td>\n","      <td>0.020329</td>\n","    </tr>\n","    <tr>\n","      <th>xcgvhhk4</th>\n","      <td>0.029632</td>\n","      <td>0.024694</td>\n","      <td>0.021703</td>\n","      <td>0.027876</td>\n","      <td>0.025436</td>\n","      <td>0.032834</td>\n","      <td>0.045379</td>\n","      <td>0.018809</td>\n","      <td>0.027026</td>\n","      <td>0.059842</td>\n","      <td>0.022656</td>\n","      <td>0.586155</td>\n","      <td>0.018414</td>\n","      <td>0.045667</td>\n","      <td>0.021246</td>\n","      <td>0.104071</td>\n","      <td>0.029614</td>\n","      <td>0.016747</td>\n","      <td>0.069704</td>\n","      <td>0.051376</td>\n","      <td>0.022044</td>\n","      <td>0.057213</td>\n","      <td>0.034877</td>\n","      <td>0.025297</td>\n","      <td>0.024501</td>\n","      <td>0.038992</td>\n","      <td>0.043264</td>\n","      <td>0.035776</td>\n","      <td>0.060338</td>\n","      <td>0.027522</td>\n","      <td>0.027082</td>\n","      <td>0.022619</td>\n","      <td>0.016465</td>\n","      <td>0.031182</td>\n","      <td>0.020904</td>\n","      <td>0.022405</td>\n","      <td>0.044752</td>\n","      <td>0.029412</td>\n","      <td>0.035587</td>\n","      <td>0.589355</td>\n","      <td>0.082821</td>\n","      <td>0.019377</td>\n","      <td>0.054996</td>\n","      <td>0.029661</td>\n","      <td>0.020728</td>\n","      <td>0.028747</td>\n","      <td>0.583369</td>\n","      <td>0.023156</td>\n","      <td>0.029290</td>\n","      <td>0.050615</td>\n","      <td>0.082606</td>\n","      <td>0.033517</td>\n","      <td>0.101297</td>\n","      <td>0.021759</td>\n","      <td>0.025236</td>\n","      <td>0.022834</td>\n","      <td>0.025876</td>\n","      <td>0.024125</td>\n","      <td>0.026476</td>\n","      <td>0.028909</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2426 rows × 60 columns</p>\n","</div>"],"text/plain":["          activation  activity       age  ...  understand   vaccine     vitro\n","cord_uid                                  ...                                \n","ibwkkeud    0.028667  0.027395  0.023224  ...    0.031329  0.037387  0.028950\n","xeaqfxqj    0.031622  0.034656  0.034714  ...    0.044110  0.020255  0.022392\n","jwcgf3op    0.031740  0.058197  0.035980  ...    0.093189  0.019133  0.027454\n","2h8yfriz    0.422114  0.120696  0.026155  ...    0.032553  0.052100  0.025562\n","yz9yhnbx    0.078960  0.047074  0.031817  ...    0.031888  0.031802  0.023979\n","...              ...       ...       ...  ...         ...       ...       ...\n","kwikwxge    0.025401  0.024342  0.046884  ...    0.051011  0.025200  0.031354\n","uznmbvom    0.038476  0.099524  0.030632  ...    0.063193  0.025094  0.046353\n","e3zx2kh5    0.407488  0.208614  0.019842  ...    0.027575  0.032675  0.020129\n","99q1bhba    0.322154  0.260354  0.017034  ...    0.026743  0.032769  0.020329\n","xcgvhhk4    0.029632  0.024694  0.021703  ...    0.024125  0.026476  0.028909\n","\n","[2426 rows x 60 columns]"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"O83hq0MpbSrc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1592601754389,"user_tz":300,"elapsed":676,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"ebfb45b0-177a-47e1-8338-74742390743f"},"source":["# 3. Determinar columnas a eliminar\n","\n","# a) Obtener todas las columnas del dataframe\n","cols = x_test.columns.values.tolist()\n","\n","# b) Especificar columnas a conservar\n","# NOTA: dinámicamente se incluirán/conservarán las columnas que comiencen con 'word'\n","cols_top_n_words = list(filter(lambda c: 'word_' in c, cols))\n","cols_conservar = ['cord_uid','document','clean','clean_tfidf','best_topic','best_topic_score'] + cols_top_n_words\n","print(cols_conservar)\n","\n","# c) Obtener columnas a eliminar\n","cols_eliminar = [c for c in cols if c not in cols_conservar]\n","print(cols_eliminar)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['cord_uid', 'document', 'clean', 'clean_tfidf', 'best_topic', 'best_topic_score', 'word_0', 'word_1', 'word_2']\n","[]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"23AmD7kLfoKU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":996},"executionInfo":{"status":"ok","timestamp":1592601757770,"user_tz":300,"elapsed":956,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"26da1ea1-903a-4fbe-ab61-f5b1ae0a9fe1"},"source":["# 4. Remover Columnas\n","x_test.drop(cols_eliminar, axis=1, inplace=True)\n","\n","# 5. Establecer como índice de 'output_df' la columna 'cord_uid'\n","x_test.set_index('cord_uid')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>clean</th>\n","      <th>clean_tfidf</th>\n","      <th>best_topic</th>\n","      <th>best_topic_score</th>\n","      <th>word_0</th>\n","      <th>word_1</th>\n","      <th>word_2</th>\n","    </tr>\n","    <tr>\n","      <th>cord_uid</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ibwkkeud</th>\n","      <td>6 Gastroenteritis. Publisher Summary Acute gas...</td>\n","      <td>[[gastroenteritis], [publisher_summary, acute_...</td>\n","      <td>[gastroenteritis, publisher_summary, acute_gas...</td>\n","      <td>topic_5</td>\n","      <td>-3.739231e-12</td>\n","      <td>diarrhea</td>\n","      <td>pig</td>\n","      <td>calf</td>\n","    </tr>\n","    <tr>\n","      <th>xeaqfxqj</th>\n","      <td>Society–Space. Our conception of society–space...</td>\n","      <td>[[societyspace], [conception, societyspace, de...</td>\n","      <td>[determines, point, transform, social, world, ...</td>\n","      <td>topic_0</td>\n","      <td>-2.012044e-08</td>\n","      <td>health</td>\n","      <td>public_health</td>\n","      <td>national</td>\n","    </tr>\n","    <tr>\n","      <th>jwcgf3op</th>\n","      <td>Accessible areas in ecological niche compariso...</td>\n","      <td>[[accessible, area, ecological, niche, compari...</td>\n","      <td>[accessible, area, ecological, niche, comparis...</td>\n","      <td>topic_13</td>\n","      <td>-2.714867e-06</td>\n","      <td>research</td>\n","      <td>need</td>\n","      <td>global</td>\n","    </tr>\n","    <tr>\n","      <th>2h8yfriz</th>\n","      <td>Persistent Foot-and-Mouth Disease Virus Infect...</td>\n","      <td>[[persistent, footandmouth_disease, virus, inf...</td>\n","      <td>[persistent, footandmouth_disease, nasopharynx...</td>\n","      <td>topic_19</td>\n","      <td>-2.622129e-08</td>\n","      <td>replication</td>\n","      <td>vitro</td>\n","      <td>cell_line</td>\n","    </tr>\n","    <tr>\n","      <th>yz9yhnbx</th>\n","      <td>The SARS-CoV-2 receptor ACE2 expression of mat...</td>\n","      <td>[[sarscov, receptor, ace, expression, maternal...</td>\n","      <td>[sarscov, receptor, ace, expression, interface...</td>\n","      <td>topic_8</td>\n","      <td>-6.039613e-14</td>\n","      <td>severe_acute_respiratory</td>\n","      <td>sarscov</td>\n","      <td>covid</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>kwikwxge</th>\n","      <td>Epidemiological characteristics and transmissi...</td>\n","      <td>[[epidemiological, characteristic, transmissib...</td>\n","      <td>[epidemiological, characteristic, transmissibi...</td>\n","      <td>topic_8</td>\n","      <td>-6.181722e-13</td>\n","      <td>severe_acute_respiratory</td>\n","      <td>sarscov</td>\n","      <td>covid</td>\n","    </tr>\n","    <tr>\n","      <th>uznmbvom</th>\n","      <td>Sex in a test tube: testing the benefits of in...</td>\n","      <td>[[sex, test, tube, test, benefit, vitro, recom...</td>\n","      <td>[sex, test, tube, test, benefit, vitro, recomb...</td>\n","      <td>topic_4</td>\n","      <td>-8.304468e-12</td>\n","      <td>sequence</td>\n","      <td>genome</td>\n","      <td>gene</td>\n","    </tr>\n","    <tr>\n","      <th>e3zx2kh5</th>\n","      <td>Angiotensin converting enzyme 2 in the brain: ...</td>\n","      <td>[[angiotensin_convert, enzyme, brain, property...</td>\n","      <td>[angiotensin_convert, enzyme, brain, property,...</td>\n","      <td>topic_3</td>\n","      <td>0.000000e+00</td>\n","      <td>activation</td>\n","      <td>mechanism</td>\n","      <td>role</td>\n","    </tr>\n","    <tr>\n","      <th>99q1bhba</th>\n","      <td>Pharmacological and Biological Antiviral Thera...</td>\n","      <td>[[pharmacological, biological, antiviral, ther...</td>\n","      <td>[pharmacological, biological, antiviral, thera...</td>\n","      <td>topic_2</td>\n","      <td>0.000000e+00</td>\n","      <td>binding</td>\n","      <td>activity</td>\n","      <td>inhibit</td>\n","    </tr>\n","    <tr>\n","      <th>xcgvhhk4</th>\n","      <td>Symptom Severity Patterns in Experimental Comm...</td>\n","      <td>[[symptom, severity, pattern, experimental, co...</td>\n","      <td>[symptom, severity, pattern, experimental, com...</td>\n","      <td>topic_14</td>\n","      <td>-4.974865e-11</td>\n","      <td>age</td>\n","      <td>day</td>\n","      <td>total</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2426 rows × 8 columns</p>\n","</div>"],"text/plain":["                                                   document  ...     word_2\n","cord_uid                                                     ...           \n","ibwkkeud  6 Gastroenteritis. Publisher Summary Acute gas...  ...       calf\n","xeaqfxqj  Society–Space. Our conception of society–space...  ...   national\n","jwcgf3op  Accessible areas in ecological niche compariso...  ...     global\n","2h8yfriz  Persistent Foot-and-Mouth Disease Virus Infect...  ...  cell_line\n","yz9yhnbx  The SARS-CoV-2 receptor ACE2 expression of mat...  ...      covid\n","...                                                     ...  ...        ...\n","kwikwxge  Epidemiological characteristics and transmissi...  ...      covid\n","uznmbvom  Sex in a test tube: testing the benefits of in...  ...       gene\n","e3zx2kh5  Angiotensin converting enzyme 2 in the brain: ...  ...       role\n","99q1bhba  Pharmacological and Biological Antiviral Thera...  ...    inhibit\n","xcgvhhk4  Symptom Severity Patterns in Experimental Comm...  ...      total\n","\n","[2426 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"0uJRV_SAWMuw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592601765216,"user_tz":300,"elapsed":1098,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"2ff8db14-ffb1-49d1-f551-7f0bc0cf39d2"},"source":["# 6. Determinar las columnas que constituyen las equitetas (features) de clasificación\n","labels = output_df.columns.values.tolist()\n","labels.remove('cord_uid')\n","print(labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['activation', 'activity', 'age', 'antibody', 'antigen', 'approach', 'background', 'base', 'binding', 'calf', 'cell_line', 'child', 'complex', 'conclusion', 'country', 'covid', 'day', 'de', 'detect', 'detection', 'development', 'diarrhea', 'effect', 'express', 'expression', 'gene', 'genome', 'global', 'health', 'increase', 'inhibit', 'interaction', 'le', 'mechanism', 'model', 'mouse', 'national', 'need', 'objective', 'patient', 'pig', 'population', 'public_health', 'que', 'replication', 'research', 'respiratory', 'review', 'role', 'sample', 'sarscov', 'sequence', 'severe_acute_respiratory', 'significantly', 'structure', 'surveillance', 'total', 'understand', 'vaccine', 'vitro']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y_FqFTTHWvcN","colab_type":"code","colab":{}},"source":["# 7. Crear columna con las N palabras principales pronosticadas por el modelo \n","x_test['top_n_predicted_words'] = output_df[labels].apply(lambda s: s.nlargest(top_n_words).index.tolist(), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lg3DZ88VXysQ","colab_type":"code","colab":{}},"source":["# 8. Crear columnas para auxiliares para determinar precisión de la predicción del modelo\n"," \n","# a) Crear columnas para las N palabras principales pronosticadas\n","cols_predicted_words = []\n","cols_accuracy_predicted_words = []\n","\n","for i in range(top_n_words):\n","  # Columna con la palabra pronosticada\n","  col_name = 'predicted_word_' + str(i)\n","  cols_predicted_words.append(col_name)\n","  x_test[col_name] = \"\"\n","  # Columna con la precisión de la palabra pronosticada\n","  col_name = 'accuracy_predicted_word_' + str(i)\n","  cols_accuracy_predicted_words.append(col_name)\n","  x_test[col_name] = 0\n","\n","# b) Crear columna para la precisión de la predicción\n","x_test['predicted_words_accuracy'] = 0.0\n","\n","# c) Crear columna con total de ocurrencias de palabras pronosticadas\n","x_test['predicted_words_occurrences'] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-n0wESfDYufZ","colab_type":"code","colab":{}},"source":["# 9. Definir función que actualiza la información de las palabras pronosticadas\n","def update_predicted_words(row, cols_predicted_words, cols_accuracy_predicted_words):\n","  row_id = row['cord_uid']\n","  #print(f\"\\nrow id: {row_id}\")\n","  try:\n","    list_series_labels = ['cord_uid']\n","    list_series_values = [row_id]    \n","    list_predicted_words = list(row['top_n_predicted_words'])\n","    #print(\"list_predicted_words\",list_predicted_words)\n","    \n","    len_predicted_words = len(cols_predicted_words)\n","    topic_words = []  \n","    accuracy_predictions = []\n","\n","    for i in range(len_predicted_words):      \n","      col_name = cols_predicted_words[i]\n","      #print(f\"predicted col_name={col_name}\")\n","\n","      predicted_word = list_predicted_words[i]\n","      topic_word = row['word_' + str(i)]\n","      #print(\"predicted_word:\", predicted_word)\n","      #print(\"topic_word:\", topic_word)\n","      #print(f\"[{i}]: predicted_word={predicted_word}, topic_word={topic_word}\")\n","\n","      # Agregar palabra de topico a lista topic_words\n","      topic_words.append(topic_word)\n","\n","      # Actualizar Columna Palabra Predecida N con palabra correspondiente\n","      row[col_name] = predicted_word\n","      list_series_labels.append(col_name)\n","      list_series_values.append(predicted_word)\n","      \n","      # Actualizar Precisión de Predicción N\n","      col_name = cols_accuracy_predicted_words[i]\n","      accuracy_predicted_word = 1 if topic_word == predicted_word else 0 \n","      accuracy_predictions.append(accuracy_predicted_word)\n","\n","      list_series_labels.append(col_name)\n","      list_series_values.append(accuracy_predicted_word)\n","\n","    # Actualizar num. de ocurrencias de palabras predecidas\n","    num_occurrences = len(set(list_predicted_words) & set(topic_words))  \n","    list_series_labels.append('predicted_words_occurrences')\n","    list_series_values.append(num_occurrences)\n","\n","    # Actualizar total de aciertos de palabras predecidas (acuracy)\n","    accuracy = sum(accuracy_predictions) / len(accuracy_predictions)\n","\n","    #row['predicted_words_accuracy'] = accuracy\n","    list_series_labels.append('predicted_words_accuracy')\n","    list_series_values.append(accuracy)\n","\n","    # Pandas Dataframe: How to update multiple columns by applying a function?\n","    # https://stackoverflow.com/questions/32603051/pandas-dataframe-how-to-update-multiple-columns-by-applying-a-function\n","\n","    # https://www.geeksforgeeks.org/creating-a-pandas-series-from-lists/\n","    updated_series = pd.Series(list_series_values, index =list_series_labels)\n","    return updated_series\n","  except Exception as error:\n","    print(\"Error \", error.__class__, \"in row id\", row_id, \"occurred.\")\n","    print(error)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RobaHvtuYuTS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":475},"executionInfo":{"status":"ok","timestamp":1592601814912,"user_tz":300,"elapsed":2738,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"b26cf17c-3115-4991-efb1-af9647fc46c9"},"source":["# 10. Crear Dataframe temporal con la información actualizada de las palabras pronosticadas\n","df = x_test.apply(lambda row: update_predicted_words(row, cols_predicted_words, cols_accuracy_predicted_words), axis=1)\n","df.set_index('cord_uid')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>predicted_word_0</th>\n","      <th>accuracy_predicted_word_0</th>\n","      <th>predicted_word_1</th>\n","      <th>accuracy_predicted_word_1</th>\n","      <th>predicted_word_2</th>\n","      <th>accuracy_predicted_word_2</th>\n","      <th>predicted_words_occurrences</th>\n","      <th>predicted_words_accuracy</th>\n","    </tr>\n","    <tr>\n","      <th>cord_uid</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ibwkkeud</th>\n","      <td>patient</td>\n","      <td>0</td>\n","      <td>child</td>\n","      <td>0</td>\n","      <td>respiratory</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>xeaqfxqj</th>\n","      <td>national</td>\n","      <td>0</td>\n","      <td>public_health</td>\n","      <td>1</td>\n","      <td>health</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <th>jwcgf3op</th>\n","      <td>model</td>\n","      <td>0</td>\n","      <td>approach</td>\n","      <td>0</td>\n","      <td>base</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2h8yfriz</th>\n","      <td>role</td>\n","      <td>0</td>\n","      <td>activation</td>\n","      <td>0</td>\n","      <td>mechanism</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>yz9yhnbx</th>\n","      <td>sarscov</td>\n","      <td>0</td>\n","      <td>covid</td>\n","      <td>0</td>\n","      <td>severe_acute_respiratory</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>kwikwxge</th>\n","      <td>sarscov</td>\n","      <td>0</td>\n","      <td>severe_acute_respiratory</td>\n","      <td>0</td>\n","      <td>covid</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <th>uznmbvom</th>\n","      <td>sequence</td>\n","      <td>1</td>\n","      <td>genome</td>\n","      <td>1</td>\n","      <td>gene</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>e3zx2kh5</th>\n","      <td>role</td>\n","      <td>0</td>\n","      <td>activation</td>\n","      <td>0</td>\n","      <td>mechanism</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>99q1bhba</th>\n","      <td>role</td>\n","      <td>0</td>\n","      <td>activation</td>\n","      <td>0</td>\n","      <td>mechanism</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>xcgvhhk4</th>\n","      <td>patient</td>\n","      <td>0</td>\n","      <td>child</td>\n","      <td>0</td>\n","      <td>respiratory</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2426 rows × 8 columns</p>\n","</div>"],"text/plain":["         predicted_word_0  ...  predicted_words_accuracy\n","cord_uid                   ...                          \n","ibwkkeud          patient  ...                  0.000000\n","xeaqfxqj         national  ...                  0.333333\n","jwcgf3op            model  ...                  0.000000\n","2h8yfriz             role  ...                  0.000000\n","yz9yhnbx          sarscov  ...                  0.000000\n","...                   ...  ...                       ...\n","kwikwxge          sarscov  ...                  0.333333\n","uznmbvom         sequence  ...                  1.000000\n","e3zx2kh5             role  ...                  0.000000\n","99q1bhba             role  ...                  0.000000\n","xcgvhhk4          patient  ...                  0.000000\n","\n","[2426 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"code","metadata":{"id":"cUWCToqNZmPm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1592601817267,"user_tz":300,"elapsed":1206,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"31d9b4fd-f958-4968-e627-97c2d0dee8a4"},"source":["# 11. Determinar columnas a actualizar en el dataframe principal\n","cols_to_update = cols_predicted_words + cols_accuracy_predicted_words\n","cols_to_update.append('predicted_words_occurrences')\n","cols_to_update.append('predicted_words_accuracy')\n","\n","cols_to_update"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['predicted_word_0',\n"," 'predicted_word_1',\n"," 'predicted_word_2',\n"," 'accuracy_predicted_word_0',\n"," 'accuracy_predicted_word_1',\n"," 'accuracy_predicted_word_2',\n"," 'predicted_words_occurrences',\n"," 'predicted_words_accuracy']"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"uFtqKnYeTyET","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"status":"ok","timestamp":1592601820851,"user_tz":300,"elapsed":817,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"1ed00df8-d5fa-4eea-d61f-cafa870db5ef"},"source":["# 12. Actualizar dataframe principal (de testing)\n","x_test[cols_to_update] = df[cols_to_update]\n","x_test.sample(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cord_uid</th>\n","      <th>document</th>\n","      <th>clean</th>\n","      <th>clean_tfidf</th>\n","      <th>best_topic</th>\n","      <th>best_topic_score</th>\n","      <th>word_0</th>\n","      <th>word_1</th>\n","      <th>word_2</th>\n","      <th>top_n_predicted_words</th>\n","      <th>predicted_word_0</th>\n","      <th>accuracy_predicted_word_0</th>\n","      <th>predicted_word_1</th>\n","      <th>accuracy_predicted_word_1</th>\n","      <th>predicted_word_2</th>\n","      <th>accuracy_predicted_word_2</th>\n","      <th>predicted_words_accuracy</th>\n","      <th>predicted_words_occurrences</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>498</th>\n","      <td>vxihrg6q</td>\n","      <td>Kawasaki disease may be a hyperimmune reaction...</td>\n","      <td>[[kawasaki, disease, may, hyperimmune, reactio...</td>\n","      <td>[kawasaki, hyperimmune, reaction, genetically,...</td>\n","      <td>topic_1</td>\n","      <td>-0.000168</td>\n","      <td>patient</td>\n","      <td>respiratory</td>\n","      <td>child</td>\n","      <td>[patient, respiratory, child]</td>\n","      <td>patient</td>\n","      <td>1</td>\n","      <td>respiratory</td>\n","      <td>1</td>\n","      <td>child</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>552</th>\n","      <td>9b11c0od</td>\n","      <td>Twenty-five years of type I interferon-based t...</td>\n","      <td>[[twentyfive, year, type, interferonbased, tre...</td>\n","      <td>[twentyfive, year, type, treatment, critical, ...</td>\n","      <td>topic_3</td>\n","      <td>-0.001842</td>\n","      <td>activation</td>\n","      <td>mechanism</td>\n","      <td>role</td>\n","      <td>[role, activation, mechanism]</td>\n","      <td>role</td>\n","      <td>0</td>\n","      <td>activation</td>\n","      <td>0</td>\n","      <td>mechanism</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1606</th>\n","      <td>krhb0wzf</td>\n","      <td>Enterovirus D68 detection in respiratory speci...</td>\n","      <td>[[enterovirus, detection, respiratory, specime...</td>\n","      <td>[enterovirus, detection, respiratory, specimen...</td>\n","      <td>topic_1</td>\n","      <td>0.000000</td>\n","      <td>patient</td>\n","      <td>respiratory</td>\n","      <td>child</td>\n","      <td>[child, patient, sample]</td>\n","      <td>child</td>\n","      <td>0</td>\n","      <td>patient</td>\n","      <td>0</td>\n","      <td>sample</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      cord_uid  ... predicted_words_occurrences\n","498   vxihrg6q  ...                           3\n","552   9b11c0od  ...                           3\n","1606  krhb0wzf  ...                           2\n","\n","[3 rows x 18 columns]"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"code","metadata":{"id":"LWEmCs8ubA-i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1592601832118,"user_tz":300,"elapsed":723,"user":{"displayName":"covid19 satAI","photoUrl":"","userId":"18239185993142444955"}},"outputId":"037612da-4e37-4c8a-ff33-73c9fed649a8"},"source":["# 13. Obtener estadísticas\n","# Stats\n","average_accuracy_pct = x_test['predicted_words_accuracy'].mean() * 100\n","average_predicted_words_occurrence = x_test['predicted_words_occurrences'].mean()\n","average_predicted_words_occurrence_pct = average_predicted_words_occurrence / top_n_words * 100\n","\n","print(\"Precisión porcentual promedio: {:.2f}%\".format(average_accuracy_pct))\n","print(\"Número promedio de ocurrencias de palabras pronosticadas: {:.2f}\".format(average_predicted_words_occurrence))\n","print(\"Porcentaje promedio de ocurrencia de palabras pronosticadas: {:.2f}%\".format(average_predicted_words_occurrence_pct))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Precisión porcentual promedio : 15.11%\n","Número promedio de ocurrencias de palabras pronosticadas: 1.83\n","Porcentaje promedio de ocurrencia de palabras pronosticadas: 60.84%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZlZNvGF7TyoG","colab_type":"text"},"source":["# 4. Conclusiones"]},{"cell_type":"markdown","metadata":{"id":"cIPBaHdecLW9","colab_type":"text"},"source":["1. Se ha procedido con el afinamiento del modelo pre-entrenado BioBert, para la tarea de clasificación de múltiples etiquetas.  \n","\n","2. Se han obtenido y almacenado los pesos del modelo entrenado, mismo que serán utilizados en el tercer notebook del proyecto, con Bert como Servicio (Bert as a Service).  \n","\n","3. La *Precisión porcentual promedio* obtenida (al tomar en cuenta `1` si se ha acertado en  que se haya acertado en el pronóstico de una etiqueta y `0` si no), es de `15.11%` la cual resulta muy baja.\n","\n","4. En contraste, el *número de ocurrencias de las palabras pronosticadas* dentro del conjunto de etiquetas asignadas correspondientes a los registros, es en promedio de `1.83` (de las `3` utilizadas para el presente análisis).  \n","\n","5. La anterior métrica, expresada de otra manera, indica que el conjunto de palabras pronosticadas, coincide en un `60.84%` con las etiquetas correspondientes al tópico principal del documento (aunque no necesariamente coinciden en el mismo orden).\n","\n","6. Nos queda como tarea pendiente el revisar:  \n","    a) Si la manera de medir la precisión de las etiquetas pronosticadas es la adecuada.  \n","    b) Independientemente del punto anterior, determinar si la clasificación por múltiples etiquetas es la más adecuada en este caso (por ejemplo, podría ser más efectivo y conveniente clasificar por una sola clase/etiqueta -la más representativa de un tópico).  \n","    c) Efectuar las tareas necesarias para aumentar el vocabulario (archivo `vocab.txt`) del modelo. "]},{"cell_type":"code","metadata":{"id":"6pJ-okpRTz3b","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}